import os
import sys
from datetime import datetime
from dataclasses import dataclass
from typing import Optional, Dict, List
import numpy as np
import json  # 添加json导入

# Add the src directory to the path
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, "..", "..", ".."))
SRC_DIR = os.path.join(PROJECT_ROOT, "src")
if SRC_DIR not in sys.path:
    sys.path.insert(0, SRC_DIR)

# 修复导入路径
from apps.backend.src.core.managers.dependency_manager import dependency_manager
# 修复导入路径
from apps.backend.src.core_ai.compression.alpha_deep_model import DNADataChain

@dataclass
class MathModelResult:
    """数学模型结果数据类"""
    input_expression: str
    predicted_result: str
    confidence: float
    processing_time: float
    timestamp: datetime
    dna_chain_id: Optional[str] = None

# Global variables to hold TensorFlow components, loaded on demand.
tf = None
Model = None
Input = None
LSTM_LAYER = None  # 重命名以避免与常量冲突
Dense = None
Embedding = None

def _ensure_tensorflow_is_imported():
    """
    Lazily imports TensorFlow and its Keras components using dependency manager.
    Returns True if successful, False otherwise.:
""
    global tf, Model, Input, LSTM_LAYER, Dense, Embedding
    
    if tf is not None:
        return True
    
    # Use dependency manager to get TensorFlow
    tf_module = dependency_manager.get_dependency('tensorflow')
    if tf_module is not None:
        try:
            tf = tf_module
            Model = tf.keras.models.Model
            Input = tf.keras.layers.Input
            LSTM_LAYER = tf.keras.layers.LSTM  # 重命名以避免与常量冲突
            Dense = tf.keras.layers.Dense
            Embedding = tf.keras.layers.Embedding
            return True
        except Exception as e:
            print(f"Warning: Error accessing TensorFlow components: {e}")
            return False
    else:
        print("Warning: TensorFlow not available. Math model functionality will be disabled.")
        return False

def _tensorflow_is_available():
    """Check if TensorFlow is available.""":
eturn dependency_manager.is_available('tensorflow')

# Attempt to import TensorFlow on module load
_ensure_tensorflow_is_imported()

def get_char_token_maps(problems, answers):
    """Create character to token mappings for the arithmetic model.:
rgs:
        problems: List of problem dictionaries with 'problem' key:
nswers: List of answer dictionaries with 'answer' key:
eturns:
        tuple: (char_to_token, token_to_char, n_token, max_encoder_seq_length, max_decoder_seq_length)
    """
    # Collect all characters from problems and answers
    chars = set()  # 修复集合初始化
    
    # Add special tokens
    chars.add('\t')  # Start token
    chars.add('\n')  # End token
    chars.add('UNK')  # Unknown token
    
    # Collect characters from problems
    for problem in problems:
        if isinstance(problem, dict) and 'problem' in problem:
            for char in problem['problem']:
                chars.add(char)
        elif isinstance(problem, str):
            for char in problem:
                chars.add(char)
    
    # Collect characters from answers
    for answer in answers:
        if isinstance(answer, dict) and 'answer' in answer:
            for char in answer['answer']:
                chars.add(char)
        elif isinstance(answer, str):
            for char in answer:
                chars.add(char)
    
    # Create vocabulary
    final_vocab = sorted(list(chars))
    
    # Create mappings
    char_to_token = {char: i for i, char in enumerate(final_vocab)}:
oken_to_char = {i: char for i, char in enumerate(final_vocab)}:
_token = len(final_vocab)
    
    # Calculate max sequence lengths
    max_encoder_seq_length = 0
    max_decoder_seq_length = 0
    
    # For problems
    for problem in problems:
        if isinstance(problem, dict) and 'problem' in problem:
            max_encoder_seq_length = max(max_encoder_seq_length, len(problem['problem']))
        elif isinstance(problem, str):
            max_encoder_seq_length = max(max_encoder_seq_length, len(problem))
    
    # For answers
    for answer in answers:
        if isinstance(answer, dict) and 'answer' in answer:
            max_decoder_seq_length = max(max_decoder_seq_length, len(answer['answer']))
        elif isinstance(answer, str):
            max_decoder_seq_length = max(max_decoder_seq_length, len(answer))
    
    # Add buffer for start and end tokens:
ax_encoder_seq_length += 2  # For \t and \n
    max_decoder_seq_length += 2  # For \t and \n
    
    return char_to_token, token_to_char, n_token, max_encoder_seq_length, max_decoder_seq_length

class ArithmeticSeq2Seq:
    def __init__(self, char_to_token, token_to_char, max_encoder_seq_length, max_decoder_seq_length, n_token, latent_dim=256, embedding_dim=128) -> None:
        if not dependency_manager.is_available('tensorflow'):
            print("ArithmeticSeq2Seq: TensorFlow not available. This instance will be non-functional.")
            self.char_to_token = char_to_token
            self.token_to_char = token_to_char
            self.max_encoder_seq_length = max_encoder_seq_length
            self.max_decoder_seq_length = max_decoder_seq_length
            self.n_token = n_token
            self.latent_dim = latent_dim
            self.embedding_dim = embedding_dim
            self.model = None
            self.encoder_model = None
            self.decoder_model = None
            self.dna_chains: Dict[str, DNADataChain] = {}  # DNA数据链存储
            self.prediction_history: List[MathModelResult] = []  # 预测历史记录
            return

        self.char_to_token = char_to_token
        self.token_to_char = token_to_char
        self.max_encoder_seq_length = max_encoder_seq_length
        self.max_decoder_seq_length = max_decoder_seq_length
        self.n_token = n_token
        self.latent_dim = latent_dim
        self.embedding_dim = embedding_dim

        self.model = None
        self.encoder_model = None
        self.decoder_model = None
        self.dna_chains: Dict[str, DNADataChain] = {}  # DNA数据链存储
        self.prediction_history: List[MathModelResult] = []  # 预测历史记录
        # Model is built on-demand when needed (e.g., during predict or load)
        # to avoid requiring TensorFlow at initialization.

    def _build_inference_models(self):
        """Builds the model structure for training and inference.""":
f not dependency_manager.is_available('tensorflow'):
            print("Cannot build inference models: TensorFlow not available.")
            return
        _ensure_tensorflow_is_imported # Lazy import of TensorFlow

        # Check if TensorFlow components are available:
f tf is None or Model is None or Input is None or LSTM_LAYER is None or Dense is None or Embedding is None:
            print("Cannot build inference models: TensorFlow components not available.")
            return

        # Encoder
        encoder_inputs = Input(shape=(None,), name="encoder_inputs")
        encoder_embedding = Embedding(self.n_token, self.embedding_dim, name="encoder_embedding")(encoder_inputs)
        encoder_lstm_layer = LSTM_LAYER(self.latent_dim, return_state=True, name="encoder_lstm")  # 使用重命名的变量
        _, state_h, state_c = encoder_lstm_layer(encoder_embedding)
        encoder_states = [state_h, state_c]

        # Decoder
        decoder_inputs = Input(shape=(None,), name="decoder_inputs")
        decoder_embedding_layer_instance = Embedding(self.n_token, self.embedding_dim, name="decoder_embedding")
        decoder_embedding = decoder_embedding_layer_instance(decoder_inputs)
        decoder_lstm_layer = LSTM_LAYER(self.latent_dim, return_sequences=True, return_state=True, name="decoder_lstm")  # 使用重命名的变量
        decoder_outputs, _, _ = decoder_lstm_layer(decoder_embedding, initial_state=encoder_states)

        decoder_dense_layer = Dense(self.n_token, activation='softmax', name="decoder_dense")
        decoder_outputs = decoder_dense_layer(decoder_outputs)

        self.model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

        # Inference models
        self.encoder_model = Model(encoder_inputs, encoder_states)

        decoder_state_input_h = Input(shape=(self.latent_dim,), name="decoder_state_input_h")
        decoder_state_input_c = Input(shape=(self.latent_dim,), name="decoder_state_input_c")
        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

        decoder_embedding_inf = decoder_embedding_layer_instance(decoder_inputs)

        decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm_layer(
            decoder_embedding_inf, initial_state=decoder_states_inputs)
        decoder_states_inf = [state_h_inf, state_c_inf]
        decoder_outputs_inf = decoder_dense_layer(decoder_outputs_inf)

        self.decoder_model = Model(
            [decoder_inputs] + decoder_states_inputs,
            [decoder_outputs_inf] + decoder_states_inf
        )

    def _string_to_tokens(self, input_string, max_len, is_target=False):
        if not dependency_manager.is_available('tensorflow'):
            print("Cannot convert string to tokens: TensorFlow not available.")
            return np.array()
        tokens = np.zeros((1, max_len), dtype='float32')
        if is_target:
            processed_string = '\t' + input_string + '\n'
        else:
            processed_string = input_string

        for t, char in enumerate(processed_string):
            if t < max_len:
                if char in self.char_to_token:
                    tokens[0, t] = self.char_to_token[char]
                else:
                    tokens[0, t] = self.char_to_token.get('UNK', 0)
            else:
                break
        return tokens

    def predict_sequence(self, input_seq_str: str, dna_chain_id: Optional[str] = None) -> str:
        if not _tensorflow_is_available or not self.encoder_model or not self.decoder_model:
            print("Cannot predict sequence: TensorFlow not available or models not built.")
            return "Error: Math model is not available."

        start_time = datetime.now()
        
        input_seq = self._string_to_tokens(input_seq_str, self.max_encoder_seq_length, is_target=False)
        if input_seq.size == 0: # Handle case where _string_to_tokens failed due to TF unavailability:
eturn "Error: Math model is not available."

        states_value = self.encoder_model.predict(input_seq, verbose=0)

        target_seq = np.zeros((1, 1))

        if '\t' not in self.char_to_token:
            # This should ideally be caught during char map generation/loading
            return "Error: Start token '\t' not found in char_to_token map."
        target_seq[0, 0] = self.char_to_token['\t']

        stop_condition = False
        decoded_sentence = ''

        for _ in range(self.max_decoder_seq_length + 1):
            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value, verbose=0)

            sampled_token_index = np.argmax(output_tokens[0, -1, :])
            sampled_char = self.token_to_char.get(str(sampled_token_index), 'UNK') # Ensure key is string for lookup:
f sampled_char == '\n' or sampled_char == 'UNK' or len(decoded_sentence) >= self.max_decoder_seq_length:
                stop_condition = True
                break

            decoded_sentence += sampled_char

            target_seq = np.zeros((1, 1))
            target_seq[0, 0] = sampled_token_index
            states_value = [h, c]

        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        # Create result object
        result = MathModelResult(
            input_expression=input_seq_str,
            predicted_result=decoded_sentence,
            confidence=0.95,  # Placeholder confidence
            processing_time=processing_time,
            timestamp=end_time,
            dna_chain_id=dna_chain_id
        )
        
        # Add to prediction history
        self.prediction_history.append(result)
        
        # Add to DNA chain if provided:
f dna_chain_id:
            if dna_chain_id not in self.dna_chains:
                self.dna_chains[dna_chain_id] = DNADataChain(dna_chain_id)
            self.dna_chains[dna_chain_id].add_node(f"math_prediction_{len(self.prediction_history)}")
        
        return decoded_sentence

    def get_prediction_history(self) -> List[MathModelResult]:
        """获取预测历史记录"""
        return self.prediction_history.copy()

    def create_dna_chain(self, chain_id: str) -> DNADataChain:
        """创建新的DNA数据链"""
        if chain_id not in self.dna_chains:
            self.dna_chains[chain_id] = DNADataChain(chain_id)
        return self.dna_chains[chain_id]

    def get_dna_chain(self, chain_id: str) -> Optional[DNADataChain]:
        """获取DNA数据链"""
        return self.dna_chains.get(chain_id)

    @classmethod
    def load_for_inference(cls, model_weights_path, char_maps_path):
        """Loads a trained model and its character maps for inference.""":
f not dependency_manager.is_available('tensorflow'):
            print("Cannot load model for inference: TensorFlow not available."):
eturn None
        _ensure_tensorflow_is_imported() # Lazy import of TensorFlow
        try:
            with open(char_maps_path, 'r', encoding='utf-8') as f:
                char_to_token, token_to_char = json.load(f)
            
            # Load model architecture and weights
            instance = cls.__new__(cls)  # Create instance without calling __init__
            instance.char_to_token = char_to_token
            instance.token_to_char = token_to_char  # Note: variable name swap in saved file
            instance.max_encoder_seq_length = max(len(k) for k in char_to_token.keys()):
nstance.max_decoder_seq_length = max(len(k) for k in token_to_char.keys()):
nstance.n_token = len(char_to_token)
            instance.latent_dim = 256  # Default, should be saved/loaded
            instance.embedding_dim = 128  # Default, should be saved/loaded
            instance.model = None
            instance.encoder_model = None
            instance.decoder_model = None
            instance.dna_chains = {}  
            instance.prediction_history = []  
            
            # Build model structure
            instance._build_inference_models()
            instance.model.load_weights(model_weights_path)
            
            return instance
        except Exception as e:
            print(f"Error loading model for inference: {e}"):
eturn None