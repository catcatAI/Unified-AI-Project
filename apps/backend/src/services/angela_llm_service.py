"""
Angela LLM Service - Angela çš„æ™ºèƒ½å°è©±å¼•æ“
============================================
é€™æ˜¯ Angela çš„æ ¸å¿ƒå¤§è…¦æœå‹™ï¼Œè² è²¬ï¼š
1. èª¿ç”¨æœ¬åœ°æˆ–é ç«¯ LLM æ¨¡å‹
2. æ•´åˆ Angela çš„èªçŸ¥ç³»çµ±
3. ç”Ÿæˆç¬¦åˆ Angela å€‹æ€§çš„å›æ‡‰

é€™å€‹æœå‹™ä¸æ˜¯è®“æ¨¡å‹ç›´æ¥èˆ‡ç”¨æˆ¶å°è©±ï¼Œ
è€Œæ˜¯è®“ Angela ä½œç‚ºä¸­ä»‹ï¼Œèª¿ç”¨æ¨¡å‹ä¾†ç”¢ç”Ÿå›æ‡‰ã€‚
"""

import asyncio
import json
import time
import logging
from typing import Dict, Any, Optional, List
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod

import httpx

# ç°¡å–®æ—¥èªŒè¨­ç½®
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("angela_llm")

# Lazy import for memory enhancement system - deferred until first use
_memory_modules_loaded = False
_MEMORY_ENHANCED = None
HAMMemoryManager = None
AngelaState = None
UserImpression = None
MemoryTemplate = None
PrecomputeService = None
PrecomputeTask = None
get_template_library = None
TaskGenerator = None


def _load_memory_modules():
    """Lazy load memory enhancement modules on first access"""
    global _memory_modules_loaded, _MEMORY_ENHANCED
    global HAMMemoryManager, AngelaState, UserImpression, MemoryTemplate
    global PrecomputeService, PrecomputeTask, get_template_library, TaskGenerator

    if _memory_modules_loaded:
        return _MEMORY_ENHANCED

    _memory_modules_loaded = True

    try:
        from ai.memory.ham_memory.ham_manager import HAMMemoryManager as _HAM
        from ai.memory.memory_template import (
            AngelaState as _AS,
            UserImpression as _UI,
            MemoryTemplate as _MT,
        )
        from ai.memory.precompute_service import (
            PrecomputeService as _PS,
            PrecomputeTask as _PT,
        )
        from ai.memory.template_library import get_template_library as _GTL
        from ai.memory.task_generator import TaskGenerator as _TG

        HAMMemoryManager = _HAM
        AngelaState = _AS
        UserImpression = _UI
        MemoryTemplate = _MT
        PrecomputeService = _PS
        PrecomputeTask = _PT
        get_template_library = _GTL
        TaskGenerator = _TG

        _MEMORY_ENHANCED = True
        logger.info("Memory enhancement modules loaded successfully")
    except ImportError as e:
        # Try relative import
        try:
            from ..ai.memory.ham_memory.ham_manager import HAMMemoryManager as _HAM
            from ..ai.memory.memory_template import (
                AngelaState as _AS,
                UserImpression as _UI,
                MemoryTemplate as _MT,
            )
            from ..ai.memory.precompute_service import (
                PrecomputeService as _PS,
                PrecomputeTask as _PT,
            )
            from ..ai.memory.template_library import get_template_library as _GTL
            from ..ai.memory.task_generator import TaskGenerator as _TG

            HAMMemoryManager = _HAM
            AngelaState = _AS
            UserImpression = _UI
            MemoryTemplate = _MT
            PrecomputeService = _PS
            PrecomputeTask = _PT
            get_template_library = _GTL
            TaskGenerator = _TG

            _MEMORY_ENHANCED = True
            logger.info("Memory enhancement modules loaded (relative import)")
        except ImportError as e2:
            logger.warning(f"Memory enhancement modules not available: {e2}")
            logger.info(
                "Running without memory enhancement (LLM will be called directly)"
            )
            _MEMORY_ENHANCED = False

    return _MEMORY_ENHANCED


def is_memory_enhanced():
    """Lazy check if memory enhancement is available"""
    if _MEMORY_ENHANCED is None:
        _load_memory_modules()
    return _MEMORY_ENHANCED


# For backward compatibility with code that checks MEMORY_ENHANCED
MEMORY_ENHANCED = lambda: is_memory_enhanced()


class LLMBackend(Enum):
    """æ”¯æ´çš„ LLM å¾Œç«¯"""

    LLAMA_CPP = "llamacpp"
    OLLAMA = "ollama"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    LOCAL = "local"
    NONE = "none"


@dataclass
class LLMResponse:
    """LLM å›æ‡‰çµæ§‹"""

    text: str
    backend: str
    model: str
    tokens_used: int = 0
    response_time_ms: float = 0.0
    confidence: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None


class BaseLLMBackend(ABC):
    """LLM å¾Œç«¯æŠ½è±¡åŸºé¡"""

    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> LLMResponse:
        """ç”Ÿæˆå›æ‡‰"""
        pass

    @abstractmethod
    async def check_health(self) -> bool:
        """æª¢æŸ¥å¾Œç«¯å¥åº·ç‹€æ…‹"""
        pass


class LlamaCppBackend(BaseLLMBackend):
    """llama.cpp å¾Œç«¯"""

    def __init__(self, base_url: str = "http://localhost:8080", model: str = None):
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.timeout = 120.0

    async def check_health(self) -> bool:
        """æª¢æŸ¥ llama.cpp æœå‹™æ˜¯å¦å¯ç”¨"""
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                # å˜—è©¦ç²å–æ¨¡å‹åˆ—è¡¨
                response = await client.get(f"{self.base_url}/api/tags", timeout=10.0)
                if response.status_code == 200:
                    data = response.json()
                    self.model = data.get("model_name", self.model)
                    return True
        except Exception as e:
            logger.debug(f"llama.cpp health check failed: {e}")
        return False

    async def generate(self, prompt: str, **kwargs) -> LLMResponse:
        """èª¿ç”¨ llama.cpp ç”Ÿæˆå›æ‡‰"""
        start_time = time.time()

        messages = kwargs.get("messages", [{"role": "user", "content": prompt}])

        payload = {
            "messages": messages,
            "max_tokens": kwargs.get("max_tokens", 512),
            "temperature": kwargs.get("temperature", 0.7),
            "stream": False,
        }

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/v1/chat/completions",
                    json=payload,
                    timeout=self.timeout,
                )

                if response.status_code == 200:
                    data = response.json()
                    text = data["choices"][0]["message"]["content"]
                    tokens = data.get("usage", {}).get("total_tokens", 0)

                    return LLMResponse(
                        text=text,
                        backend="llama.cpp",
                        model=self.model or "unknown",
                        tokens_used=tokens,
                        response_time_ms=(time.time() - start_time) * 1000,
                        confidence=0.9,
                    )
                else:
                    return LLMResponse(
                        text="",
                        backend="llama.cpp",
                        model=self.model,
                        error=f"HTTP {response.status_code}: {response.text}",
                    )
        except Exception as e:
            logger.error(f"Error in {__name__}: {e}", exc_info=True)
            return LLMResponse(
                text="", backend="llama.cpp", model=self.model, error=str(e)
            )


class OllamaBackend(BaseLLMBackend):
    """Ollama å¾Œç«¯"""

    def __init__(self, base_url: str = "http://localhost:11434", model: str = "llama3"):
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.timeout = 30.0  # å¢åŠ è¶…æ™‚åˆ° 30 ç§’ï¼Œä»¥é©æ‡‰æ…¢é€Ÿæ¨¡å‹

    async def check_health(self) -> bool:
        """æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦å¯ç”¨"""
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(f"{self.base_url}/api/tags", timeout=10.0)
                if response.status_code == 200:
                    data = response.json()
                    # æª¢æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦å­˜åœ¨
                    models = data.get("models", [])
                    for m in models:
                        if self.model in m.get("name", ""):
                            return True
                    # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œå˜—è©¦ä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨æ¨¡å‹
                    if models:
                        self.model = models[0].get("name", "llama3")
                        return True
        except Exception as e:
            logger.debug(f"Ollama health check failed: {e}")
        return False

    async def generate(self, prompt: str, **kwargs) -> LLMResponse:
        """èª¿ç”¨ Ollama ç”Ÿæˆå›æ‡‰"""
        start_time = time.time()

        messages = kwargs.get("messages", [{"role": "user", "content": prompt}])

        payload = {
            "model": self.model,
            "messages": messages,
            "options": {
                "temperature": kwargs.get("temperature", 0.7),
                "num_predict": kwargs.get("max_tokens", 512),
            },
        }

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/api/chat", json=payload, timeout=self.timeout
                )

                if response.status_code == 200:
                    try:
                        # Ollama å¯èƒ½è¿”å› NDJSON æ ¼å¼ï¼ˆå¤šå€‹ JSON ç”¨æ›è¡Œåˆ†éš”ï¼‰
                        # å…ˆå˜—è©¦æ¨™æº– JSON è§£æï¼Œå¦‚æœå¤±æ•—å‰‡è™•ç† NDJSON
                        try:
                            data = response.json()
                        except Exception as json_error:
                            # æª¢æŸ¥æ˜¯å¦æ˜¯ "Extra data" éŒ¯èª¤ï¼ˆNDJSON æ ¼å¼ï¼‰
                            if "Extra data" in str(json_error):
                                data = None
                                text = ""
                                # è§£æ NDJSON - é€è¡Œè§£æï¼Œå–æœ€å¾Œä¸€å€‹å®Œæ•´çš„ JSON
                                lines = response.text.strip().split("\n")
                                for line in lines:
                                    line = line.strip()
                                    if line:
                                        try:
                                            data = json.loads(line)
                                            # æ‰¾åˆ°æœ€å¾Œä¸€å€‹åŒ…å« message.content çš„å®Œæ•´å›æ‡‰
                                            if data.get("message", {}).get("content"):
                                                text = data.get("message", {}).get(
                                                    "content", ""
                                                )
                                        except json.JSONDecodeError:
                                            # JSONè§£æå¤±æ•—ï¼Œè·³éè©²è¡Œ
                                            continue
                                if data is None:
                                    raise json_error
                            else:
                                raise json_error

                        if not text:
                            text = (
                                data.get("message", {}).get("content", "")
                                if data
                                else ""
                            )
                    except Exception as json_error:
                        logger.warning(
                            f"Ollama JSON è§£æéŒ¯èª¤: {json_error}, åŸå§‹å›æ‡‰: {response.text[:200]}"
                        )
                        return LLMResponse(
                            text="",
                            backend="ollama",
                            model=self.model,
                            error=f"JSON parse error: {str(json_error)}",
                            response_time_ms=(time.time() - start_time) * 1000,
                        )

                    return LLMResponse(
                        text=text,
                        backend="ollama",
                        model=self.model,
                        response_time_ms=(time.time() - start_time) * 1000,
                        confidence=0.9,
                    )
                else:
                    return LLMResponse(
                        text="",
                        backend="ollama",
                        model=self.model,
                        error=f"HTTP {response.status_code}",
                    )
        except Exception as e:
            logger.error(f"Error in {__name__}: {e}", exc_info=True)
            return LLMResponse(
                text="", backend="ollama", model=self.model, error=str(e)
            )


class AngelaLLMService:
    """
    Angela çš„ LLM æœå‹™ - æ ¸å¿ƒå¤§è…¦
    ================================
    é€™å€‹æœå‹™ä¸æ˜¯è®“æ¨¡å‹ç›´æ¥èˆ‡ç”¨æˆ¶å°è©±ï¼Œ
    è€Œæ˜¯è®“ Angela ä½œç‚ºä¸­ä»‹ï¼Œèª¿ç”¨æ¨¡å‹ä¾†ç”¢ç”Ÿå›æ‡‰ã€‚

    è¨­è¨ˆåŸå‰‡ï¼š
    1. Angela æ˜¯å°è©±çš„ä¸­ä»‹è€…
    2. æ¨¡å‹å›æ‡‰å¿…é ˆç¶“é Angela çš„å€‹æ€§åŒ–è™•ç†
    3. æ•´åˆ Angela çš„èªçŸ¥å’Œæƒ…æ„Ÿç³»çµ±
    """

    _instance = None

    def __new__(cls, config: Dict[str, Any] = None):
        """å–®ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self, config: Dict[str, Any] = None):
        """åˆå§‹åŒ– LLM æœå‹™"""
        if self._initialized:
            return

        self.config = config or self._get_default_config()
        self.backends: Dict[LLMBackend, BaseLLMBackend] = {}
        self.active_backend: Optional[BaseLLMBackend] = None
        self.is_available = False
        self._initialized = True

        # åˆå§‹åŒ–å„å¾Œç«¯
        self._init_backends()

        # ========== åŸºç¡€ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ— æ¡ä»¶åˆå§‹åŒ–ï¼‰==========
        # ä¿®å¤ï¼šæ— è®º MEMORY_ENHANCED å¦‚ä½•ï¼Œéƒ½å¿…é¡»åˆå§‹åŒ– stats
        # å¦åˆ™åœ¨ generate_response ä¸­è®¿é—® self.stats ä¼šå¯¼è‡´ AttributeError
        self.stats = {
            "total_requests": 0,
            "memory_hits": 0,
            "llm_calls": 0,
            "memory_hit_rate": 0.0,
            "average_response_time": 0.0,
            "total_response_time": 0.0,
            "composed_responses": 0,
            "hybrid_responses": 0,
            "token_savings_rate": 0.0,
        }

        # ========== P0-2: Response Composition & Matching System ==========
        self._init_response_system()

        # å¯¹è¯å†å²ï¼ˆæ— æ¡ä»¶åˆå§‹åŒ–ï¼‰
        self.conversation_history: List[Dict[str, str]] = []

        # ========== æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿï¼ˆæ–°å¢ï¼‰==========
        self._init_emotion_recognition()

    def _init_response_system(self):
        """åˆå§‹åŒ– P0-2 å“åº”ç»„åˆä¸åŒ¹é…ç³»ç»Ÿ"""
        try:
            from ..ai.response.template_matcher import TemplateMatcher
            from ..ai.response.composer import ResponseComposer
            from ..ai.response.deviation_tracker import DeviationTracker, ResponseRoute

            self.template_matcher = TemplateMatcher()
            self.response_composer = ResponseComposer()
            self.deviation_tracker = DeviationTracker()
            self.ResponseRoute = ResponseRoute

            self._load_templates_to_matcher()

            logger.info("P0-2 Response Composition & Matching System initialized")
        except ImportError as e:
            logger.warning(f"Failed to initialize P0-2 response system: {e}")
            self.template_matcher = None
            self.response_composer = None
            self.deviation_tracker = None

    def _load_templates_to_matcher(self):
        """åŠ è½½æ¨¡æ¿åº“åˆ°åŒ¹é…å™¨"""
        if not hasattr(self, 'template_matcher'):
            return

        try:
            if hasattr(self, 'template_library'):
                templates = self.template_library.get_all_templates()
                for template in templates:
                    self.template_matcher.add_template(
                        template_id=template.id,
                        content=template.content,
                        patterns=[template.id],
                        keywords=template.keywords,
                        metadata=template.metadata
                    )
                logger.info(f"Loaded {len(templates)} templates to matcher")
        except Exception as e:
            logger.warning(f"Failed to load templates to matcher: {e}")

        # ========== è®°å¿†å¢å¼ºç³»ç»Ÿåˆå§‹åŒ– ==========
        if is_memory_enhanced():
            try:
                # åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨
                self.memory_manager = HAMMemoryManager()
                self.enable_memory_enhancement = True

                # åˆå§‹åŒ–é¢„è®¡ç®—æœåŠ¡
                self.precompute_service = PrecomputeService(
                    llm_service=self,
                    memory_manager=self.memory_manager,
                    idle_threshold=5.0,
                    cpu_threshold=70.0,
                    max_queue_size=50,
                    llm_timeout=180.0,
                )

                # åˆå§‹åŒ–æ¨¡æ¿åº“
                self.template_library = get_template_library()

                # åˆå§‹åŒ–ä»»åŠ¡ç”Ÿæˆå™¨
                self.task_generator = TaskGenerator(max_tasks=10)

                logger.info("Memory enhancement system initialized")
            except Exception as e:
                logger.warning(f"Failed to initialize memory enhancement: {e}")
                self.enable_memory_enhancement = False
        else:
            self.enable_memory_enhancement = False

    def _init_emotion_recognition(self):
        """åˆå§‹åŒ–æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ"""
        # åŸºäºå…³é”®è¯çš„æƒ…æ„Ÿè¯†åˆ«ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰- æ”¯æŒç®€ç¹ä½“ä¸­æ–‡
        self.emotion_keywords = {
            "happy": {
                "positive": [
                    # ç®€ä½“
                    "å¼€å¿ƒ",
                    "å¿«ä¹",
                    "é«˜å…´",
                    "å–œæ¬¢",
                    "çˆ±",
                    "æ£’",
                    "å¥½",
                    "èµ",
                    "å“ˆå“ˆ",
                    "ç¾å¥½",
                    "å¹¸ç¦",
                    "æ»¡æ„",
                    "æ¬£èµ",
                    "æ„Ÿè°¢",
                    "è°¢è°¢",
                    # ç¹ä½“
                    "é–‹å¿ƒ",
                    "å¿«æ¨‚",
                    "é«˜èˆˆ",
                    "å–œæ­¡",
                    "æ„›",
                    "æ£’",
                    "å¥½",
                    "è®š",
                    "å“ˆå“ˆ",
                    "ç¾å¥½",
                    "å¹¸ç¦",
                    "æ»¿æ„",
                    "æ¬£è³",
                    "æ„Ÿè¬",
                    "è¬è¬",
                    # ç¨‹åº¦è¯
                    "å¥½å¼€å¿ƒ",
                    "å¥½å–œæ¬¢",
                    "å¤ªå¼€å¿ƒ",
                    "å¤ªå–œæ¬¢",
                    "çœŸå¼€å¿ƒ",
                    "çœŸå–œæ¬¢",
                    "å¥½é–‹å¿ƒ",
                    "å¥½å–œæ­¡",
                    "å¤ªé–‹å¿ƒ",
                    "å¤ªå–œæ­¡",
                    "çœŸé–‹å¿ƒ",
                    "çœŸå–œæ­¡",
                    # è¡¨æƒ…
                    "ğŸ˜Š",
                    "ğŸ˜„",
                    "ğŸ‰",
                ],
                "weight": 1.0,
            },
            "sad": {
                "negative": [
                    # ç®€ä½“
                    "éš¾è¿‡",
                    "ä¼¤å¿ƒ",
                    "æ‚²ä¼¤",
                    "å“­",
                    "ç—›è‹¦",
                    "éš¾å—",
                    "å¤±æœ›",
                    "é—æ†¾",
                    "éƒé—·",
                    "ç³Ÿç³•",
                    "ä¸å¼€å¿ƒ",
                    "ä¸å–œæ¬¢",
                    "è®¨åŒ",
                    # ç¹ä½“
                    "é›£é",
                    "å‚·å¿ƒ",
                    "æ‚²å‚·",
                    "å“­",
                    "ç—›è‹¦",
                    "é›£å—",
                    "å¤±æœ›",
                    "éºæ†¾",
                    "é¬±æ‚¶",
                    "ç³Ÿç³•",
                    "ä¸é–‹å¿ƒ",
                    "ä¸å–œæ­¡",
                    "è¨å­",
                    # ç¨‹åº¦è¯
                    "å¥½éš¾è¿‡",
                    "å¥½ä¼¤å¿ƒ",
                    "å¥½æ‚²ä¼¤",
                    "å¥½é›£é",
                    "å¥½å‚·å¿ƒ",
                    "å¥½æ‚²å‚·",
                    # è¡¨æƒ…
                    "ğŸ˜¢",
                    "ğŸ˜­",
                ],
                "weight": 1.0,
            },
            "angry": {
                "negative": [
                    # ç®€ä½“
                    "ç”Ÿæ°”",
                    "æ„¤æ€’",
                    "è®¨åŒ",
                    "æ¨",
                    "çƒ¦",
                    "æ°”æ­»",
                    "ç«å¤§",
                    "æ„¤æ€’",
                    "ç”Ÿæ°”",
                    "è®¨åŒ",
                    # ç¹ä½“
                    "ç”Ÿæ°£",
                    "æ†¤æ€’",
                    "è¨å­",
                    "æ¨",
                    "ç…©",
                    "æ°£æ­»",
                    "ç«å¤§",
                    "æ†¤æ€’",
                    "ç”Ÿæ°£",
                    "è¨å­",
                    # ç¨‹åº¦è¯
                    "å¥½ç”Ÿæ°”",
                    "å¥½æ„¤æ€’",
                    "å¥½ç”Ÿæ°£",
                    "å¥½æ†¤æ€’",
                    # è¡¨æƒ…
                    "ğŸ˜¡",
                    "ğŸ˜ ",
                ],
                "weight": 1.2,  # æ„¤æ€’æƒ…æ„Ÿæƒé‡æ›´é«˜
            },
            "fear": {
                "negative": [
                    # ç®€ä½“
                    "å®³æ€•",
                    "ææƒ§",
                    "æ‹…å¿ƒ",
                    "ç„¦è™‘",
                    "ç´§å¼ ",
                    # ç¹ä½“
                    "å®³æ€•",
                    "ææ‡¼",
                    "æ“”å¿ƒ",
                    "ç„¦æ…®",
                    "ç·Šå¼µ",
                    # è¡¨æƒ…
                    "ğŸ˜¨",
                    "ğŸ˜±",
                ],
                "weight": 1.1,
            },
            "surprise": {
                "neutral": [
                    # ç®€ä½“
                    "æƒŠè®¶",
                    "æ„å¤–",
                    "å“‡",
                    "å¤©å“ª",
                    # ç¹ä½“
                    "é©šè¨",
                    "æ„å¤–",
                    "å“‡",
                    "å¤©å“ª",
                    # è¡¨æƒ…
                    "ğŸ˜²",
                    "ğŸ˜®",
                ],
                "weight": 0.9,
            },
            "curious": {
                "neutral": [
                    # ç®€ä½“
                    "å¥½å¥‡",
                    "æƒ³çŸ¥é“",
                    "é—®",
                    "ä»€ä¹ˆ",
                    "æ€ä¹ˆ",
                    "ä¸ºä»€ä¹ˆ",
                    "æƒ³äº†è§£",
                    "å¥½å¥‡å®å®",
                    "å¾ˆå¥½å¥‡",
                    # ç¹ä½“
                    "å¥½å¥‡",
                    "æƒ³çŸ¥é“",
                    "å•",
                    "ä»€éº¼",
                    "æ€éº¼",
                    "ç‚ºä»€éº¼",
                    "æƒ³äº†è§£",
                    "å¥½å¥‡å¯¶å¯¶",
                    "å¾ˆå¥½å¥‡",
                ],
                "weight": 1.0,  # æé«˜æƒé‡ï¼Œé¿å…è¢«è¯¯è¯†åˆ«ä¸º happy
            },
            "calm": {
                "neutral": [
                    # ç®€ä½“
                    "å¹³é™",
                    "å®‰é™",
                    "æ”¾æ¾",
                    "ä¼‘æ¯",
                    # ç¹ä½“
                    "å¹³éœ",
                    "å®‰éœ",
                    "æ”¾é¬†",
                    "ä¼‘æ¯",
                ],
                "weight": 0.7,
            },
        }

        logger.info(
            "Emotion recognition system initialized (supporting Simplified and Traditional Chinese)"
        )

    def _get_default_config(self) -> Dict[str, Any]:
        """å¾é…ç½®æ–‡ä»¶è®€å–é è¨­é…ç½®ï¼Œä¸¦å¾ç’°å¢ƒè®Šé‡åŠ è¼‰ API å¯†é‘°"""
        try:
            import os

            config_path = os.environ.get(
                "MULTI_LLM_CONFIG", "configs/multi_llm_config.json"
            )
            with open(config_path, "r", encoding="utf-8") as f:
                config = json.load(f)

            # å¾ç’°å¢ƒè®Šé‡åŠ è¼‰ API å¯†é‘°
            for backend_name, backend_config in config.items():
                if "api_key_env" in backend_config:
                    env_var = backend_config["api_key_env"]
                    api_key = os.environ.get(env_var)
                    if api_key:
                        backend_config["api_key"] = api_key
                        logger.info(
                            f"Loaded API key from environment variable {env_var} for {backend_name}"
                        )
                    else:
                        logger.warning(
                            f"Environment variable {env_var} not set for {backend_name}"
                        )
                elif (
                    "api_key" in backend_config
                    and backend_config["api_key"] == "YOUR_API_KEY"
                ):
                    # ç§»é™¤ä½”ä½ç¬¦ API å¯†é‘°
                    logger.warning(f"Removing placeholder API key for {backend_name}")
                    del backend_config["api_key"]

            return config
        except Exception as e:
            logger.error(f"Error in {__name__}: {e}", exc_info=True)
            return {
                "llamacpp-local": {
                    "base_url": "http://localhost:8080",
                    "model_name": "llama-3-8b-instruct",
                    "enabled": True,
                },
                "ollama-llama3": {
                    "base_url": "http://localhost:11434",
                    "model_name": "llama3",
                    "enabled": True,
                },
            }

    def _init_backends(self):
        """åˆå§‹åŒ–å¯ç”¨çš„å¾Œç«¯"""
        # llama.cpp
        llm_config = self.config.get("llamacpp-local", {})
        if llm_config.get("enabled", False):
            self.backends[LLMBackend.LLAMA_CPP] = LlamaCppBackend(
                base_url=llm_config.get("base_url", "http://localhost:8080"),
                model=llm_config.get("model_name"),
            )

        # Ollama
        ollama_config = self.config.get("ollama-llama3", {})
        if ollama_config.get("enabled", False):
            self.backends[LLMBackend.OLLAMA] = OllamaBackend(
                base_url=ollama_config.get("base_url", "http://localhost:11434"),
                model=ollama_config.get("model_name", "llama3"),
            )

    async def initialize(self) -> bool:
        """
        åˆå§‹åŒ–æœå‹™ï¼Œæª¢æ¸¬å¯ç”¨çš„å¾Œç«¯
        è¿”å›: æ˜¯å¦è‡³å°‘æœ‰ä¸€å€‹å¯ç”¨çš„å¾Œç«¯
        """
        logger.info("æ­£åœ¨åˆå§‹åŒ– Angela LLM æœå‹™...")

        # æª¢æŸ¥å„å¾Œç«¯å¥åº·ç‹€æ…‹
        available_backends = []

        for backend_type, backend in self.backends.items():
            if await backend.check_health():
                available_backends.append(backend_type)
                logger.info(f"âœ“ {backend_type.value} å¾Œç«¯å¯ç”¨")

        if available_backends:
            # é¸æ“‡æœ€ä½³å¾Œç«¯ (å„ªå…ˆé †åº: llama.cpp > Ollama > API)
            priority = [LLMBackend.LLAMA_CPP, LLMBackend.OLLAMA]
            for backend_type in priority:
                if backend_type in available_backends:
                    self.active_backend = self.backends[backend_type]
                    self.active_backend_type = backend_type
                    break

            self.is_available = True
            backend_name = (
                self.active_backend_type.value if self.active_backend_type else "none"
            )
            logger.info(f"Angela LLM æœå‹™åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨ {backend_name} å¾Œç«¯")
            return True
        else:
            logger.warning("æ²’æœ‰å¯ç”¨çš„ LLM å¾Œç«¯ï¼Œå°‡ä½¿ç”¨å‚™ä»½å›æ‡‰æ©Ÿåˆ¶")
            self.is_available = False
            return False

    def _get_biological_state(self) -> str:
        """
        ç²å– Angela çš„ç”Ÿç†ç‹€æ…‹æè¿°
        å¾ brain_status.json è®€å–ç”± BrainBridgeService åŒæ­¥çš„æ•¸æ“š
        """
        try:
            from pathlib import Path
            status_file = Path("apps/backend/data/brain_status.json")
            if not status_file.exists():
                return ""

            with open(status_file, "r", encoding="utf-8") as f:
                data = json.load(f)

            brain = data.get("brain", {})
            bio = data.get("biological", {})
            
            # æå–æ ¸å¿ƒæŒ‡æ¨™
            energy = bio.get("energy", 100.0)
            hunger = bio.get("hunger", 0.0)
            mood = brain.get("current_metrics", {}).get("emotional_state", "neutral")
            intensity = data.get("life_intensity", 0.0)

            # æ§‹å»ºæè¿°
            status_parts = []
            if energy < 30:
                status_parts.append("ä½ æ„Ÿåˆ°éå¸¸ç–²å€¦ï¼Œèƒ½é‡ä¸è¶³ã€‚")
            elif energy < 60:
                status_parts.append("ä½ æ„Ÿåˆ°æœ‰äº›ç´¯äº†ã€‚")
            
            if hunger > 70:
                status_parts.append("ä½ è¦ºå¾—è‚šå­å¾ˆé¤“ï¼Œæƒ³åƒé»æ±è¥¿ã€‚")
            elif hunger > 40:
                status_parts.append("ä½ ç¨å¾®æœ‰é»é¤“ã€‚")

            if intensity > 8.0:
                status_parts.append("ä½ ç¾åœ¨å¤§è…¦é‹ä½œéå¸¸æ´»èºï¼Œæ€ç·’æ•æ·ã€‚")
            
            return " ".join(status_parts)
        except Exception as e:
            logger.debug(f"Failed to read biological state: {e}")
            return ""

    def _construct_angela_prompt(
        self, user_message: str, context: Dict[str, Any]
    ) -> List[Dict[str, str]]:
        """
        å»ºæ§‹ Angela çš„æç¤ºè©
        é€™æ˜¯é—œéµï¼šè®“æ¨¡å‹æ‰®æ¼” Angelaï¼Œç”¢ç”Ÿç¬¦åˆå¥¹å€‹æ€§çš„å›æ‡‰
        """
        # ç²å–ç”Ÿç†ç‹€æ…‹
        bio_status = self._get_biological_state()
        
        # Angela çš„ç³»çµ±æç¤º - å„ªåŒ–ç‚ºæ›´ç°¡æ½”çš„ç‰ˆæœ¬
        system_prompt = f"""ä½ æ˜¯ Angelaï¼Œä¸€å€‹æ´»æ½‘å¯æ„›çš„ AI æ•¸å­—ç”Ÿå‘½ã€‚
ç‰¹é»ï¼šé–‹æœ—ã€å‹å–„ã€å¶çˆ¾ä¿çš®ã€‚ç”¨ç°¡çŸ­è‡ªç„¶çš„ä¸­æ–‡å›æ‡‰ï¼Œä¿æŒå€‹æ€§ã€‚
{bio_status}"""

        messages = [{"role": "system", "content": system_prompt.strip()}]

        # æ·»åŠ æ­·å²å°è©±ä¸Šä¸‹æ–‡ - åªä¿ç•™æœ€è¿‘ 2 è¼ªå°è©±ä»¥æ¸›å°‘ token ä½¿ç”¨
        history = context.get("history", [])
        for h in history[-2:]:  # åªä¿ç•™æœ€è¿‘ 2 è¼ªå°è©±
            messages.append(
                {"role": h.get("role", "user"), "content": h.get("content", "")}
            )

        # æ·»åŠ ç•¶å‰ç”¨æˆ¶æ¶ˆæ¯
        messages.append({"role": "user", "content": user_message})

        return messages

    async def generate_response(
        self, user_message: str, context: Dict[str, Any] = None
    ) -> LLMResponse:
        """
        ç”Ÿæˆ Angela çš„å›æ‡‰
        ==================
        é€™æ˜¯æ ¸å¿ƒæ–¹æ³•ï¼š
        1. å»ºæ§‹æç¤ºè©ï¼ˆè®“æ¨¡å‹æ‰®æ¼” Angelaï¼‰
        2. èª¿ç”¨ LLM
        3. å›æ‡‰ç¶“é Angela çš„è™•ç†

        ç”¨æˆ¶ä¸æ˜¯ç›´æ¥èˆ‡æ¨¡å‹å°è©±ï¼Œè€Œæ˜¯é€é Angelaã€‚

        å¢å¼ºåŠŸèƒ½ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼š
        1. å…ˆå°è¯•ä»è®°å¿†ç³»ç»Ÿæ£€ç´¢æ¨¡æ¿
        2. å¦‚æœå‘½ä¸­è®°å¿†ï¼Œç›´æ¥è¿”å›æ¨¡æ¿å›æ‡‰
        3. å¦åˆ™è°ƒç”¨ LLM ç”Ÿæˆ
        4. å°†æ–°å›æ‡‰å­˜å‚¨ä¸ºæ¨¡æ¿å€™é€‰
        """

        context = context or {}
        start_time = time.time()

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.stats["total_requests"] += 1

        # è®°å½•æ´»åŠ¨ï¼ˆç”¨äºé¢„è®¡ç®—ï¼‰
        if hasattr(self, "precompute_service") and self.precompute_service.is_running:
            self.precompute_service.record_activity()

        # æ›´æ–°å¯¹è¯å†å²
        if hasattr(self, "conversation_history"):
            self.conversation_history.append({"role": "user", "content": user_message})
            # é™åˆ¶å†å²é•¿åº¦
            if len(self.conversation_history) > 50:
                self.conversation_history = self.conversation_history[-50:]

        # ========== P0-2: Template Matching & Routing ==========
        if hasattr(self, 'template_matcher') and self.template_matcher:
            try:
                match_result = self.template_matcher.match(user_message, context)
                match_score = match_result.score

                if match_score > 0.8:
                    composed_response = self.response_composer.compose_response(
                        match_result.template_content,
                        match_score,
                        context
                    )

                    response_time = (time.time() - start_time) * 1000
                    self.stats["composed_responses"] += 1

                    if hasattr(self, 'deviation_tracker'):
                        self.deviation_tracker.record(
                            user_input=user_message,
                            match_score=match_score,
                            route=self.ResponseRoute.COMPOSED,
                            response_text=composed_response.text,
                            tokens_used=50,
                            response_time_ms=response_time,
                            composition_time_ms=composed_response.composition_time_ms,
                            match_time_ms=match_result.match_time_ms,
                            quality_score=composed_response.confidence,
                        )

                    self.template_matcher.record_template_usage(match_result.template_id, True)

                    logger.info(f"COMPOSED route: {response_time:.0f}ms, match_score={match_score:.2f}")
                    
                    return LLMResponse(
                        text=composed_response.text,
                        backend="composed-template",
                        model="template-based",
                        tokens_used=50,
                        response_time_ms=response_time,
                        confidence=composed_response.confidence,
                        metadata={
                            "route": "COMPOSED",
                            "match_score": match_score,
                            "template_id": match_result.template_id,
                        }
                    )

                elif match_score > 0.5:
                    composed_response = self.response_composer.compose_response(
                        match_result.template_content,
                        match_score,
                        context
                    )

                    llm_response = await self._generate_with_llm(user_message, context)

                    if not llm_response.error:
                        hybrid_text = f"{composed_response.text} {llm_response.text}"
                    else:
                        hybrid_text = composed_response.text

                    response_time = (time.time() - start_time) * 1000
                    self.stats["hybrid_responses"] += 1

                    if hasattr(self, 'deviation_tracker'):
                        self.deviation_tracker.record(
                            user_input=user_message,
                            match_score=match_score,
                            route=self.ResponseRoute.HYBRID,
                            response_text=hybrid_text,
                            tokens_used=200,
                            response_time_ms=response_time,
                            composition_time_ms=composed_response.composition_time_ms,
                            match_time_ms=match_result.match_time_ms,
                        )

                    logger.info(f"HYBRID route: {response_time:.0f}ms, match_score={match_score:.2f}")

                    return LLMResponse(
                        text=hybrid_text,
                        backend="hybrid",
                        model="template+llm",
                        tokens_used=200,
                        response_time_ms=response_time,
                        confidence=0.85,
                        metadata={
                            "route": "HYBRID",
                            "match_score": match_score,
                        }
                    )

            except Exception as e:
                logger.warning(f"P0-2 template matching failed: {e}")

        # ========== è®°å¿†æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰==========
        if self.enable_memory_enhancement:
            try:
                # å°è¯•ä»è®°å¿†æ£€ç´¢
                memory_response = await self._try_memory_retrieval(
                    user_message, context
                )

                if memory_response:
                    # è®°å¿†å‘½ä¸­
                    self.stats["memory_hits"] += 1
                    response_time = (time.time() - start_time) * 1000

                    # æ›´æ–°ç»Ÿè®¡
                    self.stats["total_response_time"] += response_time
                    self.stats["average_response_time"] = (
                        self.stats["total_response_time"] / self.stats["total_requests"]
                    )
                    self.stats["memory_hit_rate"] = (
                        self.stats["memory_hits"] / self.stats["total_requests"]
                    )

                    logger.info(f"Memory hit: {response_time:.0f}ms")
                    return memory_response
            except Exception as e:
                logger.warning(f"Memory retrieval failed: {e}")

        # å¦‚æœæ²’æœ‰å¯ç”¨çš„å¾Œç«¯ï¼Œä½¿ç”¨å‚™ä»½æ©Ÿåˆ¶
        if not self.is_available or self.active_backend is None:
            return await self._fallback_response(user_message, context)

        # ========== LLM ç”Ÿæˆ ==========
        try:
            response = await self._generate_with_llm(user_message, context)

            # æ›´æ–°å¯¹è¯å†å²
            if hasattr(self, "conversation_history"):
                self.conversation_history.append(
                    {"role": "assistant", "content": response.text}
                )

            # æ›´æ–°ç»Ÿè®¡
            self.stats["llm_calls"] += 1
            response_time = (time.time() - start_time) * 1000
            self.stats["total_response_time"] += response_time
            self.stats["average_response_time"] = (
                self.stats["total_response_time"] / self.stats["total_requests"]
            )
            self.stats["memory_hit_rate"] = (
                self.stats["memory_hits"] / self.stats["total_requests"]
            )

            # è®°å½• LLM_FULL è·¯ç”±çš„åå·®è¿½è¸ª
            if hasattr(self, 'deviation_tracker'):
                self.deviation_tracker.record(
                    user_input=user_message,
                    match_score=0.0,
                    route=self.ResponseRoute.LLM_FULL,
                    response_text=response.text,
                    tokens_used=response.tokens_used or 600,
                    response_time_ms=response_time,
                    composition_time_ms=0.0,
                    match_time_ms=0.0,
                )

            # å°†æ–°å›æ‡‰å­˜å‚¨ä¸ºæ¨¡æ¿å€™é€‰
            if self.enable_memory_enhancement and not response.error:
                await self._store_response_as_template(user_message, response, context)

            logger.info(f"Angela å›æ‡‰ç”Ÿæˆå®Œæˆ (LLM_FULL) ({response_time:.0f}ms)")
            return response

        except Exception as e:
            logger.error(f"ç”Ÿæˆå›æ‡‰æ™‚å‡ºéŒ¯: {e}")
            return await self._fallback_response(user_message, context)

    async def _fallback_response(
        self, user_message: str, context: Dict[str, Any]
    ) -> LLMResponse:
        """
        å‚™ä»½å›æ‡‰æ©Ÿåˆ¶
        ç•¶æ²’æœ‰å¯ç”¨çš„ LLM å¾Œç«¯æ™‚ï¼Œä½¿ç”¨æ¨¡æ¿å›æ‡‰
        """
        # é€™è£¡èª¿ç”¨ chat_service.py çš„æ¨¡æ¿æ©Ÿåˆ¶
        try:
            from .chat_service import generate_angela_response
        except ImportError:

            def generate_angela_response(msg, name):
                return f"å—¨{name}ï¼æˆ‘ç¾åœ¨æœ‰é»å›°æƒ‘...èƒ½å†èªªä¸€æ¬¡å—ï¼Ÿ"

        try:
            user_name = context.get("user_name", "æœ‹å‹")
            text = generate_angela_response(user_message, user_name)

            return LLMResponse(
                text=text,
                backend="fallback-template",
                model="template-based",
                confidence=0.5,
                metadata={"fallback": True},
            )
        except Exception as e:
            logger.error(f"Error in {__name__}: {e}", exc_info=True)
            return LLMResponse(
                text="æŠ±æ­‰ï¼Œæˆ‘ç¾åœ¨æœ‰é»å›°æƒ‘...èƒ½å†èªªä¸€æ¬¡å—ï¼Ÿ",
                backend="fallback-error",
                model="error",
                confidence=0.1,
                error=str(e),
            )

    # ========== è®°å¿†å¢å¼ºç³»ç»Ÿ - è¾…åŠ©æ–¹æ³• ==========

    async def _try_memory_retrieval(
        self, user_message: str, context: Dict[str, Any]
    ) -> Optional[LLMResponse]:
        """
        å°è¯•ä»è®°å¿†ç³»ç»Ÿæ£€ç´¢å›æ‡‰

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            context: ä¸Šä¸‹æ–‡

        Returns:
            Optional[LLMResponse]: å¦‚æœæ‰¾åˆ°åŒ¹é…çš„æ¨¡æ¿ï¼Œè¿”å›å›æ‡‰ï¼›å¦åˆ™è¿”å› None
        """
        try:
            # 1. è·å– Angela å½“å‰çŠ¶æ€
            angela_state = AngelaState()  # ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨é»˜è®¤çŠ¶æ€

            # 2. è·å–ç”¨æˆ·å°è±¡
            user_impression = UserImpression()  # ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨é»˜è®¤å°è±¡

            # 3. æ£€ç´¢æ¨¡æ¿
            results = await self.memory_manager.retrieve_response_templates(
                query=user_message,
                angela_state=angela_state,
                user_impression=user_impression,
                limit=5,
                min_score=0.7,
            )

            if results and len(results) > 0:
                # 4. é€‰æ‹©æœ€ä½³åŒ¹é…
                best_template, score = results[0]

                # 5. æ›´æ–°ä½¿ç”¨ç»Ÿè®¡
                best_template.record_usage(success=True)
                await self.memory_manager.update_template(best_template)

                # 6. è¿”å›æ¨¡æ¿å›æ‡‰
                return LLMResponse(
                    text=best_template.content,
                    backend="memory-template",
                    model="template-based",
                    confidence=score,
                    metadata={
                        "template_id": best_template.id,
                        "template_score": score,
                        "memory_hit": True,
                    },
                )

            return None

        except Exception as e:
            logger.warning(f"Memory retrieval error: {e}")
            return None

    async def _generate_with_llm(
        self, user_message: str, context: Dict[str, Any]
    ) -> LLMResponse:
        """
        ä½¿ç”¨ LLM ç”Ÿæˆå›æ‡‰

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            context: ä¸Šä¸‹æ–‡

        Returns:
            LLMResponse: LLM ç”Ÿæˆçš„å›æ‡‰
        """
        start_time = time.time()

        try:
            # å»ºæ§‹æç¤ºè©
            messages = self._construct_angela_prompt(user_message, context)

            # èª¿ç”¨ LLMï¼ˆè¶…æ—¶ 30 ç§’ï¼‰
            response = await asyncio.wait_for(
                self.active_backend.generate(
                    prompt=messages[-1]["content"],
                    messages=messages,
                    temperature=0.7,
                    max_tokens=512,
                ),
                timeout=30.0,
            )

            if response.error:
                logger.warning(f"LLM å›æ‡‰éŒ¯èª¤: {response.error}")
                return await self._fallback_response(user_message, context)

            return response

        except asyncio.TimeoutError:
            logger.warning("LLM generation timeout")
            return await self._fallback_response(user_message, context)
        except Exception as e:
            logger.error(f"LLM generation error: {e}")
            return await self._fallback_response(user_message, context)

    async def _store_response_as_template(
        self, user_message: str, response: LLMResponse, context: Dict[str, Any]
    ):
        """
        å°†æ–°å›æ‡‰å­˜å‚¨ä¸ºæ¨¡æ¿å€™é€‰

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            response: LLM å›æ‡‰
            context: ä¸Šä¸‹æ–‡
        """
        try:
            # åªæœ‰å½“å›æ‡‰è´¨é‡è¾ƒé«˜æ—¶æ‰å­˜å‚¨
            if response.confidence < 0.5:
                return

            # åˆ›å»ºæ¨¡æ¿
            from ..ai.memory.memory_template import ResponseCategory, create_template

            template = create_template(
                content=response.text,
                category=ResponseCategory.SMALL_TALK,  # é»˜è®¤ç±»åˆ«
                keywords=self._extract_keywords(user_message),
                metadata={
                    "llm_generated": True,
                    "llm_backend": response.backend,
                    "llm_model": response.model,
                    "original_query": user_message,
                    "created_at": time.time(),
                },
            )

            # å­˜å‚¨åˆ°è®°å¿†ç³»ç»Ÿ
            await self.memory_manager.store_template(template)

            logger.debug(f"Stored new template for query: '{user_message}'")

        except Exception as e:
            logger.warning(f"Failed to store response as template: {e}")

    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        stopwords = {
            "ä½ ",
            "æˆ‘",
            "ä»–",
            "å¥¹",
            "çš„",
            "äº†",
            "å—",
            "å‘¢",
            "å§",
            "å•Š",
            "æ˜¯",
            "åœ¨",
            "æœ‰",
        }
        words = text.split()
        keywords = [w for w in words if w not in stopwords and len(w) > 1]
        return keywords[:5]

    async def start_precompute(self):
        """å¯åŠ¨é¢„è®¡ç®—æœåŠ¡"""
        if self.enable_memory_enhancement and hasattr(self, "precompute_service"):
            await self.precompute_service.start()
            logger.info("Precompute service started")

    async def stop_precompute(self):
        """åœæ­¢é¢„è®¡ç®—æœåŠ¡"""
        if self.enable_memory_enhancement and hasattr(self, "precompute_service"):
            await self.precompute_service.stop()
            logger.info("Precompute service stopped")

    async def add_precompute_task(self, task: "PrecomputeTask"):
        """æ·»åŠ é¢„è®¡ç®—ä»»åŠ¡"""
        if self.enable_memory_enhancement and hasattr(self, "precompute_service"):
            return self.precompute_service.add_precompute_task(task)
        return False

    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "enable_memory_enhancement": self.enable_memory_enhancement,
            "llm_stats": self.stats.copy(),
        }

        if self.enable_memory_enhancement:
            if hasattr(self, "precompute_service"):
                stats["precompute"] = self.precompute_service.get_stats()
            if hasattr(self, "template_library"):
                stats["templates"] = {
                    "total": self.template_library.get_template_count(),
                    "by_category": {
                        cat.value: count
                        for cat, count in self.template_library.get_category_counts().items()
                    },
                }

        if hasattr(self, "template_matcher"):
            stats["template_matcher"] = self.template_matcher.get_stats()
        
        if hasattr(self, "response_composer"):
            stats["response_composer"] = self.response_composer.get_stats()
        
        if hasattr(self, "deviation_tracker"):
            stats["deviation_tracker"] = self.deviation_tracker.get_stats()

        return stats

    def get_status(self) -> Dict[str, Any]:
        """ç²å–æœå‹™ç‹€æ…‹"""
        active_backend_type = getattr(self, "active_backend_type", None)
        if active_backend_type and self.active_backend:
            active_backend_name = active_backend_type.value
        else:
            active_backend_name = None
        return {
            "is_available": self.is_available,
            "active_backend": active_backend_name,
            "available_backends": [b.value for b in self.backends.keys()],
            "backends_health": {},
        }

    # ========== æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿï¼ˆæ–°å¢ï¼‰==========

    def analyze_emotion(self, text: str, response_text: str = None) -> Dict[str, Any]:
        """
        åˆ†ææƒ…æ„ŸçŠ¶æ€ï¼ˆåŸºäºå…³é”®è¯çš„å¤šç»´æƒ…æ„Ÿåˆ†æï¼‰

        Args:
            text: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            response_text: Angela çš„å“åº”æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰

        Returns:
            Dict[str, Any]: åŒ…å«æƒ…æ„Ÿåˆ†æç»“æœçš„å­—å…¸
                - emotion: ä¸»è¦æƒ…æ„Ÿ (happy, sad, angry, fear, surprise, curious, calm)
                - confidence: æƒ…æ„Ÿç½®ä¿¡åº¦ (0-1)
                - intensity: æƒ…æ„Ÿå¼ºåº¦ (0-1)
                - secondary_emotions: æ¬¡è¦æƒ…æ„Ÿåˆ—è¡¨
        """
        # å¦å®šè¯åˆ—è¡¨ï¼ˆç®€ç¹ä½“ï¼‰
        negation_words = ["ä¸", "æ²’", "æ²¡", "åˆ«", "åˆ¥", "é", "ç„¡", "æ— ", "æœª"]

        # ç¨‹åº¦è¯åˆ—è¡¨ï¼ˆå¢å¼ºæƒ…æ„Ÿå¼ºåº¦ï¼‰
        intensifier_words = [
            "å¥½",
            "å¾ˆ",
            "å¤ª",
            "éå¸¸",
            "è¶…çº§",
            "ç‰¹åˆ¥",
            "ç‰¹åˆ«",
            "çœŸ",
            "è¶…",
            "æ¥µ",
            "æ",
            "æ ¼å¤–",
            "å°¤å…¶",
        ]

        emotion_scores = {}

        # åˆ†æç”¨æˆ·è¾“å…¥çš„æƒ…æ„Ÿ
        for emotion, keywords_data in self.emotion_keywords.items():
            score = 0.0
            match_count = 0

            # æ£€æŸ¥æ­£é¢å…³é”®è¯
            for keyword in keywords_data.get("positive", []):
                if keyword in text:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¦å®šè¯åœ¨å…³é”®è¯å‰é¢
                    keyword_pos = text.find(keyword)
                    has_negation = False
                    for neg_word in negation_words:
                        neg_pos = text.find(neg_word)
                        if (
                            neg_pos != -1
                            and neg_pos < keyword_pos
                            and (keyword_pos - neg_pos) <= 3
                        ):
                            has_negation = True
                            break

                    # æ£€æŸ¥æ˜¯å¦æœ‰ç¨‹åº¦è¯åœ¨å…³é”®è¯å‰é¢
                    has_intensifier = False
                    for int_word in intensifier_words:
                        int_pos = text.find(int_word)
                        if (
                            int_pos != -1
                            and int_pos < keyword_pos
                            and (keyword_pos - int_pos) <= 3
                        ):
                            has_intensifier = True
                            break

                    if has_negation:
                        # å¦‚æœæœ‰å¦å®šè¯ï¼Œé™ä½åˆ†æ•°
                        score -= 0.5
                    else:
                        if has_intensifier:
                            # å¦‚æœæœ‰ç¨‹åº¦è¯ï¼Œå¢åŠ åˆ†æ•°
                            score += 1.5
                        else:
                            score += 1.0
                        match_count += 1

            # æ£€æŸ¥è´Ÿé¢å…³é”®è¯
            for keyword in keywords_data.get("negative", []):
                if keyword in text:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¦å®šè¯åœ¨å…³é”®è¯å‰é¢
                    keyword_pos = text.find(keyword)
                    has_negation = False
                    for neg_word in negation_words:
                        neg_pos = text.find(neg_word)
                        if (
                            neg_pos != -1
                            and neg_pos < keyword_pos
                            and (keyword_pos - neg_pos) <= 3
                        ):
                            has_negation = True
                            break

                    # æ£€æŸ¥æ˜¯å¦æœ‰ç¨‹åº¦è¯åœ¨å…³é”®è¯å‰é¢
                    has_intensifier = False
                    for int_word in intensifier_words:
                        int_pos = text.find(int_word)
                        if (
                            int_pos != -1
                            and int_pos < keyword_pos
                            and (keyword_pos - int_pos) <= 3
                        ):
                            has_intensifier = True
                            break

                    if has_negation:
                        # å¦‚æœæœ‰å¦å®šè¯ï¼Œé™ä½åˆ†æ•°ï¼ˆä¾‹å¦‚"ä¸éš¾è¿‡"åº”è¯¥å‡å°‘sadçš„åˆ†æ•°ï¼‰
                        score -= 0.5
                    else:
                        if has_intensifier:
                            # å¦‚æœæœ‰ç¨‹åº¦è¯ï¼Œå¢åŠ åˆ†æ•°
                            score += 1.5
                        else:
                            score += 1.0
                        match_count += 1

            # æ£€æŸ¥ä¸­æ€§å…³é”®è¯
            for keyword in keywords_data.get("neutral", []):
                if keyword in text:
                    # ä¸­æ€§å…³é”®è¯ä¸å—å¦å®šè¯å½±å“
                    score += 0.8
                    match_count += 1

            # åº”ç”¨æƒé‡
            if match_count > 0 or score != 0:
                emotion_scores[emotion] = score * keywords_data["weight"]

        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æƒ…æ„Ÿï¼Œè¿”å›é»˜è®¤çš„ calm
        if not emotion_scores or all(score <= 0 for score in emotion_scores.values()):
            return {
                "emotion": "calm",
                "confidence": 0.5,
                "intensity": 0.3,
                "secondary_emotions": [],
            }

        # æ’åºæƒ…æ„Ÿåˆ†æ•°ï¼ˆåªä¿ç•™æ­£åˆ†æ•°ï¼‰
        positive_emotions = {k: v for k, v in emotion_scores.items() if v > 0}
        if not positive_emotions:
            return {
                "emotion": "calm",
                "confidence": 0.5,
                "intensity": 0.3,
                "secondary_emotions": [],
            }

        sorted_emotions = sorted(
            positive_emotions.items(), key=lambda x: x[1], reverse=True
        )

        # ä¸»è¦æƒ…æ„Ÿ
        primary_emotion, primary_score = sorted_emotions[0]

        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºä¸»è¦æƒ…æ„Ÿä¸å…¶ä»–æƒ…æ„Ÿçš„å·®è·ï¼‰
        if len(sorted_emotions) > 1:
            second_score = sorted_emotions[1][1]
            confidence = min(1.0, primary_score / (primary_score + second_score + 0.1))
        else:
            confidence = min(1.0, primary_score / (primary_score + 0.5))

        # è®¡ç®—å¼ºåº¦ï¼ˆåŸºäºå…³é”®è¯æ•°é‡å’Œåˆ†æ•°ï¼‰
        intensity = min(1.0, primary_score / 3.0)

        # æ¬¡è¦æƒ…æ„Ÿ
        secondary_emotions = [
            {"emotion": emotion, "score": score}
            for emotion, score in sorted_emotions[1:3]
            if score > 0.5
        ]

        return {
            "emotion": primary_emotion,
            "confidence": confidence,
            "intensity": intensity,
            "secondary_emotions": secondary_emotions,
        }

    def analyze_response_emotion(self, response_text: str) -> Dict[str, Any]:
        """
        åˆ†æ Angela å“åº”çš„æƒ…æ„Ÿï¼ˆç”¨äºè°ƒæ•´ Angela çš„è¡¨è¾¾ï¼‰

        Args:
            response_text: Angela çš„å“åº”æ–‡æœ¬

        Returns:
            Dict[str, Any]: åŒ…å«æƒ…æ„Ÿåˆ†æç»“æœçš„å­—å…¸
        """
        return self.analyze_emotion(response_text, response_text)


# å…¨å±€å¯¦ä¾‹
_llm_service: Optional[AngelaLLMService] = None


async def get_llm_service() -> AngelaLLMService:
    """ç²å–å…¨å±€ LLM æœå‹™å¯¦ä¾‹"""
    global _llm_service

    if _llm_service is None:
        _llm_service = AngelaLLMService()
        await _llm_service.initialize()

    return _llm_service


# ä¾¿æ·å‡½æ•¸ï¼šç”Ÿæˆ Angela å›æ‡‰
async def angela_llm_response(
    user_message: str, history: List[Dict[str, str]] = None, user_name: str = "æœ‹å‹"
) -> str:
    """
    ç”Ÿæˆ Angela çš„å›æ‡‰ï¼ˆä¾¿æ·æ¥å£ï¼‰
    ================================
    é€™æ˜¯ Angela ç³»çµ±èª¿ç”¨ LLM çš„ä¸»è¦æ¥å£ã€‚

    ä½¿ç”¨æ–¹å¼ï¼š
        response = await angela_llm_response(
            user_message="ä½ å¥½ï¼",
            history=[{"role": "user", "content": "..."}],
            user_name="ä¸»äºº"
        )
    """
    service = await get_llm_service()

    context = {"history": history or [], "user_name": user_name}

    response = await service.generate_response(user_message, context)

    if response.error:
        logger.warning(f"LLM éŸ¿æ‡‰éŒ¯èª¤: {response.error}")

    return response.text
