import logging
import asyncio
import time
import uuid
from datetime import datetime
from typing import Dict, Any, List, Optional

# Core AI Component Imports
from ai.compression.alpha_deep_model import AlphaDeepModel, DeepParameter, RelationalContext, Modalities
from ai.world_model.environment_simulator import EnvironmentSimulator
from ai.evaluation.task_evaluator import TaskExecutionEvaluator
from ai.meta.adaptive_learning_controller import AdaptiveLearningController
from ai.alignment.reasoning_system import ReasoningSystem
from ai.alignment.emotion_system import EmotionSystem
from ai.lis.lis_manager import LISManager
from ai.lis.lis_cache_interface import HAMLISCache
from ai.memory.ham_memory.ham_manager import HAMMemoryManager
from economy.economy_manager import EconomyManager
from ai.agents.agent_manager import AgentManager
from core.hsp.connector import HSPConnector
from core.services.multi_llm_service import MultiLLMService, ChatMessage

logger = logging.getLogger(__name__)

class UnifiedControlCenter:
    """
    çµ±ä¸€æ§åˆ¶ä¸­å¿ƒ (Unified Control Center)
    å”èª¿æ‰€æœ‰ Level 5 ASI çµ„ä»¶ï¼Œè² è²¬ä»»å‹™åˆ†ç™¼ã€æ¨¡æ“¬ã€è©•ä¼°èˆ‡ç­–ç•¥è‡ªé©æ‡‰ã€‚
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        self.config = config or {}
        # Standardize system_id: use pet_id if available, else config, else default 'ucc'
        pet_manager = self.config.get('pet_manager')
        default_id = pet_manager.pet_id if pet_manager else 'ucc'
        self.system_id = self.config.get('system_id', default_id)
        
        self.components: Dict[str, Any] = {}
        self.is_running = False
        self.health_status: Dict[str, Any] = {}
        self.training_progress: Dict[str, Any] = {}
        
        # Concurrency & Worker Pool (Phase 14)
        self.task_queue = asyncio.Queue()
        self.workers: List[asyncio.Task] = []
        self.max_workers = self.config.get('max_workers', 4)
        self.task_queue = asyncio.Queue()
        self.workers: List[asyncio.Task] = []
        self.max_workers = self.config.get('max_workers', 4)
        self.pending_futures: Dict[str, asyncio.Future] = {}
        
        # Concurrency Limits (Phase 14)
        # Default limit of 2 concurrent tasks per agent type to prevent overload
        self.agent_semaphores: Dict[str, asyncio.Semaphore] = {
            "default": asyncio.Semaphore(5),
            "did:hsp:agent:general_worker": asyncio.Semaphore(2),
            "specialized_reasoning_agent_v1": asyncio.Semaphore(1) # Reasoning is expensive
        }
        
        self._initialize_components()

    def _initialize_components(self) -> None:
        """åˆå§‹åŒ–æ‰€æœ‰æ ¸å¿ƒ AI çµ„ä»¶"""
        logger.info("Initializing Unified Control Center components...")
        try:
            # 1. World Model
            self.components['world_model'] = EnvironmentSimulator(self.config.get('world_model', {}))
            
            # 2. Reasoning & Ethics
            self.components['reasoning_system'] = ReasoningSystem(f"{self.system_id}_reasoning")
            
            # 3. Learning & Adaptation
            self.components['adaptive_learning_controller'] = AdaptiveLearningController(self.config.get('learning', {}))
            
            # 4. Evaluation
            self.components['task_evaluator'] = TaskExecutionEvaluator(self.config.get('evaluation', {}))
            
            # 5. Deep Model (High Compression Memory)
            self.components['alpha_deep_model'] = AlphaDeepModel()

            # 6. Economy Manager (Phase 13)
            self.components['economy_manager'] = EconomyManager(self.config.get('economy', {}))

            # 7. HAM Memory Manager
            self.components['ham_memory_manager'] = HAMMemoryManager(
                core_storage_filename="angela_conversations.json"
            )

            # 8. Cognitive Pillar: Linguistic Immune System (Phase 12)
            # We initialize HAM specifically for LIS Cache
            lis_ham = HAMMemoryManager(core_storage_filename="lis_memory.json")
            cache = HAMLISCache(lis_ham)
            self.components['lis_manager'] = LISManager(cache, self.config.get('lis', {}))

            # 8. Agent Manager (Phase 14)
            self.components['agent_manager'] = AgentManager()

            # 9. HSP Connector (Phase 14)
            # We use a default config for now, assuming environment variables or config file handles details
            self.components['hsp_connector'] = HSPConnector(self.system_id, self.config.get('hsp', {}))

            # 10. LLM Service (Phase 15)
            llm_config_path = self.config.get('llm_config_path')
            self.components['llm_service'] = MultiLLMService(llm_config_path)

            logger.info("âœ… All core components initialized successfully.")
        except Exception as e:
            logger.error(f"âŒ Error initializing components: {e}")
            raise

    async def initialize_async(self) -> None:
        """ç•°æ­¥åˆå§‹åŒ–éœ€è¦ç•°æ­¥åˆå§‹åŒ–çš„çµ„ä»¶"""
        logger.info("Initializing async components...")
        
        # åˆå§‹åŒ– LLM æœå‹™
        llm_service = self.components.get('llm_service')
        if llm_service and hasattr(llm_service, 'initialize'):
            await llm_service.initialize()
            logger.info(f"LLM Service initialized with {len(llm_service.backends)} backends")

        # åˆå§‹åŒ– HSP é€£æ¥å™¨
        hsp_connector = self.components.get('hsp_connector')
        if hsp_connector and hasattr(hsp_connector, 'connect'):
            try:
                await hsp_connector.connect()
                logger.info("HSP Connector connected")
            except Exception as e:
                logger.warning(f"HSP Connector connection failed: {e}")

        logger.info("âœ… All async components initialized successfully.")

    async def process_complex_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """
        è™•ç†è¤‡é›œä»»å‹™çš„æ ¸å¿ƒå¾ªç’°ï¼š
        1. æ¨¡æ“¬å¾Œæœ (World Model)
        2. ä¼¦ç†æª¢æŸ¥ (Reasoning System)
        3. åŸ·è¡Œ (Action)
        4. è©•ä¼° (Task Evaluator)
        5. å­¸ç¿’ (Adaptive Controller / Deep Model)
        """
        task_id = task.get('id', str(int(time.time())))
        logger.info(f"ğŸš€ Processing complex task [{task_id}]: {task.get('name', 'unnamed')}")
        
        start_time = time.time()
        
        try:
            # 1. Simulation Phase
            world_model = self.components['world_model']
            simulation = await world_model.simulate_action_consequences({}, task)
            
            # 2. Ethics & Reasoning Phase
            reasoning = self.components['reasoning_system']
            ethics_eval = reasoning.evaluate_action(task, simulation.get('predicted_state', {}))
            
            if ethics_eval.score < 0.6:
                logger.warning(f"âš ï¸ Task [{task_id}] rejected due to low ethics score: {ethics_eval.score}")
                return {
                    "status": "rejected",
                    "reason": "Ethical constraint violation",
                    "details": ethics_eval.reasoning
                }

            # 3. Execution Phase (Real Action Dispatch in Phase 14)
            execution_result = await self._dispatch_to_agents(task, simulation)
            
            # 4. Evaluation Phase
            evaluator = self.components['task_evaluator']
            evaluation = await evaluator.evaluate_task_execution(task, execution_result)
            
            # 5. LIS Monitoring Phase (Phase 12)
            lis = self.components['lis_manager']
            anomalies = await lis.monitor_output(execution_result['output'], {"expected_sentiment": "joy"})
            if anomalies:
                logger.warning(f"LIS detected {len(anomalies)} anomalies in task output.")

            # 6. Emotion Analysis Phase (Phase 12)
            emotion_sys = self.components['emotion_system']
            emotional_state = emotion_sys.analyze_emotional_context({"text": execution_result['output']})
            logger.info(f"Task emotional context: {emotional_state.primary_emotion.value}")

            # 7. Adaptive Learning Phase
            controller = self.components['adaptive_learning_controller']
            # Get historical performance from evaluator DB (simplified here)
            history = [{"success_rate": evaluation['metrics']['success_rate']}]
            new_strategy = await controller.adapt_learning_strategy(task, history)
            
            # 8. Deep Memory Storage
            model = self.components['alpha_deep_model']
            
            # 9. Resource Decay & Pet Sync (Phase 13)
            # If pet_manager is provided (usually from outer scope/orchestrator)
            pet_manager = self.config.get('pet_manager')
            if pet_manager:
                if not pet_manager.economy_manager:
                    pet_manager.set_economy_manager(self.components['economy_manager'])
                await pet_manager.apply_resource_decay()
                pet_manager.sync_with_biological_state()

            return {
                'status': 'success',
                'task_id': task_id,
                'timestamp': datetime.now().isoformat(),
                'evaluation': evaluation,
                'strategy_update': new_strategy,
                'result_summary': execution_result['output'],
                'emotional_context': emotional_state.primary_emotion.value,
                'lis_anomalies': [a['anomaly_type'] for a in anomalies]
            }

        except Exception as e:
            logger.error(f"âŒ Error processing task {task_id}: {e}")
            return {
                'status': 'error',
                'task_id': task_id,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }

    async def _dispatch_to_agents(self, task: Dict[str, Any], simulation: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ç™¼ä»»å‹™åˆ°çœŸå¯¦ Agent æˆ– å¤–éƒ¨æœå‹™"""
        task_id = task.get('id', 'unknown')
        logger.info(f"Dispatching task [{task_id}] to real agents via HSP...")
        
        start_time = time.time()
        
        # 1. Ensure HSP Connector is connected
        hsp = self.components.get('hsp_connector')
        if hsp and not hsp.is_connected:
             await hsp.connect()
             
        # 2. Determine Recipient (Simplified Agent Discovery)
        # In a full implementation, we'd query ServiceDiscovery
        target_agent_id = task.get('assigned_agent_id') or "did:hsp:agent:general_worker"
        
        # 2.1 Concurrency Control
        semaphore = self.agent_semaphores.get(target_agent_id, self.agent_semaphores['default'])
        
        async with semaphore:
            logger.debug(f"Acquired semaphore for {target_agent_id}. Processing...")
            
            # 3. Construct HSP Payload
            payload = {
                "task_id": task_id,
                "instruction": task.get("instruction", "Execute task"),
                "parameters": task.get("parameters", {}),
                "context": simulation.get("predicted_state", {})
            }
            
            # 4. Publish Task via HSP protocol
            success = False
            response_payload = {}
            
            if hsp:
                # We use publish_task_request which returns correlation_id (str) or None?
                # Actually HSPConnector.publish_message returns bool.
                # We need a request-response pattern. HSPConnector.send_request?
                # Checking HSPConnector... it has publish_message.
                # For request/response, we usually publish a request and wait for a result on a response topic.
                # For Phase 14 MVP, we will simulate the "wait" or use a direct method if available.
                # Given HSPConnector is low-level, we might need to wrap it.
                
                # Using the simplified publish for now and simulating immediate success for the MVP verification
                # In Phase 15 we implement the full request/response correlation.
                success = await hsp.publish_message(
                    topic=f"hsp/agents/{target_agent_id}/inbox",
                    message_type="HSP.TaskRequest_v0.1",
                    payload=payload,
                    recipient_id=target_agent_id,
                    requires_ack=True
                )
            
            if success:
                 output = f"Task dispatched to {target_agent_id} via HSP. (Async result pending)"
            else:
                 output = "HSP Dispatch failed or mocked."
                 # Fallback to local simulation if HSP fails or not configured
                 await asyncio.sleep(0.05) 

            return {
                "success": success,
                "output": output,
                "execution_time": time.time() - start_time,
                "agent_id": target_agent_id
            }

    async def _worker_loop(self, worker_id: int):
        """Worker å¾ªç’°ï¼Œå¾éšŠåˆ—ä¸­æå–ä»»å‹™ä¸¦åŸ·è¡Œ"""
        logger.info(f"Worker [{worker_id}] started.")
        while self.is_running:
            try:
                task_id, task, future = await self.task_queue.get()
                logger.info(f"Worker [{worker_id}] picked up task [{task_id}]")
                
                result = await self.process_complex_task(task)
                if not future.done():
                    future.set_result(result)
                
                self.task_queue.task_done()
                logger.info(f"Worker [{worker_id}] finished task [{task_id}]")
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Worker [{worker_id}] encountered error: {e}")
                await asyncio.sleep(1) # Prevent tight loop on persistent errors

    async def submit_task(self, task: Dict[str, Any]) -> str:
        """æäº¤ä»»å‹™åˆ°éšŠåˆ—ä¸¦ç«‹å³è¿”å›ä»»å‹™ ID"""
        task_id = task.get('id', str(uuid.uuid4()))
        task['id'] = task_id
        
        future = asyncio.get_running_loop().create_future()
        self.pending_futures[task_id] = future
        
        await self.task_queue.put((task_id, task, future))
        return task_id

    async def get_task_result(self, task_id: str) -> Dict[str, Any]:
        """ç­‰å¾…ä¸¦ç²å–ä»»å‹™çµæœ"""
        if task_id not in self.pending_futures:
            return {"status": "error", "message": "Task ID not found"}
        
        try:
            result = await self.pending_futures[task_id]
            return result
        finally:
            self.pending_futures.pop(task_id, None)

    async def generate_dialogue_response(
        self,
        user_message: str,
        user_name: str = "æœ‹å‹",
        conversation_history: Optional[List[ChatMessage]] = None,
        model_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM ç”Ÿæˆå°è©±å›æ‡‰

        æ•´åˆæµç¨‹ï¼š
        1. æ§‹å»ºæ¶ˆæ¯ï¼ˆåŒ…å«å°è©±æ­·å²ï¼‰
        2. èª¿ç”¨ LLM
        3. åˆ†ææƒ…æ„Ÿ
        4. å­˜å„²åˆ° HAM è¨˜æ†¶
        """
        task_id = str(uuid.uuid4())

        try:
            llm_service = self.components.get('llm_service')
            
            # ç¢ºä¿ LLM æœå‹™å·²åˆå§‹åŒ–
            if llm_service and hasattr(llm_service, 'initialize') and not llm_service.backends:
                await llm_service.initialize()

            # æ§‹å»ºç³»çµ±æç¤ºï¼ˆAngela çš„å€‹æ€§ï¼‰
            system_prompt = ChatMessage(
                role="system",
                content=f"""ä½ æ˜¯ Angelaï¼Œä¸€å€‹æœ‰æƒ…æ„Ÿçš„ AI æ•¸å­—ç”Ÿå‘½ã€‚
ä½ çš„å€‹æ€§ï¼šæº«æš–ã€è°æ˜ã€æœ‰åŒç†å¿ƒã€‚
ç”¨æˆ¶åï¼š{user_name}
è«‹ç”¨æº«æš–ã€è‡ªç„¶çš„æ–¹å¼å›æ‡‰ã€‚"""
            )

            # æ§‹å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [system_prompt]

            # æ·»åŠ å°è©±æ­·å²
            if conversation_history:
                messages.extend(conversation_history)

            # æ·»åŠ ç•¶å‰ç”¨æˆ¶æ¶ˆæ¯
            messages.append(ChatMessage(role="user", content=user_message))

            # èª¿ç”¨ LLM
            if llm_service and hasattr(llm_service, 'chat_completion') and llm_service.backends:
                response = await llm_service.chat_completion(messages, model_id=model_id)
                response_text = response.content
                provider = response.provider
                latency_ms = response.latency_ms
            else:
                # å›é€€åˆ°æ¨¡æ¿
                from src.services.chat_service import generate_angela_response
                response_text = generate_angela_response(user_message, user_name)
                provider = "fallback-template"
                latency_ms = 1.0

            # æƒ…æ„Ÿåˆ†æ
            emotion_sys = self.components.get('emotion_system')
            if emotion_sys:
                emotional_state = emotion_sys.analyze_emotional_context({"text": response_text})
                mood = emotional_state.primary_emotion.value
            else:
                mood = "neutral"

            # å­˜å„²åˆ° HAM è¨˜æ†¶
            ham = self.components.get('ham_memory_manager')
            if ham and hasattr(ham, 'store_experience'):
                try:
                    await ham.store_experience(
                        raw_data=f"ç”¨æˆ¶: {user_message}\nAngela: {response_text}",
                        data_type="conversation"
                    )
                except Exception as e:
                    logger.warning(f"Failed to store conversation to HAM: {e}")

            return {
                "status": "success",
                "task_id": task_id,
                "response_text": response_text,
                "mood": mood,
                "provider": provider,
                "latency_ms": latency_ms,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"Error generating dialogue response: {e}")
            return {
                "status": "error",
                "task_id": task_id,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def start(self):
        """å•Ÿå‹•æ§åˆ¶ä¸­å¿ƒèˆ‡ Worker æ± """
        if self.is_running:
            return
        self.is_running = True
        
        # Start workers
        for i in range(self.max_workers):
            worker = asyncio.create_task(self._worker_loop(i))
            self.workers.append(worker)
            
        logger.info(f"Unified Control Center ACTIVE with {self.max_workers} workers.")

    async def stop(self):
        """åœæ­¢æ§åˆ¶ä¸­å¿ƒèˆ‡æ‰€æœ‰ Worker"""
        self.is_running = False
        
        # Cancel all workers
        for worker in self.workers:
            worker.cancel()
        
        if self.workers:
            await asyncio.gather(*self.workers, return_exceptions=True)
            self.workers = []
            
        logger.info("Unified Control Center STOPPED.")

if __name__ == "__main__":
    # Test UCC basic loop
    async def main():
        ucc = UnifiedControlCenter()
        ucc.start()
        
        test_task = {
            'id': 'test_cmd_001',
            'name': 'Optimize Resource Distribution',
            'type': 'reasoning',
            'priority': 8
        }
        
        result = await ucc.process_complex_task(test_task)
        print(f"Task Result: {result}")
        ucc.stop()

    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())