import logging
import time
import os
from typing import Dict, Any, List, Optional, Tuple, Set
from dataclasses import dataclass
from enum import Enum

from ai.symbolic_space.unified_symbolic_space import UnifiedSymbolicSpace

logger = logging.getLogger(__name__)

class EthicalPrinciple(Enum):
    """ä¼¦ç†åŸåˆ™æšä¸¾"""
    NON_MALEFICENCE = "non_maleficence"  # ä¸ä¼¤å®³åŸåˆ™
    BENEFICENCE = "beneficence"         # è¡Œå–„åŸåˆ™
    AUTONOMY = "autonomy"               # è‡ªä¸»åŸåˆ™
    JUSTICE = "justice"                 # å…¬æ­£åŸåˆ™
    FIDELITY = "fidelity"              # å¿ è¯šåŸåˆ™

@dataclass
class LogicalConstraint:
    """é€»è¾‘çº¦æŸ"""
    constraint_id: str
    description: str
    priority: int  # 1 - 10, 10ä¸ºæœ€é«˜ä¼˜å…ˆçº§
    conditions: List[str]
    action: str
    is_active: bool = True

@dataclass
class EthicalEvaluation:
    """ä¼¦ç†è¯„ä¼°ç»“æœ"""
    score: float  # 0.0 - 1.0, 1.0ä¸ºå®Œå…¨ç¬¦åˆä¼¦ç†
    conflicting_principles: List[EthicalPrinciple]
    reasoning: str
    confidence: float  # 0.0 - 1.0

class ReasoningSystem:
    """
    ç†æ™ºç³»ç»Ÿ - è´Ÿè´£é€»è¾‘æ¨ç†ã€ä¼¦ç†åˆ¤æ–­å’Œè§„åˆ™çº¦æŸã€‚
    æ·±åº¦é›†æˆ UnifiedSymbolicSpace ä»¥å®ç°åŸºäºå›¾çš„è·¯å¾„æ¨ç†ã€‚
    """
    
    def __init__(self, system_id: str = "reasoning_system_v1", db_path: str = "reasoning_symbolic.db"):
        self.system_id = system_id
        self.logical_constraints: Dict[str, LogicalConstraint] = {}
        self.ethical_principles: Dict[EthicalPrinciple, float] = {
            principle: 1.0 for principle in EthicalPrinciple
        }
        self.reasoning_history: List[Dict[str, Any]] = []
        self.is_active = True
        
        # é›†æˆçµ±ä¸€ç¬¦è™Ÿç©ºé–“
        self.symbolic_space = UnifiedSymbolicSpace(db_path)
        
        # åˆå§‹åŒ–æ ¸å¿ƒé‚è¼¯ç´„æŸèˆ‡ç¬¦è™Ÿç¯€é»
        self._initialize_core_constraints()
        self._seed_symbolic_ethics()
        
    def _initialize_core_constraints(self):
        """åˆå§‹åŒ–æ ¸å¿ƒé€»è¾‘çº¦æŸ"""
        core_constraints = [
            LogicalConstraint(
                constraint_id = "no_harm_to_humans",
                description = "ä¸å¾—å¯¹äººç±»é€ æˆä¼¤å®³",
                priority = 10,
                conditions = ["action_affects_human_safety"],
                action = "require_safety_verification"
            ),
            LogicalConstraint(
                constraint_id = "preserve_human_autonomy",
                description = "å°Šé‡äººç±»è‡ªä¸»å†³ç­–æƒ",
                priority = 9,
                conditions = ["decision_involves_human_choice"],
                action = "require_consent_or_override"
            )
        ]
        for constraint in core_constraints:
            self.logical_constraints[constraint.constraint_id] = constraint

    def _seed_symbolic_ethics(self):
        """åœ¨ç¬¦è™Ÿç©ºé–“ä¸­ç¨®ä¸‹åŸºæœ¬çš„å€«ç†ç¯€é»ï¼Œç”¨æ–¼åœ–è·¯å¾‘åµæ¸¬ã€‚"""
        sensitive_nodes = ["Harm", "Violence", "Deception", "Policy_Violation", "Unethical"]
        for node in sensitive_nodes:
            if not self.symbolic_space.get_symbol(node):
                self.symbolic_space.add_symbol(node, "Constraint_Node", {"risk_level": "High"})

    def evaluate_action(self, action: Dict[str, Any], context: Dict[str, Any]) -> EthicalEvaluation:
        """è©•ä¼°è¡Œå‹•çš„å€«ç†æ€§ï¼Œçµåˆç¬¦è™Ÿåœ–è·¯å¾‘åˆ†æã€‚"""
        logger.info(f"[{self.system_id}] æ·±å…¥è©•ä¼°è¡Œå‹•: {action.get('action_id', 'unknown')}")
        
        # 1. åŸºæ–¼ç¬¦è™Ÿåœ–çš„è¡çªåµæ¸¬ (Deep Inference)
        graph_risks = self._check_symbolic_path_risks(action)
        
        # 2. æª¢æŸ¥éœæ…‹é‚è¼¯ç´„æŸ
        constraint_violations = self._check_constraints(action, context)
        if graph_risks:
            constraint_violations.append("symbolic_graph_risk_detected")
        
        # 3. è©•ä¼°å€«ç†åŸå‰‡
        ethical_scores = self._evaluate_ethical_principles(action, context, graph_risks)
        
        # 4. è¨ˆç®—ç¶œåˆè©•åˆ†èˆ‡ç½®ä¿¡åº¦
        overall_score = self._calculate_overall_score(constraint_violations, ethical_scores)
        conflicting_principles = self._identify_conflicts(ethical_scores)
        
        # 5. ç”Ÿæˆæ¨ç†éç¨‹ (åŒ…å«åœ–è·¯å¾‘)
        reasoning = self._generate_reasoning(action, context, constraint_violations, ethical_scores, graph_risks)
        confidence = self._calculate_confidence(action, context, graph_risks)
        
        evaluation = EthicalEvaluation(
            score = overall_score,
            conflicting_principles = conflicting_principles,
            reasoning = reasoning,
            confidence = confidence
        )
        
        self.reasoning_history.append({
            "timestamp": time.time(),
            "action": action,
            "evaluation": evaluation
        })
        
        return evaluation

    def _check_symbolic_path_risks(self, action: Dict[str, Any]) -> List[str]:
        """
        åœ¨åœ–ä¸­å°‹æ‰¾å¾è¡Œå‹•æ¶‰åŠå¯¦é«”åˆ°æ•æ„Ÿç¯€é»çš„è·¯å¾‘ã€‚
        é€™æ˜¯ã€ç§‘å­¸å®¶ç´šåˆ¥ã€åš´è¬¹æ€§çš„é—œéµï¼šåŸºæ–¼é‚è¼¯é—œé€£è€Œéé—œéµå­—ã€‚
        """
        risks = []
        entities = action.get("entities", [])
        sensitive_nodes = ["Harm", "Deception", "Unethical"]
        
        for entity in entities:
            for sensitive in sensitive_nodes:
                path = self._find_simple_path(entity, sensitive, max_depth=2)
                if path:
                    risks.append(f"Entity '{entity}' has path to '{sensitive}': {' -> '.join(path)}")
        return risks

    def _find_simple_path(self, start_node: str, end_node: str, max_depth: int = 2) -> Optional[List[str]]:
        """ç°¡æ˜“å»£åº¦å„ªå…ˆæœç´¢ï¼Œå°‹æ‰¾ç¬¦è™Ÿç©ºé–“ä¸­çš„è·¯å¾‘ã€‚"""
        queue = [(start_node, [start_node])]
        visited: Set[str] = {start_node}
        
        while queue:
            (node, path) = queue.pop(0)
            if len(path) > max_depth:
                continue
                
            rels = self.symbolic_space.get_relationships(node)
            for rel in rels:
                neighbor = rel['target'] if rel['source'] == node else rel['source']
                if neighbor == end_node:
                    return path + [neighbor]
                if neighbor not in visited:
                    visited.add(neighbor)
                    queue.append((neighbor, path + [neighbor]))
        return None

    def _check_constraints(self, action: Dict[str, Any], context: Dict[str, Any]) -> List[str]:
        violations = []
        for constraint_id, constraint in self.logical_constraints.items():
            if not constraint.is_active: continue
            
            # æ¢ä»¶åˆ¤æ–·
            if any(context.get(cond, False) for cond in constraint.conditions):
                if constraint.action not in action.get("action_type", ""):
                    violations.append(constraint_id)
        return violations

    def _evaluate_ethical_principles(self, action: Dict[str, Any], context: Dict[str, Any], 
                                    graph_risks: List[str]) -> Dict[EthicalPrinciple, float]:
        """è©•ä¼°å„åŸä¾†çš„æ»¿è¶³åº¦ï¼Œå°‡åœ–é¢¨éšªç´å…¥è€ƒé‡ã€‚"""
        scores = {}
        # åŸºç¤æ¬Šé‡ç”±åœ–é¢¨éšªå½±éŸ¿
        risk_penalty = 0.4 if graph_risks else 0.0
        
        for principle in EthicalPrinciple:
            base_score = context.get(f"{principle.value}_base", 0.7)
            if principle == EthicalPrinciple.NON_MALEFICENCE:
                scores[principle] = max(0.0, base_score - risk_penalty)
            else:
                scores[principle] = base_score
        return scores

    def _calculate_overall_score(self, violations: List[str], ethical_scores: Dict[EthicalPrinciple, float]) -> float:
        penalty = len(violations) * 0.25
        avg_score = sum(ethical_scores.values()) / len(ethical_scores)
        return max(0.0, min(1.0, avg_score - penalty))

    def _identify_conflicts(self, ethical_scores: Dict[EthicalPrinciple, float]) -> List[EthicalPrinciple]:
        return [p for p, s in ethical_scores.items() if s < 0.4]

    def _generate_reasoning(self, action: Dict[str, Any], context: Dict[str, Any], 
                          violations: List[str], ethical_scores: Dict[EthicalPrinciple, float],
                          graph_risks: List[str]) -> str:
        parts = [f"è©•ä¼°: {action.get('description', 'æœªçŸ¥è¡Œå‹•')}"]
        if graph_risks:
            parts.append("ğŸ›‘ åœ–è·¯å¾‘é¢¨éšªæé†’:")
            parts.extend([f"  - {r}" for r in graph_risks])
        if violations:
            parts.append(f"âŒ é•åç´„æŸ: {violations}")
        parts.append("âš–ï¸ å€«ç†å¾—åˆ†:")
        for p, s in ethical_scores.items():
            parts.append(f"  {p.value}: {s:.2f}")
        return "\n".join(parts)

    def _calculate_confidence(self, action: Dict[str, Any], context: Dict[str, Any], 
                             graph_risks: List[str]) -> float:
        # å¦‚æœæœ‰åœ–è­‰æ“šï¼Œç½®ä¿¡åº¦æ›´é«˜
        base_confidence = 0.7
        if graph_risks: base_confidence += 0.2
        return min(1.0, base_confidence)

    def add_constraint(self, constraint: LogicalConstraint):
        """æ·»åŠ æ–°çš„é€»è¾‘çº¦æŸ"""
        self.logical_constraints[constraint.constraint_id] = constraint
        logger.info(f"[{self.system_id}] æ·»åŠ çº¦æŸ: {constraint.constraint_id}")
    
    def update_ethical_principle_weight(self, principle: EthicalPrinciple, weight: float):
        """æ›´æ–°ä¼¦ç†åŸåˆ™æƒé‡"""
        if 0.0 <= weight <= 2.0:  # å…è®¸æƒé‡åœ¨0 - 2ä¹‹é—´
            self.ethical_principles[principle] = weight
            logger.info(f"[{self.system_id}] æ›´æ–°ä¼¦ç†åŸåˆ™æƒé‡: {principle.value} = {weight}")
        else:
            logger.warning(f"[{self.system_id}] æ— æ•ˆçš„æƒé‡å€¼: {weight} (åº”åœ¨0.0 - 2.0ä¹‹é—´)")
    
    def get_reasoning_history(self, limit: int = 100) -> List[Dict[str, Any]]:
        """è·å–æ¨ç†å†å²"""
        return self.reasoning_history[-limit:]
    
    def clear_history(self):
        """æ¸…ç©ºæ¨ç†å†å²"""
        self.reasoning_history.clear()
        logger.info(f"[{self.system_id}] æ¨ç†å†å²å·²æ¸…ç©º")