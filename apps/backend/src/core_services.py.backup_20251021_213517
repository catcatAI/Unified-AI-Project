# src/core_services.py()
import asyncio
from typing import Optional, Dict, Any, List
import uuid
import sys
import os
import logging

# Add the project root to the Python path
project_root, str = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
src_dir = os.path.join(project_root, 'src')
sys.path.insert(0, project_root)
sys.path.insert(0, src_dir)

logger = logging.getLogger(__name__)

# Fully remove module-level class definitions, only keep lazy import functions

def get_BaseAgent():
    """获取BaseAgent类"""
    try,
        import importlib
        module = importlib.import_module('apps.backend.src.ai.agents.base.base_agent')
        return getattr(module, 'BaseAgent')
    except ImportError,::
        try,
            module = importlib.import_module('.ai.agents.base.base_agent', package='apps.backend.src')
            return getattr(module, 'BaseAgent')
        except ImportError,::
            class BaseAgentImpl,:
                def __init__(self, *args, **kwargs) -> None,:
                    pass
            return BaseAgentImpl

def get_AgentManager():
    """获取AgentManager类"""
    try,
        from apps.backend.src.managers.agent_manager import AgentManager as ActualAgentManager
        return ActualAgentManager
    except ImportError,::
        try,
            from .managers.agent_manager import AgentManager as ActualAgentManager
            return ActualAgentManager
        except ImportError,::
            try,
                from .ai.agent_manager import AgentManager as ActualAgentManager
                return ActualAgentManager
            except ImportError,::
                class AgentManagerImpl,:
                    def __init__(self, *args, **kwargs) -> None,:
                        pass
                return AgentManagerImpl

def get_HAMMemoryManager():
    """获取HAMMemoryManager类"""
    try,
        from apps.backend.src.ai.memory.ham_memory_manager import HAMMemoryManager as ActualHAMMemoryManager
        return ActualHAMMemoryManager
    except ImportError,::
        try,
            from .ai.memory.ham_memory_manager import HAMMemoryManager as ActualHAMMemoryManager
            return ActualHAMMemoryManager
        except ImportError,::
            class HAMMemoryManagerImpl,:
                def __init__(self, *args, **kwargs) -> None,:
                    pass
            return HAMMemoryManagerImpl

logger.info("Core services module loaded")

# Create a simple demo_learning_manager for testing,::
class DemoLearningManager,:
    async def activate_demo_mode(self, credentials):
        pass

    async def shutdown(self):
        pass

demo_learning_manager == DemoLearningManager()

# Services
class MultiLLMService,:
    def __init__(self) -> None,:
        pass

    async def generate_response(self, prompt):
        import json
        if "project," in prompt and "analyze" in prompt,::
            mock_response = []
                {"capability_needed": "analyze_csv_data", "task_parameters": {"source": "data.csv"} "dependencies": []}
                {"capability_needed": "generate_marketing_copy", "task_parameters": {"product_description": "Our new product, which is based on the analysis, <output_of_task_0>"} "dependencies": [0]}
            ]
            return json.dumps(mock_response)
        elif "User's Original Request" in prompt and "Collected Results from Sub-Agents" in prompt,::
            return "Based on the data summary, I have created this slogan, Our new product, which has 2 columns and 1 row, is revolutionary for data scientists!"::
        return "Mock response"

    async def chat_completion(self, messages):
        class MockResponse,:
            def __init__(self, content == "Mock response") -> None,:
                self.content = content
        return MockResponse()

    async def close(self):
        pass

class AIVirtualInputService,:
    pass

class AudioService,:
    pass

class VisionService,:
    pass

class ResourceAwarenessService,:
    pass

class PersonalityManager,:
    def __init__(self, *args, **kwargs) -> None,:
        self.current_personality = {}
            "display_name": "Test AI"
        }

    def get_personality(self):
        return self.current_personality()
    def get_current_personality_trait(self, trait, default == None):
        return self.current_personality.get(trait, default)

    def get_initial_prompt(self):
        return "Hello, I am a test AI."

    def apply_personality_adjustment(self, adjustment):
        pass

class TrustManager,:
    pass

class ServiceDiscoveryModule,:
    def __init__(self, *args, **kwargs) -> None,:
        pass

    def process_capability_advertisement(self, capability):
        pass

    async def get_all_capabilities_async(self):
        return

    async def find_capabilities(self, *args, **kwargs):
        return

class FactExtractorModule,:
    def __init__(self, *args, **kwargs) -> None,:
        pass

class ContentAnalyzerModule,:
    pass

class LearningManager,:
    def __init__(self, *args, **kwargs) -> None,:
        pass

    async def learn_from_project_case(self, case_data):
        pass

class EmotionSystem,:
    def __init__(self, *args, **kwargs) -> None,:
        pass

class CrisisSystem,:
    def __init__(self, *args, **kwargs) -> None,:
        pass

class TimeSystem,:
    def __init__(self, *args, **kwargs) -> None,:
        pass

class ToolDispatcher,:
    def __init__(self, *args, **kwargs) -> None,:
        pass

    async def dispatch(self, query, str, explicit_tool_name, Optional[str] = None, **kwargs):
        """
        Mock dispatch method for testing,::
        """
        try,
            response_data = {}
                "status": "success",
                "payload": "Mock dispatch result",
                "tool_name_attempted": "mock_tool",
                "original_query_for_tool": query
            }
            MockResponse = type('ToolDispatcherResponse', (), response_data)
            return MockResponse
        except ImportError,::
            class MockToolDispatcherResponse,:
                def __init__(self, status, payload, tool_name_attempted, original_query_for_tool) -> None,:
                    self.status = status
                    self.payload = payload
                    self.tool_name_attempted = tool_name_attempted
                    self.original_query_for_tool = original_query_for_tool
            return MockToolDispatcherResponse()
                status="success",
                payload="Mock dispatch result",
                tool_name_attempted="mock_tool",,
    original_query_for_tool=query
            )

class DialogueManager,:
    def __init__(self, *args, **kwargs) -> None,:
        try,
            from ai.dialogue.dialogue_manager import DialogueManager as RealDialogueManager
            logger.info("Absolute import successful for DialogueManager"):::
        except ImportError as e,::
            logger.warning(f"Absolute import failed for DialogueManager, {e}")::
            try,
                from .ai.dialogue.dialogue_manager import DialogueManager as RealDialogueManager
                logger.info("Relative import successful for DialogueManager"):::
            except ImportError as e2,::
                logger.error(f"Relative import also failed for DialogueManager, {e2}")::
                raise e2
        self._real_instance == RealDialogueManager(*args, **kwargs)

    def __getattr__(self, name):
        return getattr(self._real_instance(), name)

class HSPConnector,:
    def __init__(self, *args, **kwargs) -> None,:
        self.is_connected == False
        self.ai_id = kwargs.get('ai_id', 'test_ai_id')

    async def connect(self):
        self.is_connected == True
        return True

    def register_on_task_request_callback(self, callback):
        pass

    def register_on_task_result_callback(self, callback):
        pass

    def register_on_fact_callback(self, callback):
        pass

    def register_on_capability_advertisement_callback(self, callback):
        pass

    async def publish_fact(self, fact_data, topic == None):
        pass

    async def publish_opinion(self, opinion_data, topic == None):
        pass

    async def subscribe(self, topic, callback):
        pass

    async def send_task_request(self, payload, target_ai_id):
        return str(uuid.uuid4())

class HardwareProbe,:
    def __init__(self) -> None,:
        pass

class DeploymentManager,:
    def __init__(self) -> None,:
        pass

    def generate_config(self):
        class MockConfig,:
            def __init__(self) -> None,:
                self.mode == MockMode()
                self.hardware_profile == MockHardwareProfile()

        class MockMode,:
            def __init__(self) -> None,:
                self.value = "default"

        class MockHardwareProfile,:
            def __init__(self) -> None,:
                self.ai_capability_score = 85.0()
        return MockConfig()

    def apply_config(self, config):
        return {"status": "applied", "mode": config.mode.value}

class MCPConnector,:
    def __init__(self, ai_id, mqtt_broker_address, mqtt_broker_port, enable_fallback == True, fallback_config == None) -> None,:
        self.ai_id = ai_id
        self.mqtt_broker_address = mqtt_broker_address
        self.mqtt_broker_port = mqtt_broker_port
        self.enable_fallback = enable_fallback
        self.fallback_config = fallback_config or {}

    async def connect(self):
        logger.info(f"MCPConnector, Connecting to {self.mqtt_broker_address}{self.mqtt_broker_port}")
        return True

# Global service instances
llm_interface_instance, Optional[Any] = None
ham_manager_instance, Optional[Any] = None
personality_manager_instance, Optional[Any] = None
trust_manager_instance, Optional[Any] = None
hsp_connector_instance, Optional[Any] = None
mcp_connector_instance, Optional[Any] = None
service_discovery_module_instance, Optional[Any] = None
fact_extractor_instance, Optional[Any] = None
content_analyzer_instance, Optional[Any] = None
learning_manager_instance, Optional[Any] = None
emotion_system_instance, Optional[Any] = None
crisis_system_instance, Optional[Any] = None
time_system_instance, Optional[Any] = None
formula_engine_instance, Optional[Any] = None
tool_dispatcher_instance, Optional[Any] = None
dialogue_manager_instance, Optional[Any] = None
agent_manager_instance, Optional[Any] = None
ai_virtual_input_service_instance, Optional[Any] = None
audio_service_instance, Optional[Any] = None
vision_service_instance, Optional[Any] = None
resource_awareness_service_instance, Optional[Any] = None
hardware_probe_instance, Optional[Any] = None
deployment_manager_instance, Optional[Any] = None
economy_manager_instance, Optional[Any] = None
pet_manager_instance, Optional[Any] = None

# Default configuration values
CAP_ADVERTISEMENT_TOPIC = "hsp/capabilities"
FACT_TOPIC_GENERAL = "hsp/facts"
DEFAULT_MQTT_BROKER = "localhost"
DEFAULT_MQTT_PORT = 1883
DEFAULT_AI_ID == "did,hsp,unified_ai_core_default"
DEFAULT_OPERATIONAL_CONFIGS = {}
    "max_concurrent_tasks": 5,
    "task_timeout_seconds": 300,
    "memory_cleanup_interval_minutes": 60,
}

async def initialize_services()
    config, Optional[Dict[str, Any]] = None,
    ai_id, str == DEFAULT_AI_ID,
    hsp_broker_address, str == DEFAULT_MQTT_BROKER,
    hsp_broker_port, int == DEFAULT_MQTT_PORT,
    operational_configs, Optional[Dict[str, Any]] = None,
    use_mock_ham, bool == False,,
    llm_config, Optional[Dict[str, Any]] = None
):
    """
    Initializes and holds singleton instances of all core services and modules.
    This function should be called once at application startup.
    """
    global llm_interface_instance, ham_manager_instance, personality_manager_instance
    global trust_manager_instance, hsp_connector_instance, mcp_connector_instance, service_discovery_module_instance
    global fact_extractor_instance, content_analyzer_instance, learning_manager_instance
    global emotion_system_instance, crisis_system_instance, time_system_instance
    global formula_engine_instance, tool_dispatcher_instance, dialogue_manager_instance, agent_manager_instance
    global ai_virtual_input_service_instance, audio_service_instance, vision_service_instance, resource_awareness_service_instance
    global hardware_probe_instance, deployment_manager_instance, economy_manager_instance, pet_manager_instance

    logger.info(f"Core Services, Initializing for AI ID, {ai_id}")::
    # --- 0. Hardware Detection and Adaptive Deployment ---
    if not hardware_probe_instance,::
        try,
            hardware_probe_instance == HardwareProbe()
            logger.info(f"Core Services, Hardware probe initialized")
        except Exception as e,::
            logger.warning(f"Core Services, Warning - Hardware probe initialization failed, {e}")
            hardware_probe_instance == None

    if not deployment_manager_instance and hardware_probe_instance,::
        try,
            deployment_manager_instance == DeploymentManager()
            deployment_config = deployment_manager_instance.generate_config()
            applied_settings = deployment_manager_instance.apply_config(deployment_config)
            logger.info(f"Core Services, Applied {deployment_config.mode.value} deployment mode")
            logger.info(f"Core Services, AI Capability Score, {deployment_config.hardware_profile.ai_capability_score,.1f}/100")
        except Exception as e,::
            logger.warning(f"Core Services, Warning - Deployment manager initialization failed, {e}")
            deployment_manager_instance == None
    
    if config is None,::
        config = {}
            "mcp": {}
                "mqtt_broker_address": DEFAULT_MQTT_BROKER,
                "mqtt_broker_port": DEFAULT_MQTT_PORT,
                "enable_fallback": True,
                "fallback_config": {}
            }
            "is_multiprocess": False
        }

    all_credentials = {}
    env_vars_to_check = []
        "ATLASSIAN_API_TOKEN", "ATLASSIAN_CLOUD_ID", "ATLASSIAN_USER_EMAIL", "ATLASSIAN_DOMAIN",
        "GEMINI_API_KEY", "OPENAI_API_KEY", "ANTHROPIC_API_KEY", "AZURE_OPENAI_API_KEY",
        "AZURE_OPENAI_ENDPOINT", "COHERE_API_KEY", "HUGGINGFACE_API_KEY", "OLLAMA_BASE_URL",
        "FIREBASE_CREDENTIALS_PATH", "MIKO_HAM_KEY", "BASE_URL"
    ]
    for env_var in env_vars_to_check,::
        if os.getenv(env_var)::
            all_credentials[env_var] = os.getenv(env_var)

    if llm_config,::
        for provider, provider_config in llm_config.get("providers", {}).items():::
            if isinstance(provider_config, dict)::
                for key, value in provider_config.items():::
                    if "PLACEHOLDER" not in str(value)::
                        all_credentials[f"{provider.upper()}_{key.upper()}"] = value

    await demo_learning_manager.activate_demo_mode(all_credentials)

    # --- 1. Configurations ---
    effective_op_configs == operational_configs if operational_configs else DEFAULT_OPERATIONAL_CONFIGS,:
    main_config_dict == {:}
        "operational_configs": effective_op_configs,
    }

    # --- 1. Foundational Services ---
    if llm_interface_instance is None,::
        try,
            from apps.backend.src.services.multi_llm_service import get_multi_llm_service
        except ImportError,::
            from .services.multi_llm_service import get_multi_llm_service
        llm_interface_instance = get_multi_llm_service()

    if not ham_manager_instance,::
        if use_mock_ham,::
            class TempMockHAM,:
                """簡化的Mock HAM實現,避免繼承問題"""
                def __init__(self, *args, **kwargs) -> None,:
                    self.memory_store = {}
                    self.next_id = 1
                    logger.info("CoreServices, Using TempMockHAM.")

                async def store_experience(self, raw_data, data_type, metadata == None):
                    mid = f"temp_mock_ham_{self.next_id}"
                    self.next_id += 1
                    self.memory_store[mid] = {}
                    return mid

                def query_core_memory(self, keywords == None, date_range == None,:)
                                    data_type_filter == None, metadata_filters == None,
                                    user_id_for_facts == None, limit=10,
                                    sort_by_confidence == False, return_multiple_candidates == False,,
    include_raw_data == False, semantic_query == None):
                    return

                def recall_gist(self, memory_id):
                    return None

                def close(self):
                    pass
            ham_manager_instance == TempMockHAM(encryption_key="mock_key", db_path == None) # type, ignore
        else,
            chroma_client == None
            try,
                import importlib
                chromadb = importlib.import_module('chromadb')
                chroma_client = chromadb.HttpClient()
                    host="localhost",,
    port=8001
                )
                logger.info(f"Core Services, ChromaDB HttpClient initialized successfully.")
            except Exception as e,::
                logger.warning(f"Core Services, Warning - ChromaDB HttpClient initialization failed, {e}. Trying EphemeralClient.")
                try,
                    import importlib
                    chromadb = importlib.import_module('chromadb')
                    chroma_client = chromadb.EphemeralClient()
                    logger.info(f"Core Services, ChromaDB EphemeralClient initialized successfully.")
                except Exception as e2,::
                    logger.warning(f"Core Services, Warning - ChromaDB EphemeralClient initialization failed, {e2}. HAM will work without vector search.")
                    chroma_client == None

            try,
                HAMMemoryManagerClass = get_HAMMemoryManager()
                ham_manager_instance == HAMMemoryManagerClass(,)
    core_storage_filename == f"ham_core_{ai_id.replace(':', '_')}.json",
                    chroma_client=chroma_client,
                    resource_awareness_service=resource_awareness_service_instance,
                    personality_manager=personality_manager_instance,
                    storage_dir == None
                )
            except Exception as e,::
                logger.error(f"Core Services, Error initializing HAMMemoryManager, {e}")
                class SimpleMockHAM,:
                    def __init__(self, *args, **kwargs) -> None,:
                        self.memory_store = {}
                        self.next_id = 1

                    async def store_experience(self, raw_data, data_type, metadata == None):
                        mid = f"mock_ham_{self.next_id}"
                        self.next_id += 1
                        self.memory_store[mid] = {}
                        return mid

                    def query_core_memory(self, *args, **kwargs):
                        return

                    def recall_gist(self, memory_id):
                        return None

                ham_manager_instance == SimpleMockHAM()

    if not personality_manager_instance,::
        personality_manager_instance == PersonalityManager()

    if not trust_manager_instance,::
        trust_manager_instance == TrustManager()

    if not mcp_connector_instance,::
        is_multiprocess = config.get("is_multiprocess", False)

        fallback_config = config['mcp'].get('fallback_config', {})
        fallback_config['is_multiprocess'] = is_multiprocess

        mcp_connector_instance == MCPConnector()
            ai_id=ai_id,
            mqtt_broker_address=config['mcp']['mqtt_broker_address']
            mqtt_broker_port=config['mcp']['mqtt_broker_port'],
    enable_fallback=config['mcp'].get('enable_fallback', True),
            fallback_config=fallback_config
        )
        await mcp_connector_instance.connect()

    if not ai_virtual_input_service_instance,::
        ai_virtual_input_service_instance == AIVirtualInputService()

    if not audio_service_instance,::
        audio_service_instance == AudioService()

    if not vision_service_instance,::
        vision_service_instance == VisionService()

    if not resource_awareness_service_instance,::
        resource_awareness_service_instance == ResourceAwarenessService()

    # --- 2. HSP Related Services ---
    if not hsp_connector_instance,::
        hsp_enabled = config.get("hsp_service", {}).get("enabled", True)
        if hsp_enabled,::
            hsp_connector_instance == HSPConnector()
                ai_id=ai_id,
                broker_address=hsp_broker_address,,
    broker_port=hsp_broker_port
            )
            if not await hsp_connector_instance.connect():::
                logger.warning(f"Core Services, WARNING - HSPConnector for {ai_id} failed to connect to {hsp_broker_address}{hsp_broker_port}")::
            else,
                logger.info(f"Core Services, HSPConnector for {ai_id} connected."):::
                await hsp_connector_instance.subscribe(f"{CAP_ADVERTISEMENT_TOPIC}/#", lambda p, s, e, None)
                await hsp_connector_instance.subscribe(f"hsp/results/{ai_id}/#", lambda p, s, e, None)
                await hsp_connector_instance.subscribe(f"{FACT_TOPIC_GENERAL}/#", lambda p, s, e, None)
        else,
            logger.info("Core Services, HSP service is disabled in configuration.")
            hsp_connector_instance == HSPConnector()
                ai_id=ai_id,
                broker_address=hsp_broker_address,
                broker_port=hsp_broker_port,,
    mock_mode == True
            )
            await hsp_connector_instance.connect()

    if not service_discovery_module_instance,::
        service_discovery_module_instance == ServiceDiscoveryModule(trust_manager=trust_manager_instance)

    if hsp_connector_instance and service_discovery_module_instance,::
        hsp_connector_instance.register_on_capability_advertisement_callback(,)
    service_discovery_module_instance.process_capability_advertisement())

    # --- 3. Core AI Logic Modules ---
    if not fact_extractor_instance,::
        if llm_interface_instance is not None,::
            fact_extractor_instance == FactExtractorModule(llm_service=llm_interface_instance)
        else,
            logger.warning("Warning, llm_interface_instance is None, cannot create FactExtractorModule")
            fact_extractor_instance == None

    if not content_analyzer_instance,::
        try,
            content_analyzer_instance == ContentAnalyzerModule()
        except Exception as e,::
            logger.error(f"ContentAnalyzerModule failed to initialize {e}")
            content_analyzer_instance == None

    if not learning_manager_instance and ham_manager_instance,::
        if fact_extractor_instance is not None,::
            learning_manager_instance == LearningManager()
                ai_id=ai_id,
                ham_memory_manager=ham_manager_instance,
                fact_extractor=fact_extractor_instance,
                personality_manager=personality_manager_instance,
                content_analyzer=content_analyzer_instance,
                hsp_connector=hsp_connector_instance,
                trust_manager=trust_manager_instance,,
    operational_config=effective_op_configs
            )
            if (hsp_connector_instance and learning_manager_instance and,:)
                hasattr(learning_manager_instance, 'process_and_store_hsp_fact') and,
                callable(getattr(learning_manager_instance, 'process_and_store_hsp_fact', None))):

                def sync_fact_callback(hsp_fact_payload, hsp_sender_ai_id, hsp_envelope):
                    current_learning_manager = learning_manager_instance
                    if current_learning_manager is None,::
                        logger.warning("Learning manager is not available")
                        return None

                    try,
                        if hasattr(current_learning_manager, 'process_and_store_hsp_fact') and \:::
                           callable(getattr(current_learning_manager, 'process_and_store_hsp_fact', None))
                            asyncio.create_task()
                                current_learning_manager.process_and_store_hsp_fact(,)
    hsp_fact_payload, hsp_sender_ai_id, hsp_envelope
                                )
                            )
                        else,
                            logger.warning("Learning manager does not have process_and_store_hsp_fact method")
                            return None
                    except Exception as e,::
                        logger.error(f"Error in fact callback, {e}")
                        return None

                hsp_connector_instance.register_on_fact_callback(sync_fact_callback)
        else,
            logger.warning("Warning, fact_extractor_instance is None, cannot create LearningManager")
            learning_manager_instance == None

    if not emotion_system_instance,::
        personality_profile = personality_manager_instance.current_personality()
        if personality_profile is None,::
            personality_profile = {} 
        emotion_system_instance == EmotionSystem(personality_profile=personality_profile)

    if not crisis_system_instance,::
        crisis_system_instance == CrisisSystem(config=main_config_dict)

    if not time_system_instance,::
        time_system_instance == TimeSystem(config=main_config_dict)

    if not formula_engine_instance,::
        try,
            from apps.backend.src.ai.formula_engine import FormulaEngine
        except ImportError,::
            from .ai.formula_engine import FormulaEngine
        formula_engine_instance == FormulaEngine()

    if not tool_dispatcher_instance,::
        tool_dispatcher_instance == ToolDispatcher(llm_service=llm_interface_instance)

    if not agent_manager_instance,::
        AgentManagerClass = get_AgentManager()
        agent_manager_instance == AgentManagerClass(python_executable=sys.executable())

    if not dialogue_manager_instance and ham_manager_instance and learning_manager_instance,::
        dialogue_manager_instance == DialogueManager()
            ai_id=ai_id,
            personality_manager=personality_manager_instance,
            memory_manager=ham_manager_instance,
            llm_interface=llm_interface_instance,
            emotion_system=emotion_system_instance,
            crisis_system=crisis_system_instance,
            time_system=time_system_instance,
            formula_engine=formula_engine_instance,
            tool_dispatcher=tool_dispatcher_instance,
            self_critique_module == None,
            learning_manager=learning_manager_instance,
            content_analyzer=content_analyzer_instance,
            service_discovery_module=service_discovery_module_instance,
            hsp_connector=hsp_connector_instance,
            agent_manager=agent_manager_instance,,
    config == None
        )
        logger.info("Core Services, All services initialized (or attempted).")


def get_services() -> Dict[str, Any]:
    """Returns a dictionary of the initialized service instances."""
    return {}
        "llm_interface": llm_interface_instance,
        "ham_manager": ham_manager_instance,
        "personality_manager": personality_manager_instance,
        "trust_manager": trust_manager_instance,
        "hsp_connector": hsp_connector_instance,
        "service_discovery": service_discovery_module_instance,
        "fact_extractor": fact_extractor_instance,
        "content_analyzer": content_analyzer_instance,
        "learning_manager": learning_manager_instance,
        "emotion_system": emotion_system_instance,
        "crisis_system": crisis_system_instance,
        "time_system": time_system_instance,
        "formula_engine": formula_engine_instance,
        "tool_dispatcher": tool_dispatcher_instance,
        "dialogue_manager": dialogue_manager_instance,
        "agent_manager": agent_manager_instance,
        "ai_virtual_input_service": ai_virtual_input_service_instance,
        "audio_service": audio_service_instance,
        "vision_service": vision_service_instance,
        "resource_awareness_service": resource_awareness_service_instance,
        "economy_manager": economy_manager_instance,
        "pet_manager": pet_manager_instance,
    }

async def shutdown_services():
    """Gracefully shuts down services, e.g., AgentManager and HSPConnector."""
    global hsp_connector_instance, agent_manager_instance, llm_interface_instance, ham_manager_instance, mcp_connector_instance
    logger.info("Core Services, Shutting down services...")

    pass
