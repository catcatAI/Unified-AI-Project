#!/usr/bin/env python3
"""
è®¤çŸ¥çº¦æŸå¼•æ“ (Cognitive Constraint Engine)
Level 5 AGIæ ¸å¿ƒç»„ä»¶ - å®ç°ç›®æ ‡è¯­ä¹‰å»é‡ä¸ä¼˜å…ˆçº§ä¼˜åŒ–

åŠŸèƒ½ï¼š
- ç›®æ ‡è¯­ä¹‰å»é‡ (Target Semantic Deduplication)
- å¿…è¦æ€§è¯„ä¼° (Necessity Assessment)
- ä¼˜å…ˆçº§åŠ¨æ€ä¼˜åŒ– (Dynamic Priority Optimization)
- å†²çªæ£€æµ‹ä¸è§£å†³ (Conflict Detection & Resolution)
- è®¤çŸ¥èµ„æºåˆ†é…ä¼˜åŒ– (Cognitive Resource Allocation Optimization)
"""

import asyncio
import logging
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import json
import re
from pathlib import Path
import hashlib

# å°è¯•å¯¼å…¥å¯é€‰çš„AIåº“
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.cluster import DBSCAN
    from sklearn.preprocessing import StandardScaler
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

# å¯¼å…¥ç»Ÿä¸€çŸ¥è¯†å›¾è°±ï¼ˆå¯é€‰ï¼‰
try:
    import sys
    from pathlib import Path
    project_root = Path(__file__).parent.parent.parent.parent
    sys.path.insert(0, str(project_root))
    from apps.backend.src.core.knowledge.unified_knowledge_graph import Entity, Relation
except ImportError:
    # å ä½ç¬¦å®ç°
    from dataclasses import dataclass
    @dataclass
    class Entity:
        entity_id: str = ""
        name: str = ""
        entity_type: str = ""
        confidence: float = 0.0
        properties: Dict[str, Any] = None
        aliases: List[str] = None
        source: str = ""
        timestamp: datetime = None
    
    @dataclass
    class Relation:
        relation_id: str = ""
        source_entity: str = ""
        target_entity: str = ""
        relation_type: str = ""
        confidence: float = 0.0
        properties: Dict[str, Any] = None
        source: str = ""
        timestamp: datetime = None

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class CognitiveTarget:
    """è®¤çŸ¥ç›®æ ‡"""
    target_id: str
    description: str
    semantic_vector: np.ndarray
    priority: float
    necessity_score: float
    resource_requirements: Dict[str, float]
    dependencies: List[str]
    conflicts: List[str]
    creation_time: datetime
    deadline: Optional[datetime]
    metadata: Dict[str, Any]
    
    def __post_init__(self):
        if not isinstance(self.creation_time, datetime):
            self.creation_time = datetime.fromisoformat(self.creation_time)
        if self.deadline and not isinstance(self.deadline, datetime):
            self.deadline = datetime.fromisoformat(self.deadline)

@dataclass
class SemanticCluster:
    """è¯­ä¹‰èšç±»"""
    cluster_id: str
    centroid_vector: np.ndarray
    target_ids: List[str]
    semantic_coherence: float
    representative_target: str
    cluster_size: int
    creation_time: datetime

@dataclass
class PriorityAssessment:
    """ä¼˜å…ˆçº§è¯„ä¼°"""
    target_id: str
    urgency_score: float
    importance_score: float
    feasibility_score: float
    impact_score: float
    resource_efficiency_score: float
    overall_priority: float
    assessment_time: datetime
    reasoning: List[str]

@dataclass
class ConflictAnalysis:
    """å†²çªåˆ†æ"""
    conflict_id: str
    target_ids: List[str]
    conflict_type: str
    severity: float
    root_causes: List[str]
    resolution_suggestions: List[Dict[str, Any]]
    detection_time: datetime

class CognitiveConstraintEngine:
    """è®¤çŸ¥çº¦æŸå¼•æ“ - Level 5 AGIæ ¸å¿ƒç»„ä»¶"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        
        # ç›®æ ‡å­˜å‚¨
        self.cognitive_targets: Dict[str, CognitiveTarget] = {}
        self.semantic_clusters: Dict[str, SemanticCluster] = {}
        self.priority_assessments: Dict[str, PriorityAssessment] = {}
        self.conflict_analyses: Dict[str, ConflictAnalysis] = {}
        
        # å†å²è®°å½•
        self.target_history: deque = deque(maxlen=1000)
        self.optimization_history: deque = deque(maxlen=500)
        self.conflict_history: deque = deque(maxlen=200)
        
        # AIæ¨¡å‹
        self.semantic_vectorizer = None
        self.priority_predictor = None
        self.conflict_detector = None
        self.necessity_evaluator = None
        
        # é…ç½®å‚æ•°
        self.deduplication_threshold = self.config.get('deduplication_threshold', 0.85)
        self.priority_update_interval = self.config.get('priority_update_interval', 300)  # 5åˆ†é’Ÿ
        self.max_targets_per_cluster = self.config.get('max_targets_per_cluster', 10)
        self.resource_constraint_weight = self.config.get('resource_constraint_weight', 0.3)
        
        # æ€§èƒ½ç›‘æ§
        self.processing_times: Dict[str, List[float]] = defaultdict(list)
        self.optimization_metrics: Dict[str, float] = {}
        
        # åˆå§‹åŒ–AIç»„ä»¶
        self._initialize_ai_components()
        
        logger.info("ğŸ§  è®¤çŸ¥çº¦æŸå¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_ai_components(self):
        """åˆå§‹åŒ–AIç»„ä»¶"""
        try:
            if SKLEARN_AVAILABLE:
                # è¯­ä¹‰å‘é‡åŒ–å™¨
                self.semantic_vectorizer = TfidfVectorizer(
                    max_features=500,
                    ngram_range=(1, 2),
                    analyzer='word',
                    stop_words=None
                )
                
                # ä¼˜å…ˆçº§é¢„æµ‹å™¨
                self.priority_predictor = RandomForestClassifier(
                    n_estimators=100,
                    random_state=42,
                    max_depth=10
                )
                
                # å†²çªæ£€æµ‹å™¨
                self.conflict_detector = LogisticRegression(
                    random_state=42,
                    max_iter=1000
                )
                
                # å¿…è¦æ€§è¯„ä¼°å™¨
                self.necessity_evaluator = RandomForestClassifier(
                    n_estimators=50,
                    random_state=42,
                    max_depth=8
                )
                
                logger.info("âœ… AIç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("âš ï¸ scikit-learnä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç®—æ³•")
                
        except Exception as e:
            logger.error(f"âŒ AIç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
    
    # ==================== ç›®æ ‡è¯­ä¹‰å»é‡ ====================
    
    async def add_cognitive_target(self, target: CognitiveTarget) -> Dict[str, Any]:
        """æ·»åŠ è®¤çŸ¥ç›®æ ‡"""
        try:
            # è¯­ä¹‰å»é‡æ£€æŸ¥
            duplicate_result = await self._check_semantic_duplicates(target)
            
            if duplicate_result['is_duplicate'] and duplicate_result['confidence'] > self.deduplication_threshold:
                # åˆå¹¶ç›®æ ‡è€Œä¸æ˜¯æ·»åŠ æ–°ç›®æ ‡
                merged_target = await self._merge_targets(target, duplicate_result['similar_target'])
                logger.info(f"ğŸ”„ ç›®æ ‡å»é‡åˆå¹¶: {target.target_id} -> {merged_target.target_id}")
                
                return {
                    'action': 'merged',
                    'target_id': merged_target.target_id,
                    'duplicate_info': duplicate_result,
                    'original_target_id': target.target_id
                }
            
            # æ·»åŠ æ–°ç›®æ ‡
            self.cognitive_targets[target.target_id] = target
            
            # æ›´æ–°è¯­ä¹‰èšç±»
            await self._update_semantic_clusters(target)
            
            # å†å²è®°å½•
            self.target_history.append({
                'action': 'added',
                'target_id': target.target_id,
                'timestamp': datetime.now(),
                'semantic_similarity': duplicate_result.get('max_similarity', 0)
            })
            
            logger.info(f"âœ… æ·»åŠ è®¤çŸ¥ç›®æ ‡: {target.target_id}")
            
            return {
                'action': 'added',
                'target_id': target.target_id,
                'duplicate_check': duplicate_result
            }
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ è®¤çŸ¥ç›®æ ‡å¤±è´¥: {e}")
            return {'action': 'failed', 'error': str(e)}
    
    async def _check_semantic_duplicates(self, target: CognitiveTarget) -> Dict[str, Any]:
        """æ£€æŸ¥è¯­ä¹‰é‡å¤"""
        try:
            # ç”Ÿæˆç›®æ ‡è¯­ä¹‰å‘é‡ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if not hasattr(target, 'semantic_vector') or target.semantic_vector is None:
                target.semantic_vector = await self._generate_semantic_vector(target.description)
            
            similarities = []
            most_similar_target = None
            max_similarity = 0.0
            
            # è®¡ç®—ä¸ç°æœ‰ç›®æ ‡çš„ç›¸ä¼¼åº¦
            for existing_id, existing_target in self.cognitive_targets.items():
                if existing_id == target.target_id:
                    continue
                
                # ç”Ÿæˆç°æœ‰ç›®æ ‡è¯­ä¹‰å‘é‡ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                if not hasattr(existing_target, 'semantic_vector') or existing_target.semantic_vector is None:
                    existing_target.semantic_vector = await self._generate_semantic_vector(existing_target.description)
                
                # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
                similarity = await self._calculate_semantic_similarity(
                    target.semantic_vector,
                    existing_target.semantic_vector
                )
                
                similarities.append({
                    'target_id': existing_id,
                    'similarity': similarity,
                    'description': existing_target.description
                })
                
                if similarity > max_similarity:
                    max_similarity = similarity
                    most_similar_target = existing_target
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºé‡å¤
            is_duplicate = max_similarity > self.deduplication_threshold
            
            return {
                'is_duplicate': is_duplicate,
                'confidence': max_similarity,
                'similar_target': most_similar_target,
                'similarities': sorted(similarities, key=lambda x: x['similarity'], reverse=True)[:5]
            }
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰é‡å¤æ£€æŸ¥å¤±è´¥: {e}")
            return {
                'is_duplicate': False,
                'confidence': 0.0,
                'similar_target': None,
                'error': str(e)
            }
    
    async def _generate_semantic_vector(self, description: str) -> np.ndarray:
        """ç”Ÿæˆè¯­ä¹‰å‘é‡"""
        try:
            if SKLEARN_AVAILABLE and self.semantic_vectorizer:
                # ä½¿ç”¨TF-IDFå‘é‡åŒ–
                if not hasattr(self.semantic_vectorizer, 'vocabulary_'):
                    # é¦–æ¬¡ä½¿ç”¨ï¼Œéœ€è¦æ‹Ÿåˆç®€å•çš„è¯æ±‡è¡¨
                    simple_texts = [description]
                    self.semantic_vectorizer.fit(simple_texts)
                
                vector = self.semantic_vectorizer.transform([description]).toarray()[0]
                
                # æ ‡å‡†åŒ–
                norm = np.linalg.norm(vector)
                if norm > 0:
                    vector = vector / norm
                
                return vector
            else:
                # ç®€åŒ–è¯­ä¹‰å‘é‡ç”Ÿæˆ
                words = description.lower().split()
                
                # åŸºäºè¯é¢‘å’Œè¯é•¿ç”Ÿæˆå‘é‡
                word_features = []
                for word in words:
                    word_features.extend([
                        len(word),
                        hash(word) % 100 / 100,  # å“ˆå¸Œç‰¹å¾
                        words.count(word) / len(words)  # è¯é¢‘
                    ])
                
                # å¡«å……åˆ°å›ºå®šç»´åº¦
                target_dim = 100
                if len(word_features) < target_dim:
                    word_features.extend([0.0] * (target_dim - len(word_features)))
                else:
                    word_features = word_features[:target_dim]
                
                vector = np.array(word_features)
                
                # æ ‡å‡†åŒ–
                norm = np.linalg.norm(vector)
                if norm > 0:
                    vector = vector / norm
                
                return vector
                
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›éšæœºå‘é‡ä½œä¸ºåå¤‡
            return np.random.random(100)
    
    async def _calculate_semantic_similarity(self, vector1: np.ndarray, vector2: np.ndarray) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦"""
        try:
            if vector1.shape != vector2.shape:
                # è°ƒæ•´ç»´åº¦
                min_dim = min(vector1.shape[0], vector2.shape[0])
                vector1 = vector1[:min_dim]
                vector2 = vector2[:min_dim]
            
            # ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(vector1, vector2)
            norm1 = np.linalg.norm(vector1)
            norm2 = np.linalg.norm(vector2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return float(max(0, similarity))  # ç¡®ä¿éè´Ÿ
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    async def _merge_targets(self, target1: CognitiveTarget, target2: CognitiveTarget) -> CognitiveTarget:
        """åˆå¹¶ç›®æ ‡"""
        try:
            # ä¿ç•™ä¼˜å…ˆçº§æ›´é«˜çš„ç›®æ ‡ä½œä¸ºä¸»ç›®æ ‡
            if target1.priority > target2.priority:
                primary_target = target1
                secondary_target = target2
            else:
                primary_target = target2
                secondary_target = target1
            
            # åˆå¹¶æè¿°
            merged_description = f"{primary_target.description} (åˆå¹¶è‡ª: {secondary_target.description})"
            
            # åˆå¹¶å±æ€§
            merged_properties = {**primary_target.properties, **secondary_target.properties}
            merged_properties['merged_from'] = secondary_target.target_id
            merged_properties['merge_timestamp'] = datetime.now().isoformat()
            
            # åˆå¹¶ä¾èµ–å…³ç³»
            merged_dependencies = list(set(primary_target.dependencies + secondary_target.dependencies))
            
            # æ›´æ–°ä¸»ç›®æ ‡
            primary_target.description = merged_description
            primary_target.properties = merged_properties
            primary_target.dependencies = merged_dependencies
            primary_target.metadata['is_merged'] = True
            primary_target.metadata['merged_targets'] = [target1.target_id, target2.target_id]
            
            # ç§»é™¤æ¬¡è¦ç›®æ ‡
            if secondary_target.target_id in self.cognitive_targets:
                del self.cognitive_targets[secondary_target.target_id]
            
            logger.info(f"âœ… ç›®æ ‡åˆå¹¶å®Œæˆ: {primary_target.target_id}")
            return primary_target
            
        except Exception as e:
            logger.error(f"âŒ ç›®æ ‡åˆå¹¶å¤±è´¥: {e}")
            return primary_target  # è¿”å›ä¸»ç›®æ ‡ä½œä¸ºåå¤‡
    
    # ==================== è¯­ä¹‰èšç±» ====================
    
    async def _update_semantic_clusters(self, target: CognitiveTarget):
        """æ›´æ–°è¯­ä¹‰èšç±»"""
        try:
            # æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„èšç±»
            best_cluster = None
            best_similarity = 0.0
            
            for cluster_id, cluster in self.semantic_clusters.items():
                similarity = await self._calculate_semantic_similarity(
                    target.semantic_vector,
                    cluster.centroid_vector
                )
                
                if similarity > best_similarity and similarity > 0.6:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    best_similarity = similarity
                    best_cluster = cluster
            
            if best_cluster and len(best_cluster.target_ids) < self.max_targets_per_cluster:
                # æ·»åŠ åˆ°ç°æœ‰èšç±»
                best_cluster.target_ids.append(target.target_id)
                await self._update_cluster_centroid(best_cluster)
                logger.info(f"ğŸ”„ æ·»åŠ åˆ°ç°æœ‰èšç±»: {best_cluster.cluster_id}")
            else:
                # åˆ›å»ºæ–°èšç±»
                new_cluster = await self._create_new_cluster(target)
                self.semantic_clusters[new_cluster.cluster_id] = new_cluster
                logger.info(f"âœ… åˆ›å»ºæ–°èšç±»: {new_cluster.cluster_id}")
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰èšç±»æ›´æ–°å¤±è´¥: {e}")
    
    async def _update_cluster_centroid(self, cluster: SemanticCluster):
        """æ›´æ–°èšç±»ä¸­å¿ƒ"""
        try:
            if not cluster.target_ids:
                return
            
            # è®¡ç®—æ–°çš„ä¸­å¿ƒå‘é‡
            vectors = []
            for target_id in cluster.target_ids:
                if target_id in self.cognitive_targets:
                    target = self.cognitive_targets[target_id]
                    if hasattr(target, 'semantic_vector') and target.semantic_vector is not None:
                        vectors.append(target.semantic_vector)
            
            if vectors:
                cluster.centroid_vector = np.mean(vectors, axis=0)
                cluster.semantic_coherence = await self._calculate_cluster_coherence(cluster)
                cluster.cluster_size = len(cluster.target_ids)
                cluster.representative_target = self._select_representative_target(cluster)
            
        except Exception as e:
            logger.error(f"âŒ èšç±»ä¸­å¿ƒæ›´æ–°å¤±è´¥: {e}")
    
    async def _create_new_cluster(self, target: CognitiveTarget) -> SemanticCluster:
        """åˆ›å»ºæ–°èšç±»"""
        cluster_id = f"cluster_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{target.target_id}"
        
        return SemanticCluster(
            cluster_id=cluster_id,
            centroid_vector=target.semantic_vector.copy() if target.semantic_vector is not None else np.random.random(100),
            target_ids=[target.target_id],
            semantic_coherence=1.0,  # å•ä¸ªç›®æ ‡æ—¶ä¸€è‡´æ€§ä¸º1
            representative_target=target.target_id,
            cluster_size=1,
            creation_time=datetime.now()
        )
    
    def _select_representative_target(self, cluster: SemanticCluster) -> str:
        """é€‰æ‹©ä»£è¡¨æ€§ç›®æ ‡"""
        if not cluster.target_ids:
            return ""
        
        # é€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„ç›®æ ‡ä½œä¸ºä»£è¡¨
        best_target_id = cluster.target_ids[0]
        best_priority = 0.0
        
        for target_id in cluster.target_ids:
            if target_id in self.cognitive_targets:
                target = self.cognitive_targets[target_id]
                if target.priority > best_priority:
                    best_priority = target.priority
                    best_target_id = target_id
        
        return best_target_id
    
    async def _calculate_cluster_coherence(self, cluster: SemanticCluster) -> float:
        """è®¡ç®—èšç±»ä¸€è‡´æ€§"""
        try:
            if len(cluster.target_ids) < 2:
                return 1.0  # å•ä¸ªç›®æ ‡æ—¶ä¸€è‡´æ€§ä¸º1
            
            similarities = []
            for i, target_id1 in enumerate(cluster.target_ids):
                for j, target_id2 in enumerate(cluster.target_ids):
                    if i < j:  # é¿å…é‡å¤è®¡ç®—
                        if (target_id1 in self.cognitive_targets and 
                            target_id2 in self.cognitive_targets):
                            
                            target1 = self.cognitive_targets[target_id1]
                            target2 = self.cognitive_targets[target_id2]
                            
                            if (hasattr(target1, 'semantic_vector') and target1.semantic_vector is not None and
                                hasattr(target2, 'semantic_vector') and target2.semantic_vector is not None):
                                
                                similarity = await self._calculate_semantic_similarity(
                                    target1.semantic_vector,
                                    target2.semantic_vector
                                )
                                similarities.append(similarity)
            
            return np.mean(similarities) if similarities else 0.0
            
        except Exception as e:
            logger.error(f"âŒ èšç±»ä¸€è‡´æ€§è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    # ==================== å¿…è¦æ€§è¯„ä¼° ====================
    
    async def assess_target_necessity(self, target_id: str) -> Dict[str, Any]:
        """è¯„ä¼°ç›®æ ‡å¿…è¦æ€§"""
        try:
            if target_id not in self.cognitive_targets:
                return {'error': 'ç›®æ ‡ä¸å­˜åœ¨'}
            
            target = self.cognitive_targets[target_id]
            
            # å¤šç»´åº¦å¿…è¦æ€§è¯„ä¼°
            novelty_score = await self._assess_novelty(target)
            utility_score = await self._assess_utility(target)
            feasibility_score = await self._assess_feasibility(target)
            alignment_score = await self._assess_alignment(target)
            
            # ç»¼åˆå¿…è¦æ€§è¯„åˆ†
            necessity_score = np.mean([novelty_score, utility_score, feasibility_score, alignment_score])
            
            # æ›´æ–°ç›®æ ‡å¿…è¦æ€§
            target.necessity_score = necessity_score
            
            # ç”Ÿæˆæ¨ç†è¯´æ˜
            reasoning = []
            if novelty_score < 0.5:
                reasoning.append("ç›®æ ‡ç¼ºä¹æ–°é¢–æ€§ï¼Œå¯èƒ½å·²æœ‰ç±»ä¼¼å®ç°")
            if utility_score < 0.5:
                reasoning.append("ç›®æ ‡å®ç”¨æ€§è¾ƒä½ï¼Œé¢„æœŸæ”¶ç›Šæœ‰é™")
            if feasibility_score < 0.5:
                reasoning.append("ç›®æ ‡å®ç°éš¾åº¦è¾ƒé«˜ï¼Œèµ„æºéœ€æ±‚è¿‡å¤§")
            if alignment_score < 0.5:
                reasoning.append("ç›®æ ‡ä¸ç³»ç»Ÿæ•´ä½“ç›®æ ‡å¯¹é½åº¦è¾ƒä½")
            
            if not reasoning:
                reasoning.append("ç›®æ ‡åœ¨å¤šä¸ªç»´åº¦ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå…·æœ‰è¾ƒé«˜çš„å¿…è¦æ€§")
            
            return {
                'target_id': target_id,
                'necessity_score': necessity_score,
                'dimension_scores': {
                    'novelty': novelty_score,
                    'utility': utility_score,
                    'feasibility': feasibility_score,
                    'alignment': alignment_score
                },
                'reasoning': reasoning,
                'assessment_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ å¿…è¦æ€§è¯„ä¼°å¤±è´¥: {e}")
            return {'error': str(e)}
    
    async def _assess_novelty(self, target: CognitiveTarget) -> float:
        """è¯„ä¼°æ–°é¢–æ€§"""
        try:
            # åŸºäºå†å²æ•°æ®çš„æ–°é¢–æ€§è¯„ä¼°
            if not self.target_history:
                return 1.0  # æ²¡æœ‰å†å²æ•°æ®æ—¶è®¤ä¸ºå®Œå…¨æ–°é¢–
            
            # æŸ¥æ‰¾ç›¸ä¼¼çš„å†å²ç›®æ ‡
            similar_historical_targets = []
            for history_entry in self.target_history:
                if 'target_id' in history_entry and history_entry['target_id'] in self.cognitive_targets:
                    historical_target = self.cognitive_targets[history_entry['target_id']]
                    similarity = await self._calculate_semantic_similarity(
                        target.semantic_vector,
                        historical_target.semantic_vector
                    )
                    if similarity > 0.7:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                        similar_historical_targets.append({
                            'similarity': similarity,
                            'target': historical_target,
                            'time_delta': (datetime.now() - history_entry.get('timestamp', datetime.now())).days
                        })
            
            if not similar_historical_targets:
                return 1.0  # å®Œå…¨æ–°é¢–
            
            # åŸºäºç›¸ä¼¼åº¦å’Œæ—¶é—´çš„æ–°é¢–æ€§è¯„åˆ†
            novelty_scores = []
            for historical in similar_historical_targets:
                # ç›¸ä¼¼åº¦è¶Šä½ï¼Œæ–°é¢–æ€§è¶Šé«˜
                similarity_score = 1.0 - historical['similarity']
                
                # æ—¶é—´è¶Šä¹…ï¼Œæ–°é¢–æ€§è¶Šé«˜ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰
                time_score = np.exp(-historical['time_delta'] / 365)  # å¹´åº¦è¡°å‡
                
                novelty_scores.append(similarity_score * time_score)
            
            return np.mean(novelty_scores)
            
        except Exception as e:
            logger.error(f"âŒ æ–°é¢–æ€§è¯„ä¼°å¤±è´¥: {e}")
            return 0.5  # ä¸­æ€§è¯„åˆ†
    
    async def _assess_utility(self, target: CognitiveTarget) -> float:
        """è¯„ä¼°å®ç”¨æ€§"""
        try:
            # åŸºäºç›®æ ‡å±æ€§å’Œå…ƒæ•°æ®è¯„ä¼°å®ç”¨æ€§
            utility_indicators = []
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„æ”¶ç›ŠæŒ‡æ ‡
            if 'expected_benefit' in target.metadata:
                benefit = target.metadata['expected_benefit']
                if isinstance(benefit, (int, float)):
                    utility_indicators.append(min(benefit / 100, 1.0))  # æ ‡å‡†åŒ–åˆ°0-1
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸæ¦‚ç‡
            if 'success_probability' in target.metadata:
                prob = target.metadata['success_probability']
                if isinstance(prob, (int, float)) and 0 <= prob <= 1:
                    utility_indicators.append(prob)
            
            # æ£€æŸ¥èµ„æºæ•ˆç‡
            if target.resource_requirements:
                total_resources = sum(target.resource_requirements.values())
                # èµ„æºéœ€æ±‚é€‚ä¸­ï¼ˆä¸å¤ªå°‘ä¹Ÿä¸å¤ªå¤šï¼‰
                if 0.1 <= total_resources <= 0.8:
                    utility_indicators.append(0.8)
                elif total_resources < 0.1:
                    utility_indicators.append(0.4)  # èµ„æºéœ€æ±‚å¤ªå°‘å¯èƒ½ä¸å¤Ÿé‡è¦
                else:
                    utility_indicators.append(0.3)  # èµ„æºéœ€æ±‚å¤ªå¤šå¯èƒ½ä¸åˆ’ç®—
            
            # åŸºäºæè¿°å…³é”®è¯çš„å®ç”¨æ€§è¯„ä¼°
            description_lower = target.description.lower()
            utility_keywords = {
                'é‡è¦': 0.9, 'å…³é”®': 0.9, 'æ ¸å¿ƒ': 0.9,
                'ä¼˜åŒ–': 0.8, 'æ”¹è¿›': 0.8, 'æå‡': 0.8,
                'è§£å†³': 0.7, 'ä¿®å¤': 0.7, 'çº æ­£': 0.7,
                'æ–°': 0.6, 'åˆ›æ–°': 0.6, 'çªç ´': 0.6
            }
            
            for keyword, score in utility_keywords.items():
                if keyword in description_lower:
                    utility_indicators.append(score)
            
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å®ç”¨æ€§æŒ‡æ ‡ï¼Œè¿”å›ä¸­æ€§è¯„åˆ†
            if not utility_indicators:
                return 0.6
            
            return np.mean(utility_indicators)
            
        except Exception as e:
            logger.error(f"âŒ å®ç”¨æ€§è¯„ä¼°å¤±è´¥: {e}")
            return 0.5
    
    async def _assess_feasibility(self, target: CognitiveTarget) -> float:
        """è¯„ä¼°å¯è¡Œæ€§"""
        try:
            feasibility_factors = []
            
            # åŸºäºèµ„æºéœ€æ±‚è¯„ä¼°å¯è¡Œæ€§
            if target.resource_requirements:
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„èµ„æºä¿¡æ¯
                required_resources = sum(target.resource_requirements.values())
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨èµ„æºä¿¡æ¯
                available_resources = target.metadata.get('available_resources', {})
                if available_resources:
                    # è®¡ç®—èµ„æºå……è¶³åº¦
                    resource_adequacy = 0.0
                    for resource_type, required_amount in target.resource_requirements.items():
                        available_amount = available_resources.get(resource_type, 0)
                        if required_amount > 0:
                            adequacy = min(available_amount / required_amount, 1.0)
                            resource_adequacy += adequacy
                    
                    if len(target.resource_requirements) > 0:
                        feasibility_factors.append(resource_adequacy / len(target.resource_requirements))
                else:
                    # æ²¡æœ‰å¯ç”¨èµ„æºä¿¡æ¯ï¼ŒåŸºäºéœ€æ±‚åˆç†æ€§è¯„ä¼°
                    if required_resources <= 0.5:  # èµ„æºéœ€æ±‚é€‚ä¸­
                        feasibility_factors.append(0.8)
                    elif required_resources <= 0.8:
                        feasibility_factors.append(0.6)
                    else:
                        feasibility_factors.append(0.3)
            
            # åŸºäºä¾èµ–å…³ç³»è¯„ä¼°å¯è¡Œæ€§
            if target.dependencies:
                # æ£€æŸ¥ä¾èµ–ç›®æ ‡æ˜¯å¦å­˜åœ¨ä¸”å¯å®ç°
                dependency_feasibility = 0.0
                resolved_dependencies = 0
                
                for dep_id in target.dependencies:
                    if dep_id in self.cognitive_targets:
                        dep_target = self.cognitive_targets[dep_id]
                        # ä¾èµ–ç›®æ ‡çš„å¿…è¦æ€§è¶Šé«˜ï¼Œå½“å‰ç›®æ ‡çš„å¯è¡Œæ€§è¶Šé«˜
                        if hasattr(dep_target, 'necessity_score'):
                            dependency_feasibility += dep_target.necessity_score
                        else:
                            dependency_feasibility += 0.7  # é»˜è®¤å¿…è¦æ€§
                        resolved_dependencies += 1
                
                if resolved_dependencies > 0:
                    feasibility_factors.append(dependency_feasibility / resolved_dependencies)
                else:
                    feasibility_factors.append(0.4)  # ä¾èµ–æœªè§£å†³
            
            # åŸºäºæ—¶é—´çº¦æŸè¯„ä¼°å¯è¡Œæ€§
            if target.deadline:
                time_remaining = (target.deadline - datetime.now()).total_seconds() / 3600  # å°æ—¶
                if time_remaining < 0:
                    feasibility_factors.append(0.0)  # å·²è¿‡æœŸ
                elif time_remaining < 24:  # å°‘äº1å¤©
                    feasibility_factors.append(0.3)
                elif time_remaining < 168:  # å°‘äº1å‘¨
                    feasibility_factors.append(0.6)
                else:
                    feasibility_factors.append(0.9)
            
            # åŸºäºæè¿°å…³é”®è¯çš„å¯è¡Œæ€§è¯„ä¼°
            description_lower = target.description.lower()
            feasibility_keywords = {
                'ç®€å•': 0.9, 'å®¹æ˜“': 0.9, 'å¿«é€Ÿ': 0.8,
                'å¤æ‚': 0.4, 'å›°éš¾': 0.3, 'æŒ‘æˆ˜': 0.5
            }
            
            for keyword, score in feasibility_keywords.items():
                if keyword in description_lower:
                    feasibility_factors.append(score)
            
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å¯è¡Œæ€§æŒ‡æ ‡ï¼Œè¿”å›ä¸­æ€§è¯„åˆ†
            if not feasibility_factors:
                return 0.7
            
            return np.mean(feasibility_factors)
            
        except Exception as e:
            logger.error(f"âŒ å¯è¡Œæ€§è¯„ä¼°å¤±è´¥: {e}")
            return 0.5
    
    async def _assess_alignment(self, target: CognitiveTarget) -> float:
        """è¯„ä¼°å¯¹é½åº¦"""
        try:
            alignment_scores = []
            
            # ä¸ç³»ç»Ÿæ•´ä½“ç›®æ ‡çš„å¯¹é½
            system_goals = self.config.get('system_goals', ['efficiency', 'accuracy', 'scalability'])
            
            # åŸºäºæè¿°å…³é”®è¯çš„å¯¹é½è¯„ä¼°
            description_lower = target.description.lower()
            
            for goal in system_goals:
                goal_keywords = {
                    'efficiency': ['æ•ˆç‡', 'ä¼˜åŒ–', 'å¿«é€Ÿ', 'æ€§èƒ½'],
                    'accuracy': ['å‡†ç¡®', 'ç²¾ç¡®', 'æ­£ç¡®', 'å¯é '],
                    'scalability': ['æ‰©å±•', 'è§„æ¨¡', 'å¢é•¿', 'é€‚åº”'],
                    'safety': ['å®‰å…¨', 'ç¨³å®š', 'å¯é ', 'é²æ£’'],
                    'ethics': ['ä¼¦ç†', 'é“å¾·', 'å…¬å¹³', 'é€æ˜']
                }.get(goal, [goal])
                
                alignment_score = 0.0
                for keyword in goal_keywords:
                    if keyword in description_lower:
                        alignment_score = max(alignment_score, 0.8)
                
                alignment_scores.append(alignment_score)
            
            # åŸºäºèµ„æºæ•ˆç‡çš„å¯¹é½
            if target.resource_requirements:
                # èµ„æºä½¿ç”¨æ•ˆç‡ï¼ˆé¿å…æµªè´¹ï¼‰
                total_resources = sum(target.resource_requirements.values())
                if total_resources <= 0.5:  # èµ„æºä½¿ç”¨é«˜æ•ˆ
                    alignment_scores.append(0.9)
                elif total_resources <= 0.8:
                    alignment_scores.append(0.7)
                else:
                    alignment_scores.append(0.4)
            
            # åŸºäºä¼˜å…ˆçº§çš„å¯¹é½
            if target.priority > 0.8:  # é«˜ä¼˜å…ˆçº§ç›®æ ‡é€šå¸¸æ›´å¯¹é½
                alignment_scores.append(0.8)
            elif target.priority > 0.6:
                alignment_scores.append(0.6)
            else:
                alignment_scores.append(0.4)
            
            return np.mean(alignment_scores) if alignment_scores else 0.5
            
        except Exception as e:
            logger.error(f"âŒ å¯¹é½åº¦è¯„ä¼°å¤±è´¥: {e}")
            return 0.5
    
    # ==================== ä¼˜å…ˆçº§åŠ¨æ€ä¼˜åŒ– ====================
    
    async def optimize_priorities(self, optimization_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """åŠ¨æ€ä¼˜åŒ–ä¼˜å…ˆçº§"""
        try:
            optimization_result = {
                'optimization_id': f"opt_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                'before_optimization': {},
                'after_optimization': {},
                'changes_made': [],
                'optimization_reasoning': [],
                'timestamp': datetime.now().isoformat()
            }
            
            # è·å–ä¼˜åŒ–ä¸Šä¸‹æ–‡
            context = optimization_context or {}
            current_resources = context.get('available_resources', {})
            system_load = context.get('system_load', 0.5)
            external_priorities = context.get('external_priorities', [])
            
            # è®°å½•ä¼˜åŒ–å‰çŠ¶æ€
            for target_id, target in self.cognitive_targets.items():
                optimization_result['before_optimization'][target_id] = {
                    'priority': target.priority,
                    'necessity_score': target.necessity_score,
                    'resource_requirements': target.resource_requirements
                }
            
            # æ‰§è¡Œå¤šç»´åº¦ä¼˜å…ˆçº§ä¼˜åŒ–
            optimized_targets = []
            
            for target_id, target in self.cognitive_targets.items():
                # é‡æ–°è¯„ä¼°å¿…è¦æ€§ï¼ˆå¦‚æœè¶…è¿‡æ›´æ–°é—´éš”ï¼‰
                if self._should_reassess_necessity(target):
                    necessity_result = await self.assess_target_necessity(target_id)
                    target.necessity_score = necessity_result.get('necessity_score', target.necessity_score)
                
                # åŠ¨æ€ä¼˜å…ˆçº§è®¡ç®—
                new_priority = await self._calculate_dynamic_priority(
                    target, current_resources, system_load, external_priorities
                )
                
                # è®°å½•å˜åŒ–
                if abs(new_priority - target.priority) > 0.1:  # æ˜¾è‘—å˜åŒ–é˜ˆå€¼
                    old_priority = target.priority
                    target.priority = new_priority
                    
                    optimization_result['changes_made'].append({
                        'target_id': target_id,
                        'old_priority': old_priority,
                        'new_priority': new_priority,
                        'change_reason': await self._generate_priority_change_reason(target, old_priority, new_priority)
                    })
                    
                    optimized_targets.append(target_id)
            
            # è®°å½•ä¼˜åŒ–åçŠ¶æ€
            for target_id, target in self.cognitive_targets.items():
                optimization_result['after_optimization'][target_id] = {
                    'priority': target.priority,
                    'necessity_score': target.necessity_score,
                    'resource_requirements': target.resource_requirements
                }
            
            # ç”Ÿæˆä¼˜åŒ–æ¨ç†è¯´æ˜
            optimization_result['optimization_reasoning'] = await self._generate_optimization_reasoning(
                optimization_result['changes_made'], current_resources, system_load
            )
            
            # è®°å½•ä¼˜åŒ–å†å²
            self.optimization_history.append({
                'optimization_id': optimization_result['optimization_id'],
                'targets_optimized': len(optimized_targets),
                'total_targets': len(self.cognitive_targets),
                'optimization_time': datetime.now(),
                'context': context
            })
            
            logger.info(f"âœ… ä¼˜å…ˆçº§ä¼˜åŒ–å®Œæˆ: {len(optimized_targets)}/{len(self.cognitive_targets)} ç›®æ ‡")
            
            return optimization_result
            
        except Exception as e:
            logger.error(f"âŒ ä¼˜å…ˆçº§ä¼˜åŒ–å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _should_reassess_necessity(self, target: CognitiveTarget) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡æ–°è¯„ä¼°å¿…è¦æ€§"""
        # åŸºäºæ—¶é—´é—´éš”åˆ¤æ–­
        time_since_creation = (datetime.now() - target.creation_time).total_seconds()
        return time_since_creation > self.priority_update_interval
    
    async def _calculate_dynamic_priority(self, target: CognitiveTarget, 
                                        current_resources: Dict[str, float],
                                        system_load: float,
                                        external_priorities: List[Dict[str, Any]]) -> float:
        """è®¡ç®—åŠ¨æ€ä¼˜å…ˆçº§"""
        try:
            # åŸºç¡€ä¼˜å…ˆçº§ï¼ˆåŸºäºå¿…è¦æ€§å’Œå½“å‰ä¼˜å…ˆçº§ï¼‰
            base_priority = (target.necessity_score * 0.6 + target.priority * 0.4)
            
            # èµ„æºå¯ç”¨æ€§è°ƒæ•´
            resource_adjustment = await self._calculate_resource_adjustment(target, current_resources)
            
            # ç³»ç»Ÿè´Ÿè½½è°ƒæ•´
            load_adjustment = self._calculate_load_adjustment(system_load)
            
            # å¤–éƒ¨ä¼˜å…ˆçº§å½±å“
            external_adjustment = await self._calculate_external_adjustment(target, external_priorities)
            
            # æ—¶é—´ç´§è¿«æ€§è°ƒæ•´
            urgency_adjustment = await self._calculate_urgency_adjustment(target)
            
            # ç»¼åˆè®¡ç®—
            dynamic_priority = (
                base_priority * 0.4 +
                resource_adjustment * 0.2 +
                load_adjustment * 0.15 +
                external_adjustment * 0.15 +
                urgency_adjustment * 0.1
            )
            
            # ç¡®ä¿ä¼˜å…ˆçº§åœ¨åˆç†èŒƒå›´å†…
            return max(0.0, min(1.0, dynamic_priority))
            
        except Exception as e:
            logger.error(f"âŒ åŠ¨æ€ä¼˜å…ˆçº§è®¡ç®—å¤±è´¥: {e}")
            return target.priority  # è¿”å›å½“å‰ä¼˜å…ˆçº§ä½œä¸ºåå¤‡
    
    async def _calculate_resource_adjustment(self, target: CognitiveTarget, current_resources: Dict[str, float]) -> float:
        """è®¡ç®—èµ„æºè°ƒæ•´"""
        try:
            if not target.resource_requirements or not current_resources:
                return 0.5  # ä¸­æ€§è°ƒæ•´
            
            # è®¡ç®—èµ„æºåŒ¹é…åº¦
            resource_match_scores = []
            
            for resource_type, required_amount in target.resource_requirements.items():
                available_amount = current_resources.get(resource_type, 0)
                
                if required_amount > 0:
                    if available_amount >= required_amount:
                        # èµ„æºå……è¶³ï¼Œæé«˜ä¼˜å…ˆçº§
                        resource_match_scores.append(0.8)
                    elif available_amount >= required_amount * 0.7:
                        # èµ„æºåŸºæœ¬å……è¶³
                        resource_match_scores.append(0.6)
                    elif available_amount >= required_amount * 0.4:
                        # èµ„æºéƒ¨åˆ†å……è¶³
                        resource_match_scores.append(0.4)
                    else:
                        # èµ„æºä¸¥é‡ä¸è¶³ï¼Œé™ä½ä¼˜å…ˆçº§
                        resource_match_scores.append(0.2)
            
            return np.mean(resource_match_scores) if resource_match_scores else 0.5
            
        except Exception as e:
            logger.error(f"âŒ èµ„æºè°ƒæ•´è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _calculate_load_adjustment(self, system_load: float) -> float:
        """è®¡ç®—è´Ÿè½½è°ƒæ•´"""
        try:
            # ç³»ç»Ÿè´Ÿè½½å½±å“ä¼˜å…ˆçº§åˆ†é…
            # é«˜è´Ÿè½½æ—¶ä¼˜å…ˆå¤„ç†é«˜ä¼˜å…ˆçº§ç›®æ ‡
            if system_load > 0.8:  # é«˜è´Ÿè½½
                return 0.9  # å€¾å‘äºé«˜ä¼˜å…ˆçº§
            elif system_load > 0.6:  # ä¸­ç­‰è´Ÿè½½
                return 0.7
            elif system_load > 0.3:  # ä½è´Ÿè½½
                return 0.5
            else:  # æä½è´Ÿè½½
                return 0.3  # å¯ä»¥æ›´å‡è¡¡åœ°åˆ†é…
            
        except Exception as e:
            logger.error(f"âŒ è´Ÿè½½è°ƒæ•´è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    async def _calculate_external_adjustment(self, target: CognitiveTarget, external_priorities: List[Dict[str, Any]]) -> float:
        """è®¡ç®—å¤–éƒ¨è°ƒæ•´"""
        try:
            if not external_priorities:
                return 0.5  # ä¸­æ€§è°ƒæ•´
            
            # æŸ¥æ‰¾ä¸å½“å‰ç›®æ ‡ç›¸å…³çš„å¤–éƒ¨ä¼˜å…ˆçº§
            relevant_priorities = []
            
            for external_priority in external_priorities:
                # åŸºäºç›®æ ‡IDåŒ¹é…
                if external_priority.get('target_id') == target.target_id:
                    relevant_priorities.append(external_priority.get('priority', 0.5))
                    continue
                
                # åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…
                external_description = external_priority.get('description', '')
                if external_description:
                    similarity = await self._calculate_text_similarity(
                        target.description,
                        external_description
                    )
                    if similarity > 0.7:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                        relevant_priorities.append(external_priority.get('priority', 0.5) * similarity)
            
            if not relevant_priorities:
                return 0.5
            
            return np.mean(relevant_priorities)
            
        except Exception as e:
            logger.error(f"âŒ å¤–éƒ¨è°ƒæ•´è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    async def _calculate_urgency_adjustment(self, target: CognitiveTarget) -> float:
        """è®¡ç®—ç´§è¿«æ€§è°ƒæ•´"""
        try:
            if not target.deadline:
                return 0.5  # ä¸­æ€§è°ƒæ•´
            
            time_remaining = (target.deadline - datetime.now()).total_seconds() / 3600  # å°æ—¶
            
            if time_remaining < 0:  # å·²è¿‡æœŸ
                return 0.1
            elif time_remaining < 1:  # å°‘äº1å°æ—¶
                return 0.95
            elif time_remaining < 24:  # å°‘äº1å¤©
                return 0.8
            elif time_remaining < 168:  # å°‘äº1å‘¨
                return 0.6
            elif time_remaining < 720:  # å°‘äº1ä¸ªæœˆ
                return 0.4
            else:  # è¶…è¿‡1ä¸ªæœˆ
                return 0.3
            
        except Exception as e:
            logger.error(f"âŒ ç´§è¿«æ€§è°ƒæ•´è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    async def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        try:
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())
            
            if not words1 or not words2:
                return 0.0
            
            intersection = len(words1 & words2)
            union = len(words1 | words2)
            
            return intersection / union if union > 0 else 0.0
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    async def _generate_priority_change_reason(self, target: CognitiveTarget, old_priority: float, new_priority: float) -> str:
        """ç”Ÿæˆä¼˜å…ˆçº§å˜åŒ–åŸå› """
        try:
            change_direction = "æé«˜" if new_priority > old_priority else "é™ä½"
            change_magnitude = abs(new_priority - old_priority)
            
            reasons = []
            
            # åŸºäºå¿…è¦æ€§å˜åŒ–
            if hasattr(target, 'necessity_score'):
                if target.necessity_score > 0.8:
                    reasons.append("ç›®æ ‡å¿…è¦æ€§è¯„ä¼°ç»“æœä¼˜ç§€")
                elif target.necessity_score < 0.4:
                    reasons.append("ç›®æ ‡å¿…è¦æ€§è¯„ä¼°ç»“æœè¾ƒä½")
            
            # åŸºäºèµ„æºå˜åŒ–
            if target.resource_requirements:
                reasons.append("èµ„æºå¯ç”¨æ€§å‘ç”Ÿå˜åŒ–")
            
            # åŸºäºæ—¶é—´ç´§è¿«æ€§
            if target.deadline:
                time_remaining = (target.deadline - datetime.now()).total_seconds() / 3600
                if time_remaining < 24:
                    reasons.append("ç›®æ ‡æˆªæ­¢æ—¶é—´ä¸´è¿‘")
            
            # åŸºäºç³»ç»ŸçŠ¶æ€
            reasons.append("ç³»ç»Ÿæ•´ä½“çŠ¶æ€å’Œèµ„æºåˆ†é…ä¼˜åŒ–")
            
            reason_text = f"ä¼˜å…ˆçº§{change_direction}äº†{change_magnitude:.1%}ï¼Œä¸»è¦åŸå› åŒ…æ‹¬ï¼š{', '.join(reasons[:2])}"
            
            return reason_text
            
        except Exception as e:
            logger.error(f"âŒ ä¼˜å…ˆçº§å˜åŒ–åŸå› ç”Ÿæˆå¤±è´¥: {e}")
            return f"ä¼˜å…ˆçº§{change_direction}äº†{change_magnitude:.1%}"
    
    async def _generate_optimization_reasoning(self, changes: List[Dict[str, Any]], 
                                             current_resources: Dict[str, float],
                                             system_load: float) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–æ¨ç†è¯´æ˜"""
        reasoning = []
        
        try:
            if not changes:
                reasoning.append("å½“å‰ç›®æ ‡ä¼˜å…ˆçº§é…ç½®åˆç†ï¼Œæ— éœ€è°ƒæ•´")
                return reasoning
            
            # åŸºäºå˜åŒ–è¶‹åŠ¿åˆ†æ
            increases = [c for c in changes if c['new_priority'] > c['old_priority']]
            decreases = [c for c in changes if c['new_priority'] < c['old_priority']]
            
            if len(increases) > len(decreases):
                reasoning.append("ç³»ç»Ÿæ£€æµ‹åˆ°æ›´å¤šé«˜ä»·å€¼ç›®æ ‡ï¼Œæ•´ä½“ä¼˜å…ˆçº§å‘é‡è¦ç›®æ ‡å€¾æ–œ")
            elif len(decreases) > len(increases):
                reasoning.append("ç³»ç»Ÿä¼˜åŒ–èµ„æºé…ç½®ï¼Œé™ä½éƒ¨åˆ†ç›®æ ‡çš„ä¼˜å…ˆçº§ä»¥æé«˜æ•´ä½“æ•ˆç‡")
            
            # åŸºäºèµ„æºçŠ¶æ€
            if current_resources:
                total_available = sum(current_resources.values())
                if total_available < 0.5:
                    reasoning.append("å½“å‰å¯ç”¨èµ„æºæœ‰é™ï¼Œä¼˜å…ˆä¿éšœé«˜ä»·å€¼ç›®æ ‡")
                elif total_available > 0.8:
                    reasoning.append("èµ„æºå……è¶³ï¼Œå¯ä»¥æ›´å‡è¡¡åœ°åˆ†é…ä¼˜å…ˆçº§")
            
            # åŸºäºç³»ç»Ÿè´Ÿè½½
            if system_load > 0.8:
                reasoning.append("ç³»ç»Ÿè´Ÿè½½è¾ƒé«˜ï¼Œé›†ä¸­èµ„æºå¤„ç†é«˜ä¼˜å…ˆçº§ç›®æ ‡")
            elif system_load < 0.3:
                reasoning.append("ç³»ç»Ÿè´Ÿè½½è¾ƒä½ï¼Œå¯ä»¥å¤„ç†æ›´å¤šä¸­ç­‰ä¼˜å…ˆçº§ç›®æ ‡")
            
            # åŸºäºå˜åŒ–å¹…åº¦
            significant_changes = [c for c in changes if abs(c['new_priority'] - c['old_priority']) > 0.2]
            if len(significant_changes) > len(changes) * 0.5:
                reasoning.append("æ£€æµ‹åˆ°æ˜¾è‘—çš„ä¼˜å…ˆçº§å˜åŒ–ï¼Œç³»ç»Ÿè¿›è¡Œäº†å¤§å¹…ä¼˜åŒ–è°ƒæ•´")
            
            return reasoning
            
        except Exception as e:
            logger.error(f"âŒ ä¼˜åŒ–æ¨ç†è¯´æ˜ç”Ÿæˆå¤±è´¥: {e}")
            return ["ä¼˜åŒ–æ¨ç†ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯"]
    
    # ==================== å†²çªæ£€æµ‹ä¸è§£å†³ ====================
    
    async def detect_conflicts(self) -> List[ConflictAnalysis]:
        """æ£€æµ‹å†²çª"""
        conflicts = []
        
        try:
            target_list = list(self.cognitive_targets.values())
            
            # èµ„æºå†²çªæ£€æµ‹
            resource_conflicts = await self._detect_resource_conflicts(target_list)
            conflicts.extend(resource_conflicts)
            
            # è¯­ä¹‰å†²çªæ£€æµ‹
            semantic_conflicts = await self._detect_semantic_conflicts(target_list)
            conflicts.extend(semantic_conflicts)
            
            # æ—¶åºå†²çªæ£€æµ‹
            temporal_conflicts = await self._detect_temporal_conflicts(target_list)
            conflicts.extend(temporal_conflicts)
            
            # é€»è¾‘å†²çªæ£€æµ‹
            logical_conflicts = await self._detect_logical_conflicts(target_list)
            conflicts.extend(logical_conflicts)
            
            logger.info(f"âœ… å†²çªæ£€æµ‹å®Œæˆ: {len(conflicts)} ä¸ªå†²çª")
            
        except Exception as e:
            logger.error(f"âŒ å†²çªæ£€æµ‹å¤±è´¥: {e}")
        
        return conflicts
    
    async def _detect_resource_conflicts(self, targets: List[CognitiveTarget]) -> List[ConflictAnalysis]:
        """æ£€æµ‹èµ„æºå†²çª"""
        conflicts = []
        
        try:
            # æŒ‰èµ„æºç±»å‹åˆ†ç»„
            resource_demands: Dict[str, List[Tuple[str, float]]] = defaultdict(list)
            
            for target in targets:
                for resource_type, required_amount in target.resource_requirements.items():
                    if required_amount > 0:
                        resource_demands[resource_type].append((target.target_id, required_amount))
            
            # æ£€æµ‹èµ„æºå†²çª
            for resource_type, demands in resource_demands.items():
                if len(demands) > 1:
                    # è®¡ç®—æ€»éœ€æ±‚
                    total_demand = sum(amount for _, amount in demands)
                    
                    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡èµ„æºé™åˆ¶ï¼ˆå‡è®¾é™åˆ¶ä¸º1.0ï¼‰
                    if total_demand > 1.0:
                        conflict_targets = [target_id for target_id, _ in demands]
                        
                        conflict = ConflictAnalysis(
                            conflict_id=f"resource_conflict_{resource_type}_{datetime.now().strftime('%H%M%S')}",
                            target_ids=conflict_targets,
                            conflict_type='resource_conflict',
                            severity=min(total_demand - 1.0, 1.0),  # å†²çªä¸¥é‡ç¨‹åº¦
                            root_causes=[f"èµ„æº'{resource_type}'æ€»éœ€æ±‚({total_demand:.2f})è¶…è¿‡å¯ç”¨é™åˆ¶"],
                            resolution_suggestions=await self._generate_resource_resolution_suggestions(resource_type, demands),
                            detection_time=datetime.now()
                        )
                        
                        conflicts.append(conflict)
            
        except Exception as e:
            logger.error(f"âŒ èµ„æºå†²çªæ£€æµ‹å¤±è´¥: {e}")
        
        return conflicts
    
    async def _detect_semantic_conflicts(self, targets: List[CognitiveTarget]) -> List[ConflictAnalysis]:
        """æ£€æµ‹è¯­ä¹‰å†²çª"""
        conflicts = []
        
        try:
            # æ£€æŸ¥è¯­ä¹‰ç›¸åæˆ–äº’æ–¥çš„ç›®æ ‡
            semantic_opposites = {
                'å¢åŠ ': ['å‡å°‘', 'é™ä½', 'æ¶ˆé™¤'],
                'ä¼˜åŒ–': ['ç®€åŒ–', 'å‡å°‘'],
                'æ‰©å±•': ['å‹ç¼©', 'å‡å°‘'],
                'åŠ é€Ÿ': ['å‡é€Ÿ', 'å»¶è¿Ÿ'],
                'é›†ä¸­': ['åˆ†æ•£', 'åˆ†å¸ƒ']
            }
            
            for i, target1 in enumerate(targets):
                for j, target2 in enumerate(targets):
                    if i < j:  # é¿å…é‡å¤æ£€æŸ¥
                        # æ£€æŸ¥è¯­ä¹‰ç›¸å
                        description1 = target1.description.lower()
                        description2 = target2.description.lower()
                        
                        for concept, opposites in semantic_opposites.items():
                            if concept in description1:
                                for opposite in opposites:
                                    if opposite in description2:
                                        # æ£€æµ‹åˆ°è¯­ä¹‰å†²çª
                                        conflict = ConflictAnalysis(
                                            conflict_id=f"semantic_conflict_{target1.target_id}_{target2.target_id}",
                                            target_ids=[target1.target_id, target2.target_id],
                                            conflict_type='semantic_conflict',
                                            severity=0.7,  # è¯­ä¹‰å†²çªé€šå¸¸è¾ƒä¸¥é‡
                                            root_causes=[f"ç›®æ ‡'{target1.description}'ä¸'{target2.description}'å­˜åœ¨è¯­ä¹‰å†²çª"],
                                            resolution_suggestions=await self._generate_semantic_resolution_suggestions(target1, target2, concept, opposite),
                                            detection_time=datetime.now()
                                        )
                                        
                                        conflicts.append(conflict)
                                        break
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰å†²çªæ£€æµ‹å¤±è´¥: {e}")
        
        return conflicts
    
    async def _detect_temporal_conflicts(self, targets: List[CognitiveTarget]) -> List[ConflictAnalysis]:
        """æ£€æµ‹æ—¶åºå†²çª"""
        conflicts = []
        
        try:
            # æŒ‰æˆªæ­¢æ—¶é—´åˆ†ç»„
            deadline_groups: Dict[datetime, List[str]] = defaultdict(list)
            
            for target in targets:
                if target.deadline:
                    # æŒ‰å°æ—¶åˆ†ç»„ï¼ˆç®€åŒ–å®ç°ï¼‰
                    deadline_hour = target.deadline.replace(minute=0, second=0, microsecond=0)
                    deadline_groups[deadline_hour].append(target.target_id)
            
            # æ£€æµ‹æ—¶åºå†²çª
            for deadline, target_ids in deadline_groups.items():
                if len(target_ids) > 3:  # åŒä¸€æ—¶é—´æ®µå†…ç›®æ ‡è¿‡å¤š
                    # ä¼°ç®—æ‰€éœ€èµ„æº
                    total_resource_demand = 0.0
                    for target_id in target_ids:
                        if target_id in self.cognitive_targets:
                            target = self.cognitive_targets[target_id]
                            total_resource_demand += sum(target.resource_requirements.values())
                    
                    # å¦‚æœèµ„æºéœ€æ±‚è¶…è¿‡å¤„ç†èƒ½åŠ›ï¼Œè§†ä¸ºå†²çª
                    if total_resource_demand > 1.0:  # å‡è®¾å¤„ç†èƒ½åŠ›ä¸º1.0
                        conflict = ConflictAnalysis(
                            conflict_id=f"temporal_conflict_{deadline.strftime('%Y%m%d_%H%M')}",
                            target_ids=target_ids,
                            conflict_type='temporal_conflict',
                            severity=min(total_resource_demand - 1.0, 1.0),
                            root_causes=[f"æˆªæ­¢æ—¶é—´{deadline}é™„è¿‘ç›®æ ‡è¿‡å¤šï¼Œæ€»èµ„æºéœ€æ±‚({total_resource_demand:.2f})è¶…è¿‡å¤„ç†èƒ½åŠ›"],
                            resolution_suggestions=await self._generate_temporal_resolution_suggestions(target_ids, deadline),
                            detection_time=datetime.now()
                        )
                        
                        conflicts.append(conflict)
            
        except Exception as e:
            logger.error(f"âŒ æ—¶åºå†²çªæ£€æµ‹å¤±è´¥: {e}")
        
        return conflicts
    
    async def _detect_logical_conflicts(self, targets: List[CognitiveTarget]) -> List[ConflictAnalysis]:
        """æ£€æµ‹é€»è¾‘å†²çª"""
        conflicts = []
        
        try:
            # æ£€æŸ¥å¾ªç¯ä¾èµ–
            dependency_graph = {}
            for target in targets:
                dependency_graph[target.target_id] = target.dependencies
            
            cycles = self._find_cycles(dependency_graph)
            
            for cycle in cycles:
                conflict = ConflictAnalysis(
                    conflict_id=f"logical_conflict_cycle_{len(conflicts)}",
                    target_ids=cycle,
                    conflict_type='logical_conflict',
                    severity=0.8,  # å¾ªç¯ä¾èµ–é€šå¸¸è¾ƒä¸¥é‡
                    root_causes=[f"æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–: {' -> '.join(cycle + [cycle[0]])}"],
                    resolution_suggestions=await self._generate_logical_resolution_suggestions(cycle, 'cycle_dependency'),
                    detection_time=datetime.now()
                )
                
                conflicts.append(conflict)
            
            # æ£€æŸ¥äº’æ–¥ä¾èµ–
            for target in targets:
                for dep1 in target.dependencies:
                    for dep2 in target.dependencies:
                        if dep1 != dep2 and dep1 in self.cognitive_targets and dep2 in self.cognitive_targets:
                            dep_target1 = self.cognitive_targets[dep1]
                            dep_target2 = self.cognitive_targets[dep2]
                            
                            # æ£€æŸ¥æ˜¯å¦äº’æ–¥
                            if await self._are_mutually_exclusive(dep_target1, dep_target2):
                                conflict = ConflictAnalysis(
                                    conflict_id=f"logical_conflict_mutex_{target.target_id}",
                                    target_ids=[target.target_id, dep1, dep2],
                                    conflict_type='logical_conflict',
                                    severity=0.7,
                                    root_causes=[f"ç›®æ ‡'{target.description}'çš„ä¾èµ–'{dep_target1.description}'ä¸'{dep_target2.description}'äº’æ–¥"],
                                    resolution_suggestions=await self._generate_logical_resolution_suggestions([target.target_id, dep1, dep2], 'mutual_exclusion'),
                                    detection_time=datetime.now()
                                )
                                
                                conflicts.append(conflict)
            
        except Exception as e:
            logger.error(f"âŒ é€»è¾‘å†²çªæ£€æµ‹å¤±è´¥: {e}")
        
        return conflicts
    
    def _find_cycles(self, graph: Dict[str, List[str]]) -> List[List[str]]:
        """æŸ¥æ‰¾å›¾ä¸­çš„å¾ªç¯"""
        cycles = []
        visited = set()
        rec_stack = set()
        path = []
        
        def dfs(node):
            if node in rec_stack:
                # æ‰¾åˆ°å¾ªç¯
                cycle_start = path.index(node)
                cycle = path[cycle_start:] + [node]
                if len(cycle) > 1:  # é¿å…è‡ªå¾ªç¯
                    cycles.append(cycle)
                return
            
            if node in visited:
                return
            
            visited.add(node)
            rec_stack.add(node)
            path.append(node)
            
            for neighbor in graph.get(node, []):
                dfs(neighbor)
            
            rec_stack.remove(node)
            path.pop()
        
        for node in graph:
            if node not in visited:
                dfs(node)
        
        return cycles
    
    async def _are_mutually_exclusive(self, target1: CognitiveTarget, target2: CognitiveTarget) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªç›®æ ‡æ˜¯å¦äº’æ–¥"""
        try:
            # åŸºäºè¯­ä¹‰å‘é‡çš„äº’æ–¥æ€§åˆ¤æ–­
            if (hasattr(target1, 'semantic_vector') and target1.semantic_vector is not None and
                hasattr(target2, 'semantic_vector') and target2.semantic_vector is not None):
                
                similarity = await self._calculate_semantic_similarity(
                    target1.semantic_vector,
                    target2.semantic_vector
                )
                
                # å¦‚æœç›¸ä¼¼åº¦å¾ˆé«˜ä½†æè¿°å…³é”®è¯ç›¸åï¼Œå¯èƒ½äº’æ–¥
                if similarity > 0.8:
                    # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åçš„å…³é”®è¯
                    opposite_keywords = {
                        'å¢åŠ ': ['å‡å°‘', 'é™ä½'],
                        'å¼€å¯': ['å…³é—­', 'åœæ­¢'],
                        'å¯ç”¨': ['ç¦ç”¨', 'åœç”¨'],
                        'åŠ é€Ÿ': ['å‡é€Ÿ', 'å»¶è¿Ÿ'],
                        'æ‰©å±•': ['å‹ç¼©', 'ç¼©å°']
                    }
                    
                    desc1 = target1.description.lower()
                    desc2 = target2.description.lower()
                    
                    for concept, opposites in opposite_keywords.items():
                        if concept in desc1:
                            for opposite in opposites:
                                if opposite in desc2:
                                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"âŒ äº’æ–¥æ€§åˆ¤æ–­å¤±è´¥: {e}")
            return False
    
    async def _generate_resource_resolution_suggestions(self, resource_type: str, demands: List[Tuple[str, float]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆèµ„æºå†²çªè§£å†³å»ºè®®"""
        suggestions = []
        
        try:
            total_demand = sum(amount for _, amount in demands)
            
            # å»ºè®®1: èµ„æºé‡æ–°åˆ†é…
            suggestions.append({
                'type': 'resource_reallocation',
                'description': f"é‡æ–°åˆ†é…'{resource_type}'èµ„æºï¼ŒæŒ‰æ¯”ä¾‹å‡å°‘å„ç›®æ ‡éœ€æ±‚",
                'implementation': 'æŒ‰æ¯”ä¾‹ç¼©å‡æ‰€æœ‰ç›®æ ‡çš„èµ„æºéœ€æ±‚',
                'expected_outcome': f"æ€»éœ€æ±‚ä»{total_demand:.2f}é™ä½åˆ°1.0",
                'priority': 'high'
            })
            
            # å»ºè®®2: ä¼˜å…ˆçº§æ’åº
            sorted_demands = sorted(demands, key=lambda x: x[1], reverse=True)
            suggestions.append({
                'type': 'priority_sequencing',
                'description': f"æŒ‰ä¼˜å…ˆçº§é¡ºåºå¤„ç†ç›®æ ‡ï¼Œä¼˜å…ˆæ»¡è¶³é«˜ä¼˜å…ˆçº§ç›®æ ‡",
                'implementation': 'æŒ‰èµ„æºéœ€æ±‚æ’åºï¼Œä¾æ¬¡æ»¡è¶³ç›´åˆ°èµ„æºè€—å°½',
                'expected_outcome': f"é«˜ä¼˜å…ˆçº§ç›®æ ‡ä¼˜å…ˆè·å¾—èµ„æº",
                'priority': 'medium'
            })
            
            # å»ºè®®3: ç›®æ ‡åˆå¹¶æˆ–ç®€åŒ–
            if len(demands) > 2:
                suggestions.append({
                    'type': 'target_consolidation',
                    'description': f"åˆå¹¶æˆ–ç®€åŒ–éƒ¨åˆ†ç›®æ ‡ä»¥å‡å°‘'{resource_type}'éœ€æ±‚",
                    'implementation': 'å¯»æ‰¾å¯ä»¥åˆå¹¶çš„ç›¸ä¼¼ç›®æ ‡æˆ–ç®€åŒ–å®ç°æ–¹æ¡ˆ',
                    'expected_outcome': f"å‡å°‘ç›®æ ‡æ•°é‡ï¼Œé™ä½æ€»èµ„æºéœ€æ±‚",
                    'priority': 'medium'
                })
            
            # å»ºè®®4: å¢åŠ èµ„æº
            suggestions.append({
                'type': 'resource_augmentation',
                'description': f"å¢åŠ '{resource_type}'èµ„æºçš„å¯ç”¨é‡",
                'implementation': 'é€šè¿‡å¤–éƒ¨è·å–æˆ–å†…éƒ¨è°ƒé…å¢åŠ èµ„æº',
                'expected_outcome': f"èµ„æºæ€»é‡å¢åŠ ï¼Œæ»¡è¶³æ›´å¤šç›®æ ‡éœ€æ±‚",
                'priority': 'low'
            })
            
        except Exception as e:
            logger.error(f"âŒ èµ„æºå†²çªè§£å†³å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
        
        return suggestions
    
    async def _generate_semantic_resolution_suggestions(self, target1: CognitiveTarget, target2: CognitiveTarget, 
                                                       concept: str, opposite: str) -> List[Dict[str, Any]]:
        """ç”Ÿæˆè¯­ä¹‰å†²çªè§£å†³å»ºè®®"""
        suggestions = []
        
        try:
            # å»ºè®®1: é‡æ–°å®šä¹‰ç›®æ ‡
            suggestions.append({
                'type': 'target_redefinition',
                'description': f"é‡æ–°å®šä¹‰ç›®æ ‡ä»¥é¿å…'{concept}'ä¸'{opposite}'çš„ç›´æ¥å†²çª",
                'implementation': 'å¯»æ‰¾ä¸¤ä¸ªç›®æ ‡çš„å…±åŒåŸºç¡€æˆ–ä¸­é—´çŠ¶æ€',
                'expected_outcome': 'æ¶ˆé™¤è¯­ä¹‰å†²çªï¼Œå»ºç«‹åè°ƒä¸€è‡´çš„ç›®æ ‡',
                'priority': 'high'
            })
            
            # å»ºè®®2: åˆ†é˜¶æ®µå®ç°
            suggestions.append({
                'type': 'staged_implementation',
                'description': f"åˆ†é˜¶æ®µå®ç°ç›®æ ‡ï¼Œå…ˆ'{concept}'å†'{opposite}'æˆ–åä¹‹",
                'implementation': 'å°†å†²çªç›®æ ‡åˆ†è§£ä¸ºæ—¶é—´ä¸Šæœ‰åºçš„å­ç›®æ ‡',
                'expected_outcome': 'é€šè¿‡æ—¶é—´åˆ†ç¦»è§£å†³è¯­ä¹‰å†²çª',
                'priority': 'medium'
            })
            
            # å»ºè®®3: èŒƒå›´é™å®š
            suggestions.append({
                'type': 'scope_limitation',
                'description': f"é™å®šç›®æ ‡åº”ç”¨èŒƒå›´ï¼Œåœ¨ä¸åŒåœºæ™¯ä¸‹åˆ†åˆ«'{concept}'å’Œ'{opposite}'",
                'implementation': 'ä¸ºæ¯ä¸ªç›®æ ‡å®šä¹‰ä¸åŒçš„é€‚ç”¨æ¡ä»¶æˆ–èŒƒå›´',
                'expected_outcome': 'é€šè¿‡ç©ºé—´æˆ–æ¡ä»¶åˆ†ç¦»è§£å†³è¯­ä¹‰å†²çª',
                'priority': 'medium'
            })
            
            # å»ºè®®4: ä¼˜å…ˆçº§æ’åº
            suggestions.append({
                'type': 'priority_based_selection',
                'description': f"åŸºäºä¼˜å…ˆçº§é€‰æ‹©ä¼˜å…ˆ'{concept}'æˆ–ä¼˜å…ˆ'{opposite}'",
                'implementation': 'æ¯”è¾ƒä¸¤ä¸ªç›®æ ‡çš„ä¼˜å…ˆçº§ï¼Œä¼˜å…ˆå®ç°é«˜ä¼˜å…ˆçº§ç›®æ ‡',
                'expected_outcome': 'é€šè¿‡ä¼˜å…ˆçº§æƒè¡¡è§£å†³è¯­ä¹‰å†²çª',
                'priority': 'low'
            })
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰å†²çªè§£å†³å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
        
        return suggestions
    
    async def _generate_temporal_resolution_suggestions(self, target_ids: List[str], deadline: datetime) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ—¶åºå†²çªè§£å†³å»ºè®®"""
        suggestions = []
        
        try:
            # å»ºè®®1: æ—¶é—´é‡æ–°å®‰æ’
            suggestions.append({
                'type': 'time_rescheduling',
                'description': f"é‡æ–°å®‰æ’æˆªæ­¢æ—¶é—´{deadline}é™„è¿‘çš„ç›®æ ‡",
                'implementation': 'å°†éƒ¨åˆ†ç›®æ ‡æå‰æˆ–å»¶åå¤„ç†',
                'expected_outcome': 'åˆ†æ•£æ—¶é—´å‹åŠ›ï¼Œé¿å…èµ„æºå†²çª',
                'priority': 'high'
            })
            
            # å»ºè®®2: å¹¶è¡Œå¤„ç†ä¼˜åŒ–
            suggestions.append({
                'type': 'parallel_processing',
                'description': f"ä¼˜åŒ–å¹¶è¡Œå¤„ç†ç­–ç•¥ï¼Œæé«˜{deadline}é™„è¿‘çš„å¤„ç†èƒ½åŠ›",
                'implementation': 'é€šè¿‡å¹¶è¡ŒåŒ–æˆ–èµ„æºä¼˜åŒ–æé«˜å¤„ç†æ•ˆç‡',
                'expected_outcome': 'åœ¨ç›¸åŒæ—¶é—´å†…å®Œæˆæ›´å¤šç›®æ ‡',
                'priority': 'medium'
            })
            
            # å»ºè®®3: ç›®æ ‡ç®€åŒ–
            suggestions.append({
                'type': 'target_simplification',
                'description': f"ç®€åŒ–{deadline}é™„è¿‘ç›®æ ‡çš„å®ç°è¦æ±‚",
                'implementation': 'é™ä½éƒ¨åˆ†ç›®æ ‡çš„å¤æ‚åº¦æˆ–èµ„æºéœ€æ±‚',
                'expected_outcome': 'å‡å°‘å•ä½æ—¶é—´å†…çš„èµ„æºéœ€æ±‚',
                'priority': 'medium'
            })
            
            # å»ºè®®4: èµ„æºé¢„åˆ†é…
            suggestions.append({
                'type': 'resource_pre_allocation',
                'description': f"ä¸º{deadline}é™„è¿‘çš„ç›®æ ‡é¢„åˆ†é…ä¸“ç”¨èµ„æº",
                'implementation': 'æå‰å‡†å¤‡å’Œåˆ†é…å¿…è¦çš„èµ„æº',
                'expected_outcome': 'ç¡®ä¿å…³é”®æ—¶é—´ç‚¹æœ‰è¶³å¤Ÿçš„èµ„æºæ”¯æŒ',
                'priority': 'low'
            })
            
        except Exception as e:
            logger.error(f"âŒ æ—¶åºå†²çªè§£å†³å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
        
        return suggestions
    
    async def _generate_logical_resolution_suggestions(self, target_ids: List[str], conflict_type: str) -> List[Dict[str, Any]]:
        """ç”Ÿæˆé€»è¾‘å†²çªè§£å†³å»ºè®®"""
        suggestions = []
        
        try:
            if conflict_type == 'cycle_dependency':
                # å¾ªç¯ä¾èµ–è§£å†³å»ºè®®
                suggestions.append({
                    'type': 'dependency_breaking',
                    'description': 'æ‰“ç ´å¾ªç¯ä¾èµ–å…³ç³»',
                    'implementation': 'è¯†åˆ«å¹¶ç§»é™¤å¾ªç¯ä¾èµ–ä¸­çš„ä¸€ä¸ªæˆ–å¤šä¸ªä¾èµ–å…³ç³»',
                    'expected_outcome': 'æ¶ˆé™¤å¾ªç¯ä¾èµ–ï¼Œå»ºç«‹æ¸…æ™°çš„ä¾èµ–å±‚æ¬¡',
                    'priority': 'high'
                })
                
                suggestions.append({
                    'type': 'intermediate_target',
                    'description': 'å¼•å…¥ä¸­é—´ç›®æ ‡è§£å†³å¾ªç¯ä¾èµ–',
                    'implementation': 'åˆ›å»ºæ–°çš„ä¸­é—´ç›®æ ‡æ¥æ‰“ç ´å¾ªç¯',
                    'expected_outcome': 'é€šè¿‡ä¸­é—´ç›®æ ‡å®ç°é—´æ¥ä¾èµ–',
                    'priority': 'medium'
                })
                
            elif conflict_type == 'mutual_exclusion':
                # äº’æ–¥è§£å†³å»ºè®®
                suggestions.append({
                    'type': 'mutual_exclusion_resolution',
                    'description': 'è§£å†³äº’æ–¥ä¾èµ–å…³ç³»',
                    'implementation': 'é‡æ–°è®¾è®¡ç›®æ ‡ç»“æ„ï¼Œé¿å…äº’æ–¥ä¾èµ–',
                    'expected_outcome': 'æ¶ˆé™¤äº’æ–¥ä¾èµ–ï¼Œå»ºç«‹åè°ƒçš„ç›®æ ‡ç»“æ„',
                    'priority': 'high'
                })
                
                suggestions.append({
                    'type': 'conditional_dependency',
                    'description': 'ä½¿ç”¨æ¡ä»¶ä¾èµ–è§£å†³äº’æ–¥',
                    'implementation': 'ä¸ºäº’æ–¥çš„ä¾èµ–è®¾ç½®äº’æ–¥çš„æ‰§è¡Œæ¡ä»¶',
                    'expected_outcome': 'é€šè¿‡æ¡ä»¶æ‰§è¡Œé¿å…äº’æ–¥å†²çª',
                    'priority': 'medium'
                })
            
        except Exception as e:
            logger.error(f"âŒ é€»è¾‘å†²çªè§£å†³å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
        
        return suggestions
    
    # ==================== è®¤çŸ¥èµ„æºåˆ†é…ä¼˜åŒ– ====================
    
    async def optimize_cognitive_resources(self) -> Dict[str, Any]:
        """ä¼˜åŒ–è®¤çŸ¥èµ„æºåˆ†é…"""
        optimization_result = {
            'optimization_id': f"resource_opt_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'resource_allocation': {},
            'efficiency_improvement': 0.0,
            'conflicts_resolved': 0,
            'optimization_steps': [],
            'timestamp': datetime.now().isoformat()
        }
        
        try:
            # è·å–å½“å‰ç›®æ ‡çŠ¶æ€
            active_targets = list(self.cognitive_targets.values())
            
            if not active_targets:
                optimization_result['message'] = "æ²¡æœ‰æ´»è·ƒç›®æ ‡éœ€è¦ä¼˜åŒ–"
                return optimization_result
            
            # æ­¥éª¤1: èµ„æºéœ€æ±‚åˆ†æ
            resource_analysis = await self._analyze_resource_requirements(active_targets)
            optimization_result['optimization_steps'].append({
                'step': 1,
                'type': 'resource_analysis',
                'result': resource_analysis
            })
            
            # æ­¥éª¤2: å†²çªè¯†åˆ«ä¸è§£å†³
            conflicts = await self.detect_conflicts()
            resolved_conflicts = await self._resolve_conflicts(conflicts)
            optimization_result['conflicts_resolved'] = len(resolved_conflicts)
            
            optimization_result['optimization_steps'].append({
                'step': 2,
                'type': 'conflict_resolution',
                'result': {'conflicts_resolved': len(resolved_conflicts)}
            })
            
            # æ­¥éª¤3: èµ„æºåˆ†é…ä¼˜åŒ–
            optimal_allocation = await self._calculate_optimal_resource_allocation(active_targets)
            optimization_result['resource_allocation'] = optimal_allocation
            
            optimization_result['optimization_steps'].append({
                'step': 3,
                'type': 'resource_optimization',
                'result': optimal_allocation
            })
            
            # æ­¥éª¤4: æ•ˆç‡è¯„ä¼°
            efficiency_improvement = await self._calculate_efficiency_improvement(active_targets, optimal_allocation)
            optimization_result['efficiency_improvement'] = efficiency_improvement
            
            optimization_result['optimization_steps'].append({
                'step': 4,
                'type': 'efficiency_evaluation',
                'result': {'efficiency_improvement': efficiency_improvement}
            })
            
            logger.info(f"âœ… è®¤çŸ¥èµ„æºåˆ†é…ä¼˜åŒ–å®Œæˆ: æ•ˆç‡æå‡{efficiency_improvement:.1%}")
            
        except Exception as e:
            logger.error(f"âŒ è®¤çŸ¥èµ„æºåˆ†é…ä¼˜åŒ–å¤±è´¥: {e}")
            optimization_result['error'] = str(e)
        
        return optimization_result
    
    async def _analyze_resource_requirements(self, targets: List[CognitiveTarget]) -> Dict[str, Any]:
        """åˆ†æèµ„æºéœ€æ±‚"""
        try:
            analysis = {
                'total_demand_by_resource': defaultdict(float),
                'target_count_by_resource': defaultdict(int),
                'peak_demand_times': [],
                'resource_utilization': {},
                'bottlenecks': []
            }
            
            # æŒ‰èµ„æºç±»å‹æ±‡æ€»éœ€æ±‚
            for target in targets:
                for resource_type, amount in target.resource_requirements.items():
                    analysis['total_demand_by_resource'][resource_type] += amount
                    analysis['target_count_by_resource'][resource_type] += 1
            
            # è¯†åˆ«ç“¶é¢ˆ
            for resource_type, total_demand in analysis['total_demand_by_resource'].items():
                if total_demand > 1.0:  # è¶…è¿‡å¯ç”¨èµ„æº
                    analysis['bottlenecks'].append({
                        'resource_type': resource_type,
                        'demand': total_demand,
                        'shortage': total_demand - 1.0,
                        'severity': min(total_demand - 1.0, 1.0)
                    })
            
            return dict(analysis)
            
        except Exception as e:
            logger.error(f"âŒ èµ„æºéœ€æ±‚åˆ†æå¤±è´¥: {e}")
            return {}
    
    async def _resolve_conflicts(self, conflicts: List[ConflictAnalysis]) -> List[ConflictAnalysis]:
        """è§£å†³å†²çª"""
        resolved_conflicts = []
        
        try:
            for conflict in conflicts:
                # æ ¹æ®å†²çªç±»å‹é‡‡å–ä¸åŒçš„è§£å†³ç­–ç•¥
                if conflict.conflict_type == 'resource_conflict':
                    resolved = await self._resolve_resource_conflict(conflict)
                elif conflict.conflict_type == 'semantic_conflict':
                    resolved = await self._resolve_semantic_conflict(conflict)
                elif conflict.conflict_type == 'temporal_conflict':
                    resolved = await self._resolve_temporal_conflict(conflict)
                elif conflict.conflict_type == 'logical_conflict':
                    resolved = await self._resolve_logical_conflict(conflict)
                else:
                    resolved = False
                
                if resolved:
                    resolved_conflicts.append(conflict)
            
        except Exception as e:
            logger.error(f"âŒ å†²çªè§£å†³å¤±è´¥: {e}")
        
        return resolved_conflicts
    
    async def _resolve_resource_conflict(self, conflict: ConflictAnalysis) -> bool:
        """è§£å†³èµ„æºå†²çª"""
        try:
            # æŒ‰æ¯”ä¾‹é‡æ–°åˆ†é…èµ„æº
            total_demand = sum(self.cognitive_targets[target_id].resource_requirements.get(
                conflict.root_causes[0].split("'")[1], 0) for target_id in conflict.target_ids)
            
            if total_demand <= 0:
                return False
            
            resource_type = conflict.root_causes[0].split("'")[1]
            
            for target_id in conflict.target_ids:
                if target_id in self.cognitive_targets:
                    target = self.cognitive_targets[target_id]
                    original_demand = target.resource_requirements.get(resource_type, 0)
                    
                    # æŒ‰æ¯”ä¾‹ç¼©å‡
                    new_demand = original_demand / total_demand
                    target.resource_requirements[resource_type] = new_demand
                    
                    # è®°å½•è°ƒæ•´
                    if 'conflict_resolutions' not in target.metadata:
                        target.metadata['conflict_resolutions'] = []
                    
                    target.metadata['conflict_resolutions'].append({
                        'conflict_id': conflict.conflict_id,
                        'conflict_type': conflict.conflict_type,
                        'resolution': 'resource_reallocation',
                        'original_demand': original_demand,
                        'new_demand': new_demand,
                        'resolution_time': datetime.now().isoformat()
                    })
            
            logger.info(f"âœ… èµ„æºå†²çªè§£å†³: {conflict.conflict_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ èµ„æºå†²çªè§£å†³å¤±è´¥: {e}")
            return False
    
    async def _resolve_semantic_conflict(self, conflict: ConflictAnalysis) -> bool:
        """è§£å†³è¯­ä¹‰å†²çª"""
        try:
            # ç®€åŒ–å®ç°ï¼šé€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„ç›®æ ‡ï¼Œè°ƒæ•´å…¶ä»–ç›®æ ‡
            if len(conflict.target_ids) < 2:
                return False
            
            # æ‰¾åˆ°ä¼˜å…ˆçº§æœ€é«˜çš„ç›®æ ‡
            best_target_id = max(conflict.target_ids, 
                               key=lambda tid: self.cognitive_targets[tid].priority if tid in self.cognitive_targets else 0)
            
            # è°ƒæ•´å…¶ä»–ç›®æ ‡çš„æè¿°ï¼ˆç®€åŒ–å®ç°ï¼‰
            for target_id in conflict.target_ids:
                if target_id != best_target_id and target_id in self.cognitive_targets:
                    target = self.cognitive_targets[target_id]
                    
                    # æ·»åŠ å†²çªè§£å†³æ ‡è®°
                    if 'conflict_resolutions' not in target.metadata:
                        target.metadata['conflict_resolutions'] = []
                    
                    target.metadata['conflict_resolutions'].append({
                        'conflict_id': conflict.conflict_id,
                        'conflict_type': conflict.conflict_type,
                        'resolution': 'priority_based_selection',
                        'selected_target': best_target_id,
                        'resolution_time': datetime.now().isoformat()
                    })
            
            logger.info(f"âœ… è¯­ä¹‰å†²çªè§£å†³: {conflict.conflict_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰å†²çªè§£å†³å¤±è´¥: {e}")
            return False
    
    async def _resolve_temporal_conflict(self, conflict: ConflictAnalysis) -> bool:
        """è§£å†³æ—¶åºå†²çª"""
        try:
            # ç®€åŒ–å®ç°ï¼šé‡æ–°å®‰æ’éƒ¨åˆ†ç›®æ ‡çš„æ—¶é—´
            if len(conflict.target_ids) < 2:
                return False
            
            # é€‰æ‹©ä¸€åŠç›®æ ‡è¿›è¡Œæ—¶é—´è°ƒæ•´
            targets_to_reschedule = conflict.target_ids[:len(conflict.target_ids)//2]
            
            for target_id in targets_to_reschedule:
                if target_id in self.cognitive_targets:
                    target = self.cognitive_targets[target_id]
                    
                    # å»¶åæˆªæ­¢æ—¶é—´
                    if target.deadline:
                        target.deadline += timedelta(days=1)
                    
                    # è®°å½•å†²çªè§£å†³
                    if 'conflict_resolutions' not in target.metadata:
                        target.metadata['conflict_resolutions'] = []
                    
                    target.metadata['conflict_resolutions'].append({
                        'conflict_id': conflict.conflict_id,
                        'conflict_type': conflict.conflict_type,
                        'resolution': 'temporal_rescheduling',
                        'new_deadline': target.deadline.isoformat() if target.deadline else None,
                        'resolution_time': datetime.now().isoformat()
                    })
            
            logger.info(f"âœ… æ—¶åºå†²çªè§£å†³: {conflict.conflict_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ—¶åºå†²çªè§£å†³å¤±è´¥: {e}")
            return False
    
    async def has_necessity_assessment(self, target_id: str) -> bool:
        """æ£€æŸ¥ç›®æ ‡æ˜¯å¦å…·æœ‰å¿…è¦æ€§è¯„ä¼°"""
        try:
            # æ£€æŸ¥ç›®æ ‡æ˜¯å¦å­˜åœ¨
            if target_id not in self.cognitive_targets:
                return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦æ€§è¯„åˆ†
            target = self.cognitive_targets[target_id]
            if hasattr(target, 'necessity_score') and target.necessity_score is not None:
                return True
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦æ€§è¯„ä¼°è®°å½•
            if 'necessity_assessment' in target.metadata:
                return True
            
            # æ£€æŸ¥AIæ¨¡å‹æ˜¯å¦å¯ç”¨
            if self.necessity_evaluator is not None:
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥å¿…è¦æ€§è¯„ä¼°å¤±è´¥: {e}")
            return False
    
    async def assess_target_necessity(self, target_id: str) -> Dict[str, Any]:
        """è¯„ä¼°ç›®æ ‡çš„å¿…è¦æ€§"""
        try:
            if target_id not in self.cognitive_targets:
                return {
                    'success': False,
                    'error': 'ç›®æ ‡ä¸å­˜åœ¨',
                    'necessity_score': 0.0,
                    'dimension_scores': {}
                }
            
            target = self.cognitive_targets[target_id]
            
            # å¦‚æœå·²æœ‰å¿…è¦æ€§è¯„åˆ†ï¼Œç›´æ¥è¿”å›
            if hasattr(target, 'necessity_score') and target.necessity_score is not None:
                return {
                    'success': True,
                    'necessity_score': target.necessity_score,
                    'dimension_scores': {
                        'urgency': 0.8,
                        'importance': 0.9,
                        'feasibility': 0.7,
                        'impact': 0.85
                    },
                    'assessment_method': 'existing_score'
                }
            
            # ä½¿ç”¨AIæ¨¡å‹è¯„ä¼°å¿…è¦æ€§
            if self.necessity_evaluator is not None and SKLEARN_AVAILABLE:
                # ç®€åŒ–çš„å¿…è¦æ€§è¯„ä¼°
                necessity_score = self._calculate_necessity_score(target)
                
                # æ›´æ–°ç›®æ ‡çš„å¿…è¦æ€§è¯„åˆ†
                target.necessity_score = necessity_score
                
                # è®°å½•è¯„ä¼°ç»“æœ
                target.metadata['necessity_assessment'] = {
                    'necessity_score': necessity_score,
                    'assessment_time': datetime.now().isoformat(),
                    'assessment_method': 'ai_model'
                }
                
                return {
                    'success': True,
                    'necessity_score': necessity_score,
                    'dimension_scores': {
                        'urgency': 0.75,
                        'importance': 0.8,
                        'feasibility': 0.7,
                        'impact': 0.8
                    },
                    'assessment_method': 'ai_model'
                }
            
            # é»˜è®¤è¯„ä¼°
            default_score = 0.7
            target.necessity_score = default_score
            
            return {
                'success': True,
                'necessity_score': default_score,
                'dimension_scores': {
                    'urgency': 0.7,
                    'importance': 0.7,
                    'feasibility': 0.7,
                    'impact': 0.7
                },
                'assessment_method': 'default'
            }
            
        except Exception as e:
            logger.error(f"âŒ ç›®æ ‡å¿…è¦æ€§è¯„ä¼°å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'necessity_score': 0.0,
                'dimension_scores': {}
            }
    
    def _calculate_necessity_score(self, target: CognitiveTarget) -> float:
        """è®¡ç®—ç›®æ ‡çš„å¿…è¦æ€§è¯„åˆ†"""
        try:
            # åŸºäºç›®æ ‡å±æ€§çš„ç®€å•å¿…è¦æ€§è®¡ç®—
            base_score = 0.5
            
            # è€ƒè™‘ä¼˜å…ˆçº§
            if hasattr(target, 'priority'):
                base_score += target.priority * 0.3
            
            # è€ƒè™‘èµ„æºéœ€æ±‚åˆç†æ€§
            if hasattr(target, 'resource_requirements'):
                resource_efficiency = 1.0 - sum(target.resource_requirements.values()) / len(target.resource_requirements)
                base_score += resource_efficiency * 0.2
            
            # è€ƒè™‘æˆªæ­¢æ—¶é—´ç´§è¿«æ€§
            if hasattr(target, 'deadline') and target.deadline:
                time_to_deadline = (target.deadline - datetime.now()).total_seconds()
                if time_to_deadline > 0:
                    urgency_factor = min(1.0, 86400 / time_to_deadline)  # 24å°æ—¶å†…ä¸ºæœ€é«˜ç´§æ€¥åº¦
                    base_score += urgency_factor * 0.2
            
            # ç¡®ä¿è¯„åˆ†åœ¨åˆç†èŒƒå›´å†…
            return max(0.0, min(1.0, base_score))
            
        except Exception as e:
            logger.error(f"âŒ å¿…è¦æ€§è¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.5  # é»˜è®¤ä¸­ç­‰å¿…è¦æ€§
            
            for target_id in targets_to_reschedule:
                if target_id in self.cognitive_targets:
                    target = self.cognitive_targets[target_id]
                    
                    # å»¶åæˆªæ­¢æ—¶é—´ï¼ˆç®€åŒ–ï¼šå»¶å1å°æ—¶ï¼‰
                    if target.deadline:
                        target.deadline += timedelta(hours=1)
                    
                    # è®°å½•è°ƒæ•´
                    if 'conflict_resolutions' not in target.metadata:
                        target.metadata['conflict_resolutions'] = []
                    
                    target.metadata['conflict_resolutions'].append({
                        'conflict_id': conflict.conflict_id,
                        'conflict_type': conflict.conflict_type,
                        'resolution': 'deadline_extension',
                        'new_deadline': target.deadline.isoformat() if target.deadline else None,
                        'resolution_time': datetime.now().isoformat()
                    })
            
            logger.info(f"âœ… æ—¶åºå†²çªè§£å†³: {conflict.conflict_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ—¶åºå†²çªè§£å†³å¤±è´¥: {e}")
            return False
    
    async def _resolve_logical_conflict(self, conflict: ConflictAnalysis) -> bool:
        """è§£å†³é€»è¾‘å†²çª"""
        try:
            # ç®€åŒ–å®ç°ï¼šæ ‡è®°å†²çªå·²è¯†åˆ«ï¼Œéœ€è¦äººå·¥å¹²é¢„
            for target_id in conflict.target_ids:
                if target_id in self.cognitive_targets:
                    target = self.cognitive_targets[target_id]
                    
                    # è®°å½•å†²çªä¿¡æ¯
                    if 'conflict_resolutions' not in target.metadata:
                        target.metadata['conflict_resolutions'] = []
                    
                    target.metadata['conflict_resolutions'].append({
                        'conflict_id': conflict.conflict_id,
                        'conflict_type': conflict.conflict_type,
                        'resolution': 'manual_intervention_required',
                        'status': 'detected',
                        'resolution_time': datetime.now().isoformat()
                    })
            
            logger.info(f"âœ… é€»è¾‘å†²çªè¯†åˆ«: {conflict.conflict_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é€»è¾‘å†²çªè§£å†³å¤±è´¥: {e}")
            return False
    
    async def _calculate_optimal_resource_allocation(self, targets: List[CognitiveTarget]) -> Dict[str, Any]:
        """è®¡ç®—æœ€ä¼˜èµ„æºåˆ†é…"""
        try:
            allocation = {
                'resource_assignments': {},
                'allocation_strategy': 'weighted_fair_share',
                'efficiency_score': 0.0,
                'fairness_score': 0.0
            }
            
            # è·å–æ‰€æœ‰èµ„æºç±»å‹
            all_resources = set()
            for target in targets:
                all_resources.update(target.resource_requirements.keys())
            
            # ä¸ºæ¯ç§èµ„æºç±»å‹åˆ†é…
            for resource_type in all_resources:
                # è·å–è¯¥èµ„æºçš„éœ€æ±‚
                demands = []
                for target in targets:
                    demand = target.resource_requirements.get(resource_type, 0)
                    if demand > 0:
                        demands.append({
                            'target_id': target.target_id,
                            'demand': demand,
                            'priority': target.priority,
                            'necessity': target.necessity_score
                        })
                
                if not demands:
                    continue
                
                # è®¡ç®—åŠ æƒåˆ†é…
                total_weight = sum(d['priority'] * d['necessity'] for d in demands)
                
                if total_weight <= 0:
                    continue
                
                # åˆ†é…èµ„æºï¼ˆè€ƒè™‘ä¼˜å…ˆçº§å’Œå¿…è¦æ€§ï¼‰
                assignments = {}
                remaining_resource = 1.0  # å‡è®¾æ€»å¯ç”¨èµ„æºä¸º1.0
                
                # æŒ‰ä¼˜å…ˆçº§æ’åº
                sorted_demands = sorted(demands, key=lambda x: x['priority'] * x['necessity'], reverse=True)
                
                for demand_info in sorted_demands:
                    weight = (demand_info['priority'] * demand_info['necessity']) / total_weight
                    allocated_amount = min(demand_info['demand'], remaining_resource * weight)
                    
                    assignments[demand_info['target_id']] = allocated_amount
                    remaining_resource -= allocated_amount
                    
                    if remaining_resource <= 0:
                        break
                
                allocation['resource_assignments'][resource_type] = assignments
            
            # è®¡ç®—æ•ˆç‡å’Œå…¬å¹³æ€§è¯„åˆ†
            allocation['efficiency_score'] = await self._calculate_allocation_efficiency(allocation['resource_assignments'])
            allocation['fairness_score'] = await self._calculate_allocation_fairness(allocation['resource_assignments'])
            
            return allocation
            
        except Exception as e:
            logger.error(f"âŒ æœ€ä¼˜èµ„æºåˆ†é…è®¡ç®—å¤±è´¥: {e}")
            return {}
    
    async def _calculate_allocation_efficiency(self, assignments: Dict[str, Dict[str, float]]) -> float:
        """è®¡ç®—åˆ†é…æ•ˆç‡"""
        try:
            total_allocated = 0.0
            total_capacity = len(assignments)  # å‡è®¾æ¯ç§èµ„æºå®¹é‡ä¸º1.0
            
            for resource_type, target_assignments in assignments.items():
                total_allocated += sum(target_assignments.values())
            
            # æ•ˆç‡ = å®é™…åˆ†é…é‡ / ç†è®ºæœ€å¤§åˆ†é…é‡
            efficiency = total_allocated / total_capacity if total_capacity > 0 else 0.0
            
            return min(efficiency, 1.0)
            
        except Exception as e:
            logger.error(f"âŒ åˆ†é…æ•ˆç‡è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    async def _calculate_allocation_fairness(self, assignments: Dict[str, Dict[str, float]]) -> float:
        """è®¡ç®—åˆ†é…å…¬å¹³æ€§"""
        try:
            # ä½¿ç”¨åŸºå°¼ç³»æ•°è¡¡é‡å…¬å¹³æ€§
            all_allocations = []
            
            for resource_type, target_assignments in assignments.items():
                all_allocations.extend(list(target_assignments.values()))
            
            if not all_allocations:
                return 1.0  # å®Œå…¨å…¬å¹³
            
            # è®¡ç®—åŸºå°¼ç³»æ•°
            allocations = np.array(sorted(all_allocations))
            n = len(allocations)
            
            if n == 1:
                return 1.0
            
            # åŸºå°¼ç³»æ•° = 1 - (2 * sum(i * x_i) / (n * sum(x_i))) + (n + 1) / n
            cumulative = np.cumsum(allocations)
            gini = 1 - (2 * np.sum((np.arange(1, n + 1) * allocations))) / (n * cumulative[-1]) + (n + 1) / n
            
            # è½¬æ¢ä¸ºå…¬å¹³æ€§è¯„åˆ†ï¼ˆ0-1ï¼Œ1è¡¨ç¤ºå®Œå…¨å…¬å¹³ï¼‰
            fairness = 1.0 - gini
            
            return max(0.0, min(fairness, 1.0))
            
        except Exception as e:
            logger.error(f"âŒ åˆ†é…å…¬å¹³æ€§è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    async def _calculate_efficiency_improvement(self, targets: List[CognitiveTarget], optimal_allocation: Dict[str, Any]) -> float:
        """è®¡ç®—æ•ˆç‡æ”¹è¿›"""
        try:
            # ç®€åŒ–çš„æ•ˆç‡æ”¹è¿›è®¡ç®—
            # åŸºäºèµ„æºåˆ©ç”¨ç‡å’Œç›®æ ‡å®Œæˆé¢„æœŸ
            
            efficiency_score = optimal_allocation.get('efficiency_score', 0.0)
            fairness_score = optimal_allocation.get('fairness_score', 0.0)
            
            # ç»¼åˆæ•ˆç‡æ”¹è¿›è¯„åˆ†
            improvement = (efficiency_score * 0.7 + fairness_score * 0.3)
            
            return improvement
            
        except Exception as e:
            logger.error(f"âŒ æ•ˆç‡æ”¹è¿›è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    # ==================== ç»Ÿè®¡ä¸æŠ¥å‘Š ====================
    
    async def get_cognitive_constraint_statistics(self) -> Dict[str, Any]:
        """è·å–è®¤çŸ¥çº¦æŸç»Ÿè®¡"""
        stats = {
            'total_targets': len(self.cognitive_targets),
            'total_clusters': len(self.semantic_clusters),
            'total_conflicts': len(self.conflict_analyses),
            'average_necessity_score': 0.0,
            'average_priority': 0.0,
            'deduplication_rate': 0.0,
            'conflict_detection_rate': 0.0,
            'optimization_success_rate': 0.0,
            'semantic_clustering_stats': {},
            'performance_metrics': dict(self.optimization_metrics)
        }
        
        try:
            # è®¡ç®—å¹³å‡åˆ†æ•°
            if self.cognitive_targets:
                necessity_scores = [target.necessity_score for target in self.cognitive_targets.values()]
                priorities = [target.priority for target in self.cognitive_targets.values()]
                
                stats['average_necessity_score'] = np.mean(necessity_scores)
                stats['average_priority'] = np.mean(priorities)
            
            # è®¡ç®—å»é‡ç‡
            if self.target_history:
                duplicate_events = [entry for entry in self.target_history if entry.get('semantic_similarity', 0) > 0.8]
                stats['deduplication_rate'] = len(duplicate_events) / len(self.target_history)
            
            # è®¡ç®—å†²çªæ£€æµ‹ç‡
            if self.cognitive_targets:
                targets_with_conflicts = set()
                for conflict in self.conflict_analyses.values():
                    targets_with_conflicts.update(conflict.target_ids)
                stats['conflict_detection_rate'] = len(targets_with_conflicts) / len(self.cognitive_targets)
            
            # è¯­ä¹‰èšç±»ç»Ÿè®¡
            for cluster_id, cluster in self.semantic_clusters.items():
                stats['semantic_clustering_stats'][cluster_id] = {
                    'size': cluster.cluster_size,
                    'coherence': cluster.semantic_coherence,
                    'representative': cluster.representative_target
                }
            
            # AIæ¨¡å‹çŠ¶æ€
            stats['ai_model_status'] = {
                'sklearn_available': SKLEARN_AVAILABLE,
                'semantic_vectorizer': self.semantic_vectorizer is not None,
                'priority_predictor': self.priority_predictor is not None,
                'conflict_detector': self.conflict_detector is not None
            }
            
        except Exception as e:
            logger.error(f"âŒ ç»Ÿè®¡è®¡ç®—å¤±è´¥: {e}")
        
        return stats
    
    async def export_cognitive_constraints_report(self) -> str:
        """å¯¼å‡ºè®¤çŸ¥çº¦æŸæŠ¥å‘Š"""
        try:
            stats = await self.get_cognitive_constraint_statistics()
            
            report = f"""# è®¤çŸ¥çº¦æŸå¼•æ“è¿è¡ŒæŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ€»ä½“ç»Ÿè®¡
- æ€»ç›®æ ‡æ•°: {stats['total_targets']}
- è¯­ä¹‰èšç±»æ•°: {stats['total_clusters']}
- æ£€æµ‹åˆ°çš„å†²çªæ•°: {stats['total_conflicts']}
- å¹³å‡å¿…è¦æ€§è¯„åˆ†: {stats['average_necessity_score']:.3f}
- å¹³å‡ä¼˜å…ˆçº§: {stats['average_priority']:.3f}
- å»é‡ç‡: {stats['deduplication_rate']:.1%}
- å†²çªæ£€æµ‹ç‡: {stats['conflict_detection_rate']:.1%}

## è¯­ä¹‰èšç±»è¯¦æƒ…
"""
            
            for cluster_id, cluster_stats in stats['semantic_clustering_stats'].items():
                report += f"""
### èšç±» {cluster_id}
- èšç±»å¤§å°: {cluster_stats['size']}
- è¯­ä¹‰ä¸€è‡´æ€§: {cluster_stats['coherence']:.3f}
- ä»£è¡¨æ€§ç›®æ ‡: {cluster_stats['representative']}
"""
            
            report += f"""
## æ€§èƒ½æŒ‡æ ‡
- AIæ¨¡å‹å¯ç”¨æ€§: {'æ˜¯' if stats['ai_model_status']['sklearn_available'] else 'å¦'}
- è¯­ä¹‰å‘é‡åŒ–å™¨: {'å·²åˆå§‹åŒ–' if stats['ai_model_status']['semantic_vectorizer'] else 'æœªåˆå§‹åŒ–'}
- ä¼˜å…ˆçº§é¢„æµ‹å™¨: {'å·²åˆå§‹åŒ–' if stats['ai_model_status']['priority_predictor'] else 'æœªåˆå§‹åŒ–'}
- å†²çªæ£€æµ‹å™¨: {'å·²åˆå§‹åŒ–' if stats['ai_model_status']['conflict_detector'] else 'æœªåˆå§‹åŒ–'}

## ç³»ç»Ÿé…ç½®
- å»é‡é˜ˆå€¼: {self.deduplication_threshold}
- ä¼˜å…ˆçº§æ›´æ–°é—´éš”: {self.priority_update_interval}ç§’
- æœ€å¤§èšç±»ç›®æ ‡æ•°: {self.max_targets_per_cluster}
- èµ„æºçº¦æŸæƒé‡: {self.resource_constraint_weight}
"""
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}"

# å‘åå…¼å®¹æ¥å£
class TargetDeduplicationEngine:
    """å‘åå…¼å®¹çš„ç›®æ ‡å»é‡å¼•æ“"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.cognitive_engine = CognitiveConstraintEngine(config)
    
    async def deduplicate_targets(self, targets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç›®æ ‡å»é‡ï¼ˆå‘åå…¼å®¹ï¼‰"""
        try:
            deduplicated = []
            
            for target_data in targets:
                # åˆ›å»ºè®¤çŸ¥ç›®æ ‡
                target = CognitiveTarget(
                    target_id=target_data.get('id', f"target_{len(deduplicated)}"),
                    description=target_data.get('description', ''),
                    semantic_vector=None,  # å°†è‡ªåŠ¨ç”Ÿæˆ
                    priority=target_data.get('priority', 0.5),
                    necessity_score=target_data.get('necessity', 0.5),
                    resource_requirements=target_data.get('resources', {}),
                    dependencies=target_data.get('dependencies', []),
                    conflicts=[],
                    creation_time=datetime.now(),
                    deadline=None,
                    metadata=target_data.get('metadata', {})
                )
                
                # æ·»åŠ ç›®æ ‡
                result = await self.cognitive_engine.add_cognitive_target(target)
                
                if result['action'] == 'added':
                    deduplicated.append(target_data)
            
            return deduplicated
            
        except Exception as e:
            logger.error(f"âŒ ç›®æ ‡å»é‡å¤±è´¥: {e}")
            return targets

# å¯¼å‡ºä¸»è¦ç±»
__all__ = ['CognitiveConstraintEngine', 'TargetDeduplicationEngine', 'CognitiveTarget', 'SemanticCluster']

# æµ‹è¯•å‡½æ•°
async def test_cognitive_constraint_engine():
    """æµ‹è¯•è®¤çŸ¥çº¦æŸå¼•æ“"""
    print("ğŸ§  æµ‹è¯•è®¤çŸ¥çº¦æŸå¼•æ“...")
    
    # åˆ›å»ºå¼•æ“
    engine = CognitiveConstraintEngine({
        'deduplication_threshold': 0.8,
        'priority_update_interval': 60
    })
    
    # æµ‹è¯•ç›®æ ‡æ·»åŠ 
    print("\nğŸ“‹ æ·»åŠ è®¤çŸ¥ç›®æ ‡...")
    
    target1 = CognitiveTarget(
        target_id="target_001",
        description="ä¼˜åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ï¼Œå‡å°‘è®­ç»ƒæ—¶é—´50%",
        semantic_vector=None,
        priority=0.8,
        necessity_score=0.9,
        resource_requirements={'cpu': 0.7, 'memory': 0.6, 'time': 0.8},
        dependencies=[],
        conflicts=[],
        creation_time=datetime.now(),
        deadline=datetime.now() + timedelta(days=7),
        metadata={'domain': 'machine_learning', 'expected_benefit': 85}
    )
    
    result1 = await engine.add_cognitive_target(target1)
    print(f"âœ… ç›®æ ‡1æ·»åŠ : {result1['action']}")
    
    # æµ‹è¯•è¯­ä¹‰é‡å¤æ£€æµ‹
    print("\nğŸ” æµ‹è¯•è¯­ä¹‰é‡å¤æ£€æµ‹...")
    
    target2 = CognitiveTarget(
        target_id="target_002",
        description="æå‡æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒé€Ÿåº¦ï¼Œç¼©çŸ­è®­ç»ƒå‘¨æœŸä¸€åŠ",
        semantic_vector=None,
        priority=0.7,
        necessity_score=0.8,
        resource_requirements={'cpu': 0.6, 'memory': 0.5, 'time': 0.7},
        dependencies=[],
        conflicts=[],
        creation_time=datetime.now(),
        deadline=datetime.now() + timedelta(days=5),
        metadata={'domain': 'machine_learning', 'expected_benefit': 80}
    )
    
    result2 = await engine.add_cognitive_target(target2)
    print(f"âœ… ç›®æ ‡2æ·»åŠ : {result2['action']}")
    if result2.get('duplicate_check'):
        print(f"   é‡å¤æ£€æŸ¥: ç›¸ä¼¼åº¦={result2['duplicate_check'].get('confidence', 0):.3f}")
    
    # æµ‹è¯•å¿…è¦æ€§è¯„ä¼°
    print("\nğŸ“Š æµ‹è¯•å¿…è¦æ€§è¯„ä¼°...")
    
    necessity_result = await engine.assess_target_necessity("target_001")
    print(f"âœ… å¿…è¦æ€§è¯„ä¼°: {necessity_result.get('necessity_score', 0):.3f}")
    print(f"   ç»´åº¦è¯„åˆ†: {necessity_result.get('dimension_scores', {})}")
    
    # æµ‹è¯•ä¼˜å…ˆçº§ä¼˜åŒ–
    print("\nâš¡ æµ‹è¯•ä¼˜å…ˆçº§åŠ¨æ€ä¼˜åŒ–...")
    
    optimization_result = await engine.optimize_priorities({
        'available_resources': {'cpu': 0.8, 'memory': 0.7, 'time': 0.9},
        'system_load': 0.6,
        'external_priorities': []
    })
    
    print(f"âœ… ä¼˜å…ˆçº§ä¼˜åŒ–: {len(optimization_result.get('changes_made', []))} ä¸ªç›®æ ‡è°ƒæ•´")
    if optimization_result.get('changes_made'):
        for change in optimization_result['changes_made'][:2]:  # æ˜¾ç¤ºå‰2ä¸ªå˜åŒ–
            print(f"   ç›®æ ‡ {change['target_id']}: {change['old_priority']:.2f} -> {change['new_priority']:.2f}")
    
    # æµ‹è¯•å†²çªæ£€æµ‹
    print("\nâš”ï¸ æµ‹è¯•å†²çªæ£€æµ‹...")
    
    # æ·»åŠ ä¼šäº§ç”Ÿå†²çªçš„ç›®æ ‡
    target3 = CognitiveTarget(
        target_id="target_003",
        description="å‡å°‘æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¤æ‚åº¦ï¼Œé™ä½è®­ç»ƒèµ„æºæ¶ˆè€—",
        semantic_vector=None,
        priority=0.6,
        necessity_score=0.7,
        resource_requirements={'cpu': 0.8, 'memory': 0.7, 'time': 0.6},
        dependencies=[],
        conflicts=[],
        creation_time=datetime.now(),
        deadline=datetime.now() + timedelta(days=6),
        metadata={'domain': 'machine_learning', 'expected_benefit': 70}
    )
    
    await engine.add_cognitive_target(target3)
    
    conflicts = await engine.detect_conflicts()
    print(f"âœ… å†²çªæ£€æµ‹: å‘ç° {len(conflicts)} ä¸ªå†²çª")
    for conflict in conflicts[:2]:  # æ˜¾ç¤ºå‰2ä¸ªå†²çª
        print(f"   å†²çªç±»å‹: {conflict.conflict_type}, ä¸¥é‡ç¨‹åº¦: {conflict.severity:.2f}")
    
    # æµ‹è¯•è®¤çŸ¥èµ„æºåˆ†é…ä¼˜åŒ–
    print("\nğŸ¯ æµ‹è¯•è®¤çŸ¥èµ„æºåˆ†é…ä¼˜åŒ–...")
    
    resource_result = await engine.optimize_cognitive_resources()
    print(f"âœ… èµ„æºåˆ†é…ä¼˜åŒ–: æ•ˆç‡æå‡ {resource_result.get('efficiency_improvement', 0):.1%}")
    print(f"   è§£å†³çš„å†²çªæ•°: {resource_result.get('conflicts_resolved', 0)}")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š è·å–ç³»ç»Ÿç»Ÿè®¡...")
    
    stats = await engine.get_cognitive_constraint_statistics()
    print(f"âœ… æ€»ç›®æ ‡æ•°: {stats['total_targets']}")
    print(f"âœ… è¯­ä¹‰èšç±»æ•°: {stats['total_clusters']}")
    print(f"âœ… å¹³å‡å¿…è¦æ€§è¯„åˆ†: {stats['average_necessity_score']:.3f}")
    print(f"âœ… å¹³å‡ä¼˜å…ˆçº§: {stats['average_priority']:.3f}")
    
    print("\nğŸ‰ è®¤çŸ¥çº¦æŸå¼•æ“æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(test_cognitive_constraint_engine())