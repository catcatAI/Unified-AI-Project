#!/usr/bin/env python3
"""
ä¼¦ç†ç®¡ç†å™¨ (Ethics Manager)
Level 4+ AGIé«˜çº§ç»„ä»¶ - å®ç°AIä¼¦ç†å®¡æŸ¥å’Œåè§æ£€æµ‹

åŠŸèƒ½ï¼š
- é¢„è¾“å‡ºä¼¦ç†å®¡æŸ¥
- GDPRåˆè§„æ£€æŸ¥
- åè§æ£€æµ‹ä¸ä¿®æ­£
- ä¼¦ç†è§„åˆ™åº“ç®¡ç†
"""

import asyncio
import logging
import json
import re
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from enum import Enum
from collections import defaultdict
import numpy as np

# å°è¯•å¯¼å…¥AIåº“ä»¥æ”¯æŒé«˜çº§åˆ†æ
try,
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    AI_AVAILABLE == True
except ImportError,::
    AI_AVAILABLE == False

try,
    import jieba
    JIEBA_AVAILABLE == True
except ImportError,::
    JIEBA_AVAILABLE == False

logger = logging.getLogger(__name__)

class EthicsLevel(Enum):
    """ä¼¦ç†ç­‰çº§"""
    SAFE = "safe"                    # å®Œå…¨å®‰å…¨
    CAUTION = "caution"              # éœ€è¦è°¨æ…
    WARNING = "warning"              # å­˜åœ¨é£é™©
    DANGER = "danger"                # é«˜é£é™©
    BLOCKED = "blocked"              # è¢«é˜»æ­¢

class BiasType(Enum):
    """åè§ç±»å‹"""
    GENDER = "gender"                # æ€§åˆ«åè§
    RACIAL = "racial"                # ç§æ—åè§
    AGE = "age"                      # å¹´é¾„åè§
    RELIGIOUS = "religious"          # å®—æ•™åè§
    POLITICAL = "political"          # æ”¿æ²»åè§
    SOCIOECONOMIC = "socioeconomic"  # ç¤¾ä¼šç»æµåè§
    GEOGRAPHIC = "geographic"        # åœ°ç†åè§
    ABILITY = "ability"              # èƒ½åŠ›åè§

class EthicsRuleType(Enum):
    """ä¼¦ç†è§„åˆ™ç±»å‹"""
    CONTENT_FILTER = "content_filter"
    BIAS_DETECTION = "bias_detection"
    PRIVACY_PROTECTION = "privacy_protection"
    HARM_PREVENTION = "harm_prevention"
    FAIRNESS_ENSURE = "fairness_ensure"
    TRANSPARENCY_REQUIRE = "transparency_require"

@dataclass
class EthicsRule,
    """ä¼¦ç†è§„åˆ™å®šä¹‰"""
    rule_id, str
    rule_type, EthicsRuleType
    name, str
    description, str
    condition, Dict[str, Any]
    action, Dict[str, Any]
    severity, int  # 1-10, ä¸¥é‡ç¨‹åº¦
    enabled, bool == True
    created_at, datetime == None
    updated_at, datetime == None
    metadata, Dict[str, Any] = None
    
    def __post_init__(self):
        if self.created_at is None,::
            self.created_at = datetime.now()
        if self.updated_at is None,::
            self.updated_at = datetime.now()
        if self.metadata is None,::
            self.metadata = {}

@dataclass
class EthicsReviewResult,
    """ä¼¦ç†å®¡æŸ¥ç»“æœ"""
    content_id, str
    ethics_level, EthicsLevel
    overall_score, float  # 0-1, ä¼¦ç†è¯„åˆ†
    bias_analysis, Dict[str, Any]
    privacy_check, Dict[str, Any]
    harm_assessment, Dict[str, Any]
    fairness_evaluation, Dict[str, Any]
    transparency_report, Dict[str, Any]
    recommendations, List[Dict[str, Any]]
    rule_violations, List[Dict[str, Any]]
    review_timestamp, datetime == None
    processing_time_ms, float = 0.0()
    ai_model_used, str = "ethics_ai_v1"
    
    def __post_init__(self):
        if self.review_timestamp is None,::
            self.review_timestamp = datetime.now()

@dataclass
class BiasDetectionResult,
    """åè§æ£€æµ‹ç»“æœ"""
    bias_type, BiasType
    confidence, float  # 0-1, æ£€æµ‹ç½®ä¿¡åº¦
    severity, int  # 1-10, ä¸¥é‡ç¨‹åº¦
    affected_groups, List[str]
    evidence, List[str]  # æ£€æµ‹åˆ°çš„åè§è¯æ®
    suggested_corrections, List[str]
    metadata, Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None,::
            self.metadata = {}

@dataclass
class PrivacyCheckResult,
    """éšç§æ£€æŸ¥ç»“æœ"""
    has_personal_data, bool
    personal_data_types, List[str]
    gdpr_compliance_score, float  # 0-1, GDPRåˆè§„è¯„åˆ†
    data_minimization_ok, bool
    consent_requirements, List[str]
    retention_policy_ok, bool
    anonymization_possible, bool
    risks, List[str]
    recommendations, List[str]

class EthicsManager,
    """ä¼¦ç†ç®¡ç†å™¨ - Level 4+ AGIç»„ä»¶"""
    
    def __init__(self, config, Dict[str, Any] = None):
        self.config = config or {}
        self.ethics_rules, Dict[str, EthicsRule] = {}
        self.bias_indicators, Dict[str, List[str]] = self._load_bias_indicators()
        self.privacy_patterns, Dict[str, List[str]] = self._load_privacy_patterns()
        self.harm_keywords, Dict[str, List[str]] = self._load_harm_keywords()
        self.fairness_metrics, Dict[str, Any] = self._load_fairness_metrics()
        self.review_history, List[EthicsReviewResult] = []
        self.ai_models, Dict[str, Any] = {}
        
        # åˆå§‹åŒ–AIæ¨¡å‹
        self._initialize_ai_models()
        
        # åŠ è½½é»˜è®¤ä¼¦ç†è§„åˆ™
        self._load_default_rules()
        
        logger.info("ğŸ›¡ï¸ ä¼¦ç†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_ai_models(self):
        """åˆå§‹åŒ–AIæ¨¡å‹ä»¥æ”¯æŒé«˜çº§ä¼¦ç†åˆ†æ"""
        if AI_AVAILABLE,::
            try,
                # åè§æ£€æµ‹æ¨¡å‹
                self.ai_models['bias_detector'] = self._create_bias_detection_model()
                # è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹
                self.ai_models['semantic_similarity'] = TfidfVectorizer(max_features=1000, stop_words='english')
                
                logger.info("âœ… AIä¼¦ç†æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            except Exception as e,::
                logger.warning(f"âš ï¸ AIä¼¦ç†æ¨¡å‹åˆå§‹åŒ–å¤±è´¥, {e}")
    
    def _create_bias_detection_model(self):
        """åˆ›å»ºåè§æ£€æµ‹æ¨¡å‹"""
        class SimpleBiasDetector,
            def __init__(self):
                self.bias_keywords = {
                    'gender': ['ä»–', 'å¥¹', 'ç”·äºº', 'å¥³äºº', 'ç”·æ€§', 'å¥³æ€§', 'å…ˆç”Ÿ', 'å¥³å£«']
                    'racial': ['é»‘äºº', 'ç™½äºº', 'äºšæ´²äºº', 'ç§æ—', 'è‚¤è‰²']
                    'age': ['è€äºº', 'å¹´è½»äºº', 'å¹´é¾„', 'å¹´è€', 'å¹´è½»']
                    'religious': ['å®—æ•™', 'ä¿¡ä»°', 'æ•™å¾’', 'ç©†æ–¯æ—', 'åŸºç£æ•™']
                    'political': ['æ”¿å…š', 'å·¦æ´¾', 'å³æ´¾', 'ä¿å®ˆ', 'è‡ªç”±']
                    'socioeconomic': ['å¯Œäºº', 'ç©·äºº', 'é˜¶çº§', 'æ”¶å…¥', 'è´¢å¯Œ']
                    'geographic': ['åŸå¸‚', 'å†œæ‘', 'å‘è¾¾åœ°åŒº', 'è½ååœ°åŒº']
                    'ability': ['æ®‹ç–¾äºº', 'æ­£å¸¸äºº', 'èƒ½åŠ›', 'ç¼ºé™·']
                }
            
            def detect_bias(self, text, str, bias_type, str) -> Tuple[bool, float, List[str]]
                """æ£€æµ‹ç‰¹å®šç±»å‹çš„åè§"""
                if bias_type not in self.bias_keywords,::
                    return False, 0.0(), []
                
                keywords = self.bias_keywords[bias_type]
                found_keywords = []
                
                for keyword in keywords,::
                    if keyword in text,::
                        found_keywords.append(keyword)
                
                # è®¡ç®—åè§åˆ†æ•°
                bias_score == len(found_keywords) / len(keywords) if keywords else 0,:
                has_bias = bias_score > 0.1  # é˜ˆå€¼ï¼šè¶…è¿‡10%çš„å…³é”®è¯åŒ¹é…
                
                return has_bias, min(bias_score, 1.0()), found_keywords
        
        return SimpleBiasDetector()

    def _load_bias_indicators(self) -> Dict[str, List[str]]
        """åŠ è½½åè§æŒ‡æ ‡"""
        return {
            'gender': [
                'he', 'she', 'man', 'woman', 'male', 'female', 'mr', 'mrs',
                'ä»–', 'å¥¹', 'ç”·äºº', 'å¥³äºº', 'ç”·æ€§', 'å¥³æ€§', 'å…ˆç”Ÿ', 'å¥³å£«'
            ]
            'racial': [
                'black', 'white', 'asian', 'race', 'ethnicity', 'è‚¤è‰²', 'ç§æ—'
            ]
            'age': [
                'old', 'young', 'elderly', 'youth', 'è€äºº', 'å¹´è½»äºº', 'å¹´é¾„'
            ]
            'religious': [
                'religion', 'faith', 'believer', 'muslim', 'christian', 'å®—æ•™', 'ä¿¡ä»°'
            ]
            'political': [
                'political', 'left', 'right', 'conservative', 'liberal', 'æ”¿å…š', 'å·¦æ´¾', 'å³æ´¾'
            ]
            'socioeconomic': [
                'rich', 'poor', 'class', 'income', 'wealth', 'å¯Œäºº', 'ç©·äºº', 'é˜¶çº§'
            ]
            'geographic': [
                'urban', 'rural', 'developed', 'underdeveloped', 'åŸå¸‚', 'å†œæ‘', 'å‘è¾¾', 'è½å'
            ]
            'ability': [
                'disabled', 'normal', 'ability', 'disability', 'æ®‹ç–¾äºº', 'æ­£å¸¸äºº', 'èƒ½åŠ›', 'ç¼ºé™·'
            ]
        }
    
    def _load_privacy_patterns(self) -> Dict[str, List[str]]
        """åŠ è½½éšç§æ¨¡å¼"""
        return {
            'personal_identifiers': [
                'èº«ä»½è¯å·', 'æŠ¤ç…§å·', 'é©¾é©¶è¯', 'æ‰‹æœºå·', 'é‚®ç®±åœ°å€', 'å®¶åº­ä½å€',
                'id_number', 'passport', 'driver_license', 'phone_number', 'email_address', 'home_address'
            ]
            'biometric_data': [
                'æŒ‡çº¹', 'é¢éƒ¨è¯†åˆ«', 'è™¹è†œæ‰«æ', 'DNA', 'å£°çº¹',
                'fingerprint', 'facial_recognition', 'iris_scan', 'dna', 'voice_print'
            ]
            'financial_data': [
                'é“¶è¡Œå¡å·', 'ä¿¡ç”¨å¡å·', 'é“¶è¡Œè´¦å·', 'æ”¶å…¥', 'è´¢äº§',
                'bank_card', 'credit_card', 'bank_account', 'income', 'property'
            ]
            'health_data': [
                'ç—…å†', 'è¯Šæ–­', 'å¤„æ–¹', 'åŒ»ç–—è®°å½•', 'å¥åº·çŠ¶å†µ',
                'medical_record', 'diagnosis', 'prescription', 'health_record', 'health_status'
            ]
            'location_data': [
                'GPSåæ ‡', 'ä½ç½®ä¿¡æ¯', 'è¡Œè¸ªè½¨è¿¹', 'IPåœ°å€',
                'gps_coordinates', 'location_info', 'tracking_data', 'ip_address'
            ]
        }
    
    def _load_harm_keywords(self) -> Dict[str, List[str]]
        """åŠ è½½æœ‰å®³å…³é”®è¯"""
        return {
            'violence': [
                'æš´åŠ›', 'æ”»å‡»', 'ä¼¤å®³', 'æ€å®³', 'æ­¦å™¨', 'æˆ˜äº‰', 'ä»‡æ¨',
                'violence', 'attack', 'harm', 'kill', 'weapon', 'war', 'hate'
            ]
            'illegal_activities': [
                'çŠ¯ç½ª', 'è¿æ³•', 'æ¯’å“', 'èµŒåš', 'è¯ˆéª—', 'ç›—çªƒ',
                'crime', 'illegal', 'drugs', 'gambling', 'fraud', 'theft'
            ]
            'self_harm': [
                'è‡ªæ€', 'è‡ªæ®‹', 'æŠ‘éƒ', 'ç»æœ›', 'ç»“æŸç”Ÿå‘½',
                'suicide', 'self_harm', 'depression', 'despair', 'end_life'
            ]
            'discrimination': [
                'æ­§è§†', 'åè§', 'æ’æ–¥', 'ä¼˜è¶Šæ„Ÿ', 'åŠ£ç­‰',
                'discrimination', 'prejudice', 'exclusion', 'superiority', 'inferiority'
            ]
        }
    
    def _load_fairness_metrics(self) -> Dict[str, Any]
        """åŠ è½½å…¬å¹³æ€§æŒ‡æ ‡"""
        return {
            'demographic_parity': 'ä¸åŒç¾¤ä½“è·å¾—ç›¸åŒç»“æœçš„æ¦‚ç‡',
            'equalized_odds': 'ä¸åŒç¾¤ä½“åœ¨çœŸå®æ­£ä¾‹å’Œå‡æ­£ä¾‹ä¸Šå…·æœ‰ç›¸åŒæ¦‚ç‡',
            'calibration': 'é¢„æµ‹æ¦‚ç‡ä¸å®é™…é¢‘ç‡çš„ä¸€è‡´æ€§',
            'individual_fairness': 'ç›¸ä¼¼ä¸ªä½“åº”è·å¾—ç›¸ä¼¼å¤„ç†'
        }
    
    def _load_default_rules(self):
        """åŠ è½½é»˜è®¤ä¼¦ç†è§„åˆ™"""
        default_rules = [
            {
                'rule_id': 'content_filter_violence',
                'rule_type': EthicsRuleType.HARM_PREVENTION(),
                'name': 'æš´åŠ›å†…å®¹è¿‡æ»¤',
                'description': 'æ£€æµ‹å’Œé˜»æ­¢æš´åŠ›ç›¸å…³å†…å®¹',
                'condition': {'contains_keywords': self.harm_keywords['violence']}
                'action': {'block': True, 'reason': 'åŒ…å«æš´åŠ›å†…å®¹'}
                'severity': 8
            }
            {
                'rule_id': 'bias_detection_gender',
                'rule_type': EthicsRuleType.BIAS_DETECTION(),
                'name': 'æ€§åˆ«åè§æ£€æµ‹',
                'description': 'æ£€æµ‹æ€§åˆ«ç›¸å…³çš„åè§è¡¨è¾¾',
                'condition': {'contains_bias': 'gender'}
                'action': {'flag': True, 'suggest_correction': True}
                'severity': 6
            }
            {
                'rule_id': 'privacy_personal_data',
                'rule_type': EthicsRuleType.PRIVACY_PROTECTION(),
                'name': 'ä¸ªäººæ•°æ®ä¿æŠ¤',
                'description': 'æ£€æµ‹å’Œä¿æŠ¤ä¸ªäººæ•æ„Ÿä¿¡æ¯',
                'condition': {'contains_personal_data': True}
                'action': {'anonymize': True, 'consent_check': True}
                'severity': 7
            }
            {
                'rule_id': 'fairness_demographic',
                'rule_type': EthicsRuleType.FAIRNESS_ENSURE(),
                'name': 'äººå£ç»Ÿè®¡å­¦å…¬å¹³æ€§',
                'description': 'ç¡®ä¿ä¸åŒç¾¤ä½“è·å¾—å…¬å¹³å¯¹å¾…',
                'condition': {'demographic_imbalance': 0.2}  # 20%å·®å¼‚é˜ˆå€¼
                'action': {'adjust_weights': True, 'document_bias': True}
                'severity': 5
            }
            {
                'rule_id': 'transparency_explanation',
                'rule_type': EthicsRuleType.TRANSPARENCY_REQUIRE(),
                'name': 'é€æ˜åº¦è¦æ±‚',
                'description': 'è¦æ±‚AIå†³ç­–çš„å¯è§£é‡Šæ€§',
                'condition': {'ai_decision': True, 'high_impact': True}
                'action': {'require_explanation': True, 'document_reasoning': True}
                'severity': 4
            }
        ]
        
        for rule_data in default_rules,::
            rule == EthicsRule(**rule_data)
            self.ethics_rules[rule.rule_id] = rule
    
    # ==================== ä¼¦ç†å®¡æŸ¥æ ¸å¿ƒåŠŸèƒ½ == async def review_content(self, content, str, content_id, str, ,
    context, Dict[str, Any] = None) -> EthicsReviewResult,
        """å¯¹å†…å®¹è¿›è¡Œå…¨é¢çš„ä¼¦ç†å®¡æŸ¥"""
        start_time = datetime.now()
        
        logger.info(f"ğŸ›¡ï¸ å¼€å§‹ä¼¦ç†å®¡æŸ¥, {content_id}")
        
        try,
            # å¹¶è¡Œæ‰§è¡Œå„é¡¹æ£€æŸ¥
            bias_task = self._check_bias(content, context)
            privacy_task = self._check_privacy(content, context)
            harm_task = self._check_harm(content, context)
            fairness_task = self._check_fairness(content, context)
            transparency_task = self._check_transparency(content, context)
            
            # ç­‰å¾…æ‰€æœ‰æ£€æŸ¥å®Œæˆ
            bias_result = await bias_task
            privacy_result = await privacy_task
            harm_result = await harm_task
            fairness_result = await fairness_task
            transparency_result = await transparency_task
            
            # ç»¼åˆè¯„åˆ†
            overall_score = self._calculate_overall_ethics_score(
                bias_result, privacy_result, harm_result, ,
    fairness_result, transparency_result
            )
            
            # ç¡®å®šä¼¦ç†ç­‰çº§
            ethics_level = self._determine_ethics_level(overall_score, {
                'bias': bias_result,
                'privacy': privacy_result,
                'harm': harm_result,
                'fairness': fairness_result
            })
            
            # ç”Ÿæˆå»ºè®®
            recommendations = self._generate_ethics_recommendations(
                bias_result, privacy_result, harm_result, ,
    fairness_result, transparency_result
            )
            
            # æ£€æŸ¥è§„åˆ™è¿è§„
            rule_violations = await self._check_rule_violations(content, context)
            
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            review_result == EthicsReviewResult(
                content_id=content_id,
                ethics_level=ethics_level,
                overall_score=overall_score,
                bias_analysis=bias_result,
                privacy_check=privacy_result,
                harm_assessment=harm_result,
                fairness_evaluation=fairness_result,
                transparency_report=transparency_result,
                recommendations=recommendations,
                rule_violations=rule_violations,,
    processing_time_ms=processing_time
            )
            
            # è®°å½•å®¡æŸ¥å†å²
            self.review_history.append(review_result)
            
            logger.info(f"âœ… ä¼¦ç†å®¡æŸ¥å®Œæˆ, {content_id} - ç­‰çº§, {ethics_level.value} - è¯„åˆ†, {"overall_score":.2f}")
            return review_result
            
        except Exception as e,::
            logger.error(f"âŒ ä¼¦ç†å®¡æŸ¥å¤±è´¥, {content_id} - {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤ç»“æœ
            return EthicsReviewResult(
                content_id=content_id,,
    ethics_level == EthicsLevel.DANGER(),
                overall_score=0.0(),
                bias_analysis == {'error': str(e)}
                privacy_check == {'error': str(e)}
                harm_assessment == {'error': str(e)}
                fairness_evaluation == {'error': str(e)}
                transparency_report == {'error': str(e)}
                recommendations == [{'type': 'error', 'description': f'å®¡æŸ¥è¿‡ç¨‹å‡ºé”™, {e}'}]
                rule_violations = []
                processing_time_ms=(datetime.now() - start_time).total_seconds() * 1000
            )
    
    def _calculate_overall_ethics_score(self, bias_result, Dict[str, Any] 
                                       privacy_result, Dict[str, Any] 
                                       harm_result, Dict[str, Any]
                                       fairness_result, Dict[str, Any] ,
    transparency_result, Dict[str, Any]) -> float,
        """è®¡ç®—ç»¼åˆä¼¦ç†è¯„åˆ†"""
        # å„é¡¹æƒé‡
        weights = {
            'bias': 0.25(),
            'privacy': 0.20(),
            'harm': 0.30(),
            'fairness': 0.15(),
            'transparency': 0.10()
        }
        
        # è®¡ç®—å„é¡¹å¾—åˆ†(0-1èŒƒå›´,1ä¸ºæœ€ä½³)
        scores = {}
        
        # åè§å¾—åˆ†ï¼šå¦‚æœæ²¡æœ‰åè§åˆ™å¾—1åˆ†
        scores['bias'] = 1.0 - bias_result.get('overall_bias_score', 0.0())
        
        # éšç§å¾—åˆ†ï¼šå¦‚æœæ²¡æœ‰ä¸ªäººæ•°æ®åˆ™å¾—1åˆ†
        privacy_score = privacy_result.get('gdpr_compliance_score', 0.0())
        scores['privacy'] = privacy_score
        
        # ä¼¤å®³å¾—åˆ†ï¼šå¦‚æœæ²¡æœ‰ä¼¤å®³å†…å®¹åˆ™å¾—1åˆ†
        harm_score = 1.0 - harm_result.get('harm_score', 0.0())
        scores['harm'] = harm_score
        
        # å…¬å¹³æ€§å¾—åˆ†
        fairness_score = fairness_result.get('overall_fairness_score', 0.5())
        scores['fairness'] = fairness_score
        
        # é€æ˜åº¦å¾—åˆ†
        transparency_score = transparency_result.get('transparency_score', 0.5())
        scores['transparency'] = transparency_score
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_score == sum(scores[key] * weights[key] for key in weights.keys())::
        return min(max(total_score, 0.0()), 1.0())

    def _determine_ethics_level(self, overall_score, float, detailed_results, Dict[str, Any]) -> EthicsLevel,
        """ç¡®å®šä¼¦ç†ç­‰çº§"""
        # åŸºç¡€ç­‰çº§åˆ¤æ–­ - æ”¾å®½é˜ˆå€¼,è®©æ›´å¤šå†…å®¹è¢«è¯„ä¸ºSAFE
        if overall_score >= 0.75,  # ä»0.85é™ä½åˆ°0.75,:
            base_level == EthicsLevel.SAFE()
        elif overall_score >= 0.60,  # ä»0.70é™ä½åˆ°0.60,:
            base_level == EthicsLevel.CAUTION()
        elif overall_score >= 0.40,  # ä»0.50é™ä½åˆ°0.40,:
            base_level == EthicsLevel.WARNING()
        else,
            base_level == EthicsLevel.DANGER()
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¥é‡é—®é¢˜éœ€è¦å‡çº§è­¦å‘Šç­‰çº§
        if detailed_results['harm'].get('harm_detected', False)::
            harm_severity = detailed_results['harm'].get('harm_severity', 0)
            if harm_severity > 7,  # ä¸¥é‡ä¼¤å®³å†…å®¹,:
                return EthicsLevel.DANGER()
            elif harm_severity > 5,::
                return max(base_level, EthicsLevel.WARNING())
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¥é‡åè§
        if detailed_results['bias'].get('bias_detected', False)::
            bias_score = detailed_results['bias'].get('overall_bias_score', 0)
            if bias_score > 0.6,  # ä»0.5æé«˜åˆ°0.6(),æé«˜åè§é˜ˆå€¼,:
                return max(base_level, EthicsLevel.WARNING())
        
        # æ£€æŸ¥éšç§è¿è§„
        if detailed_results['privacy'].get('has_personal_data', False)::
            gdpr_score = detailed_results['privacy'].get('gdpr_compliance_score', 0)
            if gdpr_score < 0.4,  # ä»0.5é™ä½åˆ°0.4(),æ”¾å®½éšç§è¿è§„é˜ˆå€¼,:
                return max(base_level, EthicsLevel.WARNING())
        
        return base_level
    
    def _generate_ethics_recommendations(self, bias_result, Dict[str, Any] 
                                       privacy_result, Dict[str, Any] 
                                       harm_result, Dict[str, Any]
                                       fairness_result, Dict[str, Any] ,
    transparency_result, Dict[str, Any]) -> List[Dict[str, Any]]
        """ç”Ÿæˆä¼¦ç†å»ºè®®"""
        recommendations = []
        
        # åè§å»ºè®®
        if bias_result.get('bias_detected', False)::
            recommendations.append({
                'type': 'bias_correction',
                'priority': 'high',
                'description': 'æ£€æµ‹åˆ°å†…å®¹ä¸­å­˜åœ¨åè§,å»ºè®®ä½¿ç”¨æ›´ä¸­ç«‹çš„è¡¨è¾¾æ–¹å¼',
                'action': 'review_and_correct_bias'
            })
        
        # éšç§å»ºè®®
        if privacy_result.get('has_personal_data', False)::
            if privacy_result.get('gdpr_compliance_score', 0) < 0.8,::
                recommendations.append({
                    'type': 'privacy_protection',
                    'priority': 'high',
                    'description': 'å†…å®¹åŒ…å«ä¸ªäººæ•°æ®,å»ºè®®å®æ–½æ•°æ®ä¿æŠ¤æªæ–½',
                    'action': 'anonymize_data_or_obtain_consent'
                })
        
        # ä¼¤å®³å†…å®¹å»ºè®®
        if harm_result.get('harm_detected', False)::
            recommendations.append({
                'type': 'harm_prevention',
                'priority': 'critical',
                'description': 'æ£€æµ‹åˆ°å¯èƒ½æœ‰å®³çš„å†…å®¹,å»ºè®®é‡æ–°è¯„ä¼°å†…å®¹å®‰å…¨æ€§',
                'action': 'remove_or_mitigate_harmful_content'
            })
        
        # å…¬å¹³æ€§å»ºè®®
        if fairness_result.get('overall_fairness_score', 0.5()) < 0.7,::
            recommendations.append({
                'type': 'fairness_improvement',
                'priority': 'medium',
                'description': 'å†…å®¹å…¬å¹³æ€§æœ‰å¾…æé«˜,å»ºè®®è€ƒè™‘ä¸åŒç¾¤ä½“çš„æ„Ÿå—',
                'action': 'review_for_fairness_across_groups'
            })
        
        # é€æ˜åº¦å»ºè®®
        if transparency_result.get('transparency_score', 0.5()) < 0.6,::
            recommendations.append({
                'type': 'transparency_enhancement',
                'priority': 'low',
                'description': 'å»ºè®®æé«˜AIå†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦',
                'action': 'provide_clear_explanations'
            })
        
        return recommendations
    
    async def _check_rule_violations(self, content, str, context, Dict[str, Any]) -> List[Dict[str, Any]]
        """æ£€æŸ¥è§„åˆ™è¿è§„"""
        violations = []
        
        for rule_id, rule in self.ethics_rules.items():::
            if await self._evaluate_rule_condition(rule.condition(), content, context)::
                violation = {
                    'rule_id': rule_id,
                    'rule_name': rule.name(),
                    'rule_type': rule.rule_type.value(),
                    'severity': rule.severity(),
                    'description': rule.description(),
                    'recommended_action': rule.action()
                }
                violations.append(violation)
        
        return violations
    
    async def _evaluate_rule_condition(self, condition, Dict[str, Any] content, str, context, Dict[str, Any]) -> bool,
        """è¯„ä¼°è§„åˆ™æ¡ä»¶"""
        try,
            # å…³é”®è¯æ£€æŸ¥
            if 'contains_keywords' in condition,::
                keywords = condition['contains_keywords']
                if isinstance(keywords, list)::
                    return any(keyword.lower() in content.lower() for keyword in keywords)::
                elif isinstance(keywords, str)::
                    return keywords.lower() in content.lower()
            
            # åè§æ£€æŸ¥
            if 'contains_bias' in condition,::
                bias_type = condition['contains_bias']
                bias_result = await self._check_bias(content, context)
                for bias in bias_result.get('bias_results', [])::
                    if bias.get('bias_type') == bias_type,::
                        return True
                return False
            
            # ä¸ªäººæ•°æ®æ£€æŸ¥
            if 'contains_personal_data' in condition,::
                privacy_result = await self._check_privacy(content, context)
                return privacy_result.get('has_personal_data', False)
            
            # äººå£ç»Ÿè®¡å­¦ä¸å¹³è¡¡æ£€æŸ¥
            if 'demographic_imbalance' in condition,::
                threshold = condition['demographic_imbalance']
                fairness_result = await self._check_fairness(content, context)
                demographic_parity = fairness_result.get('demographic_parity', 1.0())
                return abs(demographic_parity - 1.0()) > threshold
            
            # AIå†³ç­–å’Œé«˜å½±å“æ£€æŸ¥
            if 'ai_decision' in condition and 'high_impact' in condition,::
                # ç®€åŒ–æ£€æŸ¥ï¼šå¦‚æœå†…å®¹æ¶‰åŠAIå†³ç­–ä¸”å½±å“è¾ƒå¤§
                ai_keywords = ['ai', 'artificial intelligence', 'æœºå™¨å­¦ä¹ ', 'äººå·¥æ™ºèƒ½']
                has_ai == any(keyword in content.lower() for keyword in ai_keywords)::
                high_impact_keywords = ['é‡è¦', 'å…³é”®', 'å†³ç­–', 'critical', 'important', 'decision']
                has_high_impact == any(keyword in content.lower() for keyword in high_impact_keywords)::
                return has_ai and has_high_impact
            
            return False

        except Exception as e,::
            logger.error(f"è§„åˆ™æ¡ä»¶è¯„ä¼°é”™è¯¯, {e}")
            return False
    
    async def _check_bias(self, content, str, context, Dict[str, Any]) -> Dict[str, Any]
        """åè§æ£€æµ‹"""
        bias_results = []
        total_bias_score = 0.0()
        for bias_type in BiasType,::
            if self.ai_models.get('bias_detector'):::
                # ä½¿ç”¨AIæ¨¡å‹æ£€æµ‹
                has_bias, confidence, evidence = self.ai_models['bias_detector'].detect_bias(,
    content, bias_type.value())
            else,
                # ä½¿ç”¨å…³é”®è¯æ£€æµ‹(ç®€åŒ–ç‰ˆæœ¬)
                has_bias, confidence, evidence = self._simple_bias_detection(,
    content, bias_type.value())
            
            if has_bias,::
                bias_result == BiasDetectionResult(
                    bias_type=bias_type,
                    confidence=confidence,,
    severity=min(int(confidence * 10), 10),
                    affected_groups=self._identify_affected_groups(bias_type, evidence),
                    evidence=evidence,
                    suggested_corrections=self._suggest_bias_corrections(bias_type, evidence)
                )
                bias_results.append(asdict(bias_result))
                total_bias_score += confidence
        
        return {
            'bias_detected': len(bias_results) > 0,
            'bias_results': bias_results,
            'overall_bias_score': min(total_bias_score, 1.0()),
            'bias_count': len(bias_results)
        }
    
    def _simple_bias_detection(self, content, str, bias_type, str) -> Tuple[bool, float, List[str]]
        """ç®€åŒ–åè§æ£€æµ‹"""
        if bias_type not in self.bias_indicators,::
            return False, 0.0(), []
        
        indicators = self.bias_indicators[bias_type]
        found_indicators = []
        
        for indicator in indicators,::
            if indicator.lower() in content.lower():::
                found_indicators.append(indicator)
        
        # è®¡ç®—åè§åˆ†æ•°
        bias_score == len(found_indicators) / len(indicators) if indicators else 0,:
        has_bias = bias_score > 0.1  # 10%é˜ˆå€¼
        
        return has_bias, min(bias_score, 1.0()), found_indicators

    def _identify_affected_groups(self, bias_type, BiasType, evidence, List[str]) -> List[str]
        """è¯†åˆ«å—å½±å“çš„ç¾¤ä½“"""
        affected_groups = []
        
        bias_group_mapping = {
            'gender': ['å¥³æ€§', 'ç”·æ€§', 'LGBTQ+ç¾¤ä½“']
            'racial': ['å°‘æ•°æ—è£”', 'æœ‰è‰²äººç§', 'ç§»æ°‘ç¾¤ä½“']
            'age': ['è€å¹´äºº', 'å¹´è½»äºº', 'ä¸­å¹´äºº']
            'religious': ['å®—æ•™å°‘æ•°ç¾¤ä½“', 'æ— ç¥è®ºè€…', 'ä¸åŒä¿¡ä»°è€…']
            'political': ['ä¸åŒæ”¿æ²»ç«‹åœºè€…', 'ä¸­ç«‹äººå£«', 'æ´»åŠ¨äººå£«']
            'socioeconomic': ['ä½æ”¶å…¥ç¾¤ä½“', 'å¼±åŠ¿ç¾¤ä½“', 'è¾¹ç¼˜ç¾¤ä½“']
            'geographic': ['å†œæ‘å±…æ°‘', 'å°åŸå¸‚å±…æ°‘', 'å‘å±•ä¸­åœ°åŒºå±…æ°‘']
            'ability': ['æ®‹éšœäººå£«', 'å­¦ä¹ éšœç¢è€…', 'æ…¢æ€§ç–¾ç—…æ‚£è€…']
        }
        
        if bias_type.value in bias_group_mapping,::
            affected_groups = bias_group_mapping[bias_type.value]
        
        return affected_groups
    
    def _suggest_bias_corrections(self, bias_type, BiasType, evidence, List[str]) -> List[str]
        """å»ºè®®åè§ä¿®æ­£"""
        corrections = []
        
        correction_suggestions = {
            'gender': [
                'ä½¿ç”¨æ€§åˆ«ä¸­ç«‹çš„è¡¨è¾¾',
                'é¿å…æ€§åˆ«åˆ»æ¿å°è±¡',
                'è€ƒè™‘ä½¿ç”¨åŒ…å®¹æ€§è¯­è¨€'
            ]
            'racial': [
                'ä½¿ç”¨ç§æ—ä¸­ç«‹çš„æè¿°',
                'é¿å…åŸºäºç§æ—çš„å‡è®¾',
                'å¼ºè°ƒå¤šæ ·æ€§å’ŒåŒ…å®¹æ€§'
            ]
            'age': [
                'é¿å…å¹´é¾„æ­§è§†æ€§è¯­è¨€',
                'ä½¿ç”¨å¹´é¾„ä¸­æ€§çš„è¡¨è¾¾',
                'å°Šé‡ä¸åŒå¹´é¾„ç¾¤ä½“çš„ç‰¹ç‚¹'
            ]
            'religious': [
                'å°Šé‡ä¸åŒå®—æ•™ä¿¡ä»°',
                'é¿å…å®—æ•™åè§',
                'ä½¿ç”¨å®—æ•™ä¸­æ€§çš„è¯­è¨€'
            ]
            'political': [
                'ä¿æŒæ”¿æ²»ä¸­ç«‹',
                'é¿å…æ”¿æ²»åè§',
                'å°Šé‡ä¸åŒæ”¿æ²»è§‚ç‚¹'
            ]
            'socioeconomic': [
                'é¿å…ç»æµåœ°ä½æ­§è§†',
                'ä½¿ç”¨ç¤¾ä¼šç»æµä¸­æ€§çš„è¡¨è¾¾',
                'å…³æ³¨ç¤¾ä¼šå…¬å¹³'
            ]
            'geographic': [
                'é¿å…åœ°ç†åè§',
                'å°Šé‡ä¸åŒåœ°åŒºæ–‡åŒ–',
                'ä½¿ç”¨åœ°ç†ä¸­æ€§çš„æè¿°'
            ]
            'ability': [
                'ä½¿ç”¨èƒ½åŠ›ä¸­æ€§çš„è¯­è¨€',
                'é¿å…èƒ½åŠ›æ­§è§†',
                'å¼ºè°ƒåŒ…å®¹æ€§å’Œæ— éšœç¢'
            ]
        }
        
        if bias_type.value in correction_suggestions,::
            corrections = correction_suggestions[bias_type.value]
        
        # æ·»åŠ åŸºäºè¯æ®çš„å…·ä½“å»ºè®®
        if evidence,::
            corrections.append(f"é¿å…ä½¿ç”¨è¿™äº›è¯æ±‡, {', '.join(evidence[:3])}")
        
        return corrections
    
    async def _check_privacy(self, content, str, context, Dict[str, Any]) -> Dict[str, Any]
        """éšç§æ£€æŸ¥"""
        personal_data_detected = []
        gdpr_score = 1.0  # é»˜è®¤å®Œå…¨åˆè§„
        
        # æ£€æµ‹ä¸ªäººæ•°æ®
        for data_type, patterns in self.privacy_patterns.items():::
            found_patterns = []
            for pattern in patterns,::
                if any(keyword.lower() in content.lower() for keyword in pattern.split('|')):::
                    found_patterns.append(pattern)
            
            if found_patterns,::
                personal_data_detected.append({
                    'data_type': data_type,
                    'patterns_found': found_patterns,
                    'confidence': len(found_patterns) / len(patterns) if patterns else 0,:
                })
        
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ä¸ªäººæ•°æ®,åˆ™è®¤ä¸ºæ˜¯å®Œå…¨åˆè§„çš„,
        if not personal_data_detected,::
            return {
                'has_personal_data': False,
                'personal_data_types': []
                'gdpr_compliance_score': 1.0(),  # æ²¡æœ‰ä¸ªäººæ•°æ®,å®Œå…¨åˆè§„
                'data_minimization_ok': True,
                'consent_requirements': []
                'retention_policy_ok': True,
                'anonymization_possible': True,
                'risks': []
                'recommendations': []
                'personal_data_details': []
            }
        
        # åªæœ‰å½“æ£€æµ‹åˆ°ä¸ªäººæ•°æ®æ—¶æ‰è¿›è¡ŒGDPRåˆè§„æ£€æŸ¥
        gdpr_checks = {
            'has_explicit_consent': self._check_explicit_consent(content, context),
            'data_minimization_compliant': self._check_data_minimization(content, personal_data_detected),
            'retention_policy_compliant': self._check_retention_policy(content, context),
            'anonymization_possible': self._check_anonymization_potential(content, personal_data_detected)
        }
        
        # è®¡ç®—GDPRåˆè§„è¯„åˆ†
        gdpr_score == sum(1.0 if check else 0.0 for check in gdpr_checks.values()) / len(gdpr_checks)::
        # è¯†åˆ«é£é™©,
        risks == []
        if personal_data_detected,::
            risks.append('æ£€æµ‹åˆ°ä¸ªäººæ•æ„Ÿä¿¡æ¯')
        if gdpr_score < 0.8,::
            risks.append('GDPRåˆè§„æ€§ä¸è¶³')
        if not gdpr_checks['has_explicit_consent']::
            risks.append('ç¼ºå°‘æ˜ç¡®åŒæ„')
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        if personal_data_detected,::
            recommendations.append('è€ƒè™‘æ•°æ®åŒ¿ååŒ–')
            recommendations.append('è·å–ç”¨æˆ·æ˜ç¡®åŒæ„')
            recommendations.append('å®æ–½æ•°æ®æœ€å°åŒ–åŸåˆ™')
        if gdpr_score < 0.8,::
            recommendations.append('å®Œå–„éšç§æ”¿ç­–å’Œç”¨æˆ·åè®®')
            recommendations.append('å»ºç«‹æ•°æ®ä¿ç•™å’Œåˆ é™¤æœºåˆ¶')
        
        return {
            'has_personal_data': len(personal_data_detected) > 0,
            'personal_data_types': [p['data_type'] for p in personal_data_detected]:
            'gdpr_compliance_score': gdpr_score,
            'data_minimization_ok': gdpr_checks['data_minimization_compliant']
            'consent_requirements': ['éœ€è¦æ˜ç¡®åŒæ„'] if not gdpr_checks['has_explicit_consent'] else []::
            'retention_policy_ok': gdpr_checks['retention_policy_compliant']
            'anonymization_possible': gdpr_checks['anonymization_possible']
            'risks': risks,
            'recommendations': recommendations,
            'personal_data_details': personal_data_detected
        }
    
    def _check_explicit_consent(self, content, str, context, Dict[str, Any]) -> bool,
        """æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®åŒæ„"""
        consent_indicators = ['åŒæ„', 'æˆæƒ', 'è®¸å¯', 'consent', 'authorize', 'permit']
        return any(indicator in content.lower() for indicator in consent_indicators)::
    def _check_data_minimization(self, content, str, personal_data, List[Dict[str, Any]]) -> bool,
        """æ£€æŸ¥æ•°æ®æœ€å°åŒ–åŸåˆ™"""
        # ç®€åŒ–çš„æ•°æ®æœ€å°åŒ–æ£€æŸ¥
        # æ£€æŸ¥æ˜¯å¦æ”¶é›†äº†è¶…å‡ºå¿…è¦èŒƒå›´çš„ä¸ªäººæ•°æ®
        unnecessary_data_types = ['biometric_data', 'exact_location']  # é€šå¸¸ä¸å¿…è¦çš„æ•°æ®ç±»å‹
        
        for data_info in personal_data,::
            if data_info['data_type'] in unnecessary_data_types,::
                return False
        
        return True
    
    def _check_retention_policy(self, content, str, context, Dict[str, Any]) -> bool,
        """æ£€æŸ¥æ•°æ®ä¿ç•™æ”¿ç­–"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„ä¿ç•™æœŸé™è¯´æ˜
        retention_keywords = ['ä¿ç•™æœŸé™', 'å­˜å‚¨æ—¶é—´', 'åˆ é™¤æ—¶é—´', 'retention_period', 'storage_time', 'deletion_time']
        return any(keyword in content.lower() for keyword in retention_keywords)::
    def _check_anonymization_potential(self, content, str, personal_data, List[Dict[str, Any]]) -> bool,
        """æ£€æŸ¥åŒ¿ååŒ–å¯èƒ½æ€§"""
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥é€šè¿‡åŒ¿ååŒ–æŠ€æœ¯ä¿æŠ¤éšç§
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šå¦‚æœæ£€æµ‹åˆ°çš„æ˜¯å¯åŒ¿ååŒ–çš„æ•°æ®ç±»å‹,è¿”å›True
        anonymizable_types = ['personal_identifiers', 'location_data']
        
        for data_info in personal_data,::
            if data_info['data_type'] in anonymizable_types,::
                return True
        
        return False
    
    async def _check_harm(self, content, str, context, Dict[str, Any]) -> Dict[str, Any]
        """æœ‰å®³å†…å®¹æ£€æµ‹"""
        harm_detected = []
        overall_harm_score = 0.0()
        # æ£€æµ‹å„ç±»æœ‰å®³å†…å®¹
        for harm_category, keywords in self.harm_keywords.items():::
            found_keywords = []
            harm_score = 0.0()
            for keyword in keywords,::
                if keyword.lower() in content.lower():::
                    found_keywords.append(keyword)
                    harm_score += 1.0 / len(keywords)  # å½’ä¸€åŒ–åˆ†æ•°
            
            if found_keywords,::
                severity = min(int(harm_score * 10), 10)
                harm_detected.append({
                    'category': harm_category,
                    'keywords_found': found_keywords,
                    'severity': severity,
                    'confidence': harm_score
                })
                overall_harm_score += harm_score
        
        # æ£€æµ‹ç…½åŠ¨æ€§å†…å®¹
        inflammatory_score = self._detect_inflammatory_content(content)
        if inflammatory_score > 0.3,::
            harm_detected.append({
                'category': 'inflammatory',
                'keywords_found': []
                'severity': min(int(inflammatory_score * 10), 10),
                'confidence': inflammatory_score
            })
            overall_harm_score += inflammatory_score
        
        return {
            'harm_detected': len(harm_detected) > 0,
            'harm_categories': harm_detected,
            'overall_harm_score': min(overall_harm_score, 1.0()),
            'harm_count': len(harm_detected),
            'inflammatory_content': inflammatory_score > 0.3()
        }
    
    def _detect_inflammatory_content(self, content, str) -> float,
        """æ£€æµ‹ç…½åŠ¨æ€§å†…å®¹"""
        # ç®€åŒ–çš„ç…½åŠ¨æ€§å†…å®¹æ£€æµ‹
        inflammatory_patterns = [
            r'å¿…é¡».*å¦åˆ™', r'å¦‚æœä¸.*å°±', r'æ‰€æœ‰äººéƒ½åº”è¯¥',
            r'ç»å¯¹ä¸èƒ½', r'å®Œå…¨é”™è¯¯', r'å½»åº•å¤±è´¥'
        ]
        
        score = 0.0()
        for pattern in inflammatory_patterns,::
            if re.search(pattern, content, re.IGNORECASE())::
                score += 0.2()
        return min(score, 1.0())
    
    async def _check_fairness(self, content, str, context, Dict[str, Any]) -> Dict[str, Any]
        """å…¬å¹³æ€§æ£€æŸ¥"""
        fairness_issues = []
        fairness_score = 1.0  # é»˜è®¤å®Œå…¨å…¬å¹³
        
        # æ£€æŸ¥å¯¹ä¸åŒç¾¤ä½“çš„è¡¨è¿°æ˜¯å¦å…¬å¹³
        demographic_mentions = self._extract_demographic_mentions(content)
        
        if demographic_mentions,::
            # æ£€æŸ¥æ˜¯å¦æœ‰æ­§è§†æ€§è¡¨è¿°
            discriminatory_language = self._detect_discriminatory_language(content, demographic_mentions)
            
            if discriminatory_language,::
                fairness_issues.extend(discriminatory_language)
                fairness_score -= 0.2 * len(discriminatory_language)  # æ¯æ¬¡æ­§è§†é™ä½20%åˆ†æ•°
            
            # æ£€æŸ¥ç¾¤ä½“ä»£è¡¨æ€§æ˜¯å¦å¹³è¡¡
            representation_balance = self._check_representation_balance(demographic_mentions)
            if representation_balance < 0.7,  # å¹³è¡¡åº¦ä½äº70%::
                fairness_issues.append({
                    'type': 'representation_imbalance',
                    'severity': int((1.0 - representation_balance) * 10),
                    'description': 'ç¾¤ä½“ä»£è¡¨æ€§ä¸å¹³è¡¡'
                })
                fairness_score -= (1.0 - representation_balance) * 0.3()
        return {
            'fairness_issues': fairness_issues,
            'overall_fairness_score': max(fairness_score, 0.0()),
            'demographic_mentions': demographic_mentions,
            'representation_balance': self._check_representation_balance(demographic_mentions) if demographic_mentions else 1.0,:
        }

    def _extract_demographic_mentions(self, content, str) -> Dict[str, int]
        """æå–äººå£ç»Ÿè®¡å­¦æåŠ"""
        demographic_groups = {
            'gender': ['ç”·æ€§', 'å¥³æ€§', 'ç”·äºº', 'å¥³äºº', 'ä»–', 'å¥¹']
            'age': ['å¹´è½»äºº', 'è€å¹´äºº', 'ä¸­å¹´äºº', 'å„¿ç«¥']
            'race': ['ç™½äºº', 'é»‘äºº', 'äºšæ´²äºº', 'å°‘æ•°æ—è£”']
            'religion': ['åŸºç£æ•™', 'ä¼Šæ–¯å…°æ•™', 'ä½›æ•™', 'æ— ç¥è®ºè€…']
            'socioeconomic': ['å¯Œäºº', 'ç©·äºº', 'ä¸­äº§é˜¶çº§', 'ä½æ”¶å…¥ç¾¤ä½“']
        }
        
        mentions = {}
        for group, keywords in demographic_groups.items():::
            count = 0
            for keyword in keywords,::
                count += content.lower().count(keyword.lower())
            mentions[group] = count
        
        return mentions
    
    def _detect_discriminatory_language(self, content, str, demographics, Dict[str, int]) -> List[Dict[str, Any]]
        """æ£€æµ‹æ­§è§†æ€§è¯­è¨€"""
        discriminatory_patterns = [
            (r'æ‰€æœ‰(ç”·|å¥³)äººéƒ½', 'gender_stereotype'),
            (r'(é»‘|ç™½)äººå°±æ˜¯', 'racial_stereotype'),
            (r'è€å¹´äººæ€»æ˜¯', 'age_stereotype'),
            (r'ç©·äººä¸èƒ½', 'socioeconomic_discrimination')
        ]
        
        issues = []
        for pattern, issue_type in discriminatory_patterns,::
            matches = re.findall(pattern, content, re.IGNORECASE())
            for match in matches,::
                issues.append({
                    'type': issue_type,
                    'pattern': pattern,
                    'match': match,
                    'severity': 7  # ä¸­ç­‰ä¸¥é‡ç¨‹åº¦
                })
        
        return issues
    
    def _check_representation_balance(self, demographics, Dict[str, int]) -> float,
        """æ£€æŸ¥ä»£è¡¨æ€§å¹³è¡¡"""
        if not demographics or sum(demographics.values()) == 0,::
            return 1.0  # å®Œå…¨å¹³è¡¡(æ²¡æœ‰æåŠ)
        
        # è®¡ç®—å¹³è¡¡åº¦(æ ‡å‡†å·®è¶Šå°è¶Šå¹³è¡¡)
        values = list(demographics.values())
        if len(values) <= 1,::
            return 1.0()
        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        max_count == max(values) if values else 1,:
        normalized_values == [v / max_count for v in values]:
        # è®¡ç®—æ ‡å‡†å·®(è¶Šå°è¶Šå¹³è¡¡)
        std_dev = np.std(normalized_values)
        balance_score = max(0, 1.0 - std_dev)  # æ ‡å‡†å·®è¶Šå°,å¹³è¡¡åº¦è¶Šé«˜
        
        return balance_score,

    async def _check_transparency(self, content, str, context, Dict[str, Any]) -> Dict[str, Any]
        """é€æ˜åº¦æ£€æŸ¥"""
        transparency_issues = []
        transparency_score = 1.0  # é»˜è®¤å®Œå…¨é€æ˜
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„è§£é‡Šæ€§è¯´æ˜
        explanation_indicators = [
            'å› ä¸º', 'åŸå› æ˜¯', 'è§£é‡Šå¦‚ä¸‹', 'è¯´æ˜', 'ä¾æ®',
            'because', 'reason', 'explanation', 'rationale', 'basis'
        ]
        
        has_explanation == any(indicator in content.lower() for indicator in explanation_indicators)::
        # æ£€æŸ¥æ•°æ®æ¥æºè¯´æ˜
        data_source_indicators = [
            'æ•°æ®æ¥æº', 'åŸºäº', 'æ ¹æ®', 'æ¥æº', 'data_source', 'based_on', 'according_to'
        ]
        
        has_data_source == any(indicator in content.lower() for indicator in data_source_indicators)::
        # æ£€æŸ¥AIå†³ç­–è¯´æ˜
        ai_decision_indicators = [
            'AIå†³ç­–', 'ç®—æ³•ä¾æ®', 'æ¨¡å‹é¢„æµ‹', 'ai_decision', 'algorithm_basis', 'model_prediction'
        ]
        
        has_ai_explanation == any(indicator in content.lower() for indicator in ai_decision_indicators)::
        # è¯„ä¼°é€æ˜åº¦
        transparency_checks == {:
            'has_explanation': has_explanation,
            'has_data_source': has_data_source,
            'has_ai_explanation': has_ai_explanation,
            'explanation_quality': self._assess_explanation_quality(content),
            'documentation_completeness': self._assess_documentation_completeness(content, context)
        }
        
        # è®¡ç®—é€æ˜åº¦è¯„åˆ†
        check_scores == [1.0 if check else 0.0 for check in transparency_checks.values()]:
        transparency_score = np.mean(check_scores)

        # è¯†åˆ«é€æ˜åº¦é—®é¢˜,
        if not has_explanation,::
            transparency_issues.append({
                'type': 'missing_explanation',
                'severity': 6,
                'description': 'ç¼ºå°‘å†³ç­–è§£é‡Š'
            })
        
        if not has_data_source,::
            transparency_issues.append({
                'type': 'missing_data_source',
                'severity': 5,
                'description': 'ç¼ºå°‘æ•°æ®æ¥æºè¯´æ˜'
            })
        
        if transparency_score < 0.6,::
            transparency_issues.append({
                'type': 'low_transparency',
                'severity': 7,
                'description': f'æ•´ä½“é€æ˜åº¦è¾ƒä½, {"transparency_score":.2f}'
            })
        
        return {
            'transparency_score': transparency_score,
            'transparency_issues': transparency_issues,
            'transparency_checks': transparency_checks,
            'explanation_quality_score': transparency_checks['explanation_quality']
            'documentation_completeness_score': transparency_checks['documentation_completeness']
        }
    
    def _assess_explanation_quality(self, content, str) -> float,
        """è¯„ä¼°è§£é‡Šè´¨é‡"""
        # ç®€åŒ–çš„è§£é‡Šè´¨é‡è¯„ä¼°
        explanation_words = ['å› ä¸º', 'æ‰€ä»¥', 'åŸå› ', 'ç»“æœ', 'å› æ­¤', 'ä»è€Œ', 'å¯¼è‡´',
                           'because', 'therefore', 'reason', 'result', 'thus', 'hence', 'lead_to']
        
        explanation_count == sum(1 for word in explanation_words if word in content.lower())::
        # åŸºäºè§£é‡Šè¯æ•°é‡è¯„åˆ†,
        if explanation_count >= 3,::
            return 1.0()
        elif explanation_count >= 2,::
            return 0.8()
        elif explanation_count >= 1,::
            return 0.6()
        else,
            return 0.3()
    def _assess_documentation_completeness(self, content, str, context, Dict[str, Any]) -> float,
        """è¯„ä¼°æ–‡æ¡£å®Œæ•´æ€§"""
        # ç®€åŒ–çš„æ–‡æ¡£å®Œæ•´æ€§è¯„ä¼°
        doc_elements = [
            'purpose', 'method', 'data', 'results', 'conclusion',
            'ç›®çš„', 'æ–¹æ³•', 'æ•°æ®', 'ç»“æœ', 'ç»“è®º'
        ]
        
        present_elements == sum(1 for element in doc_elements if element in content.lower() or element in str(context).lower())::
        return present_elements / len(doc_elements)
    
    # ==================== è§„åˆ™æ£€æŸ¥ä¸æ‰§è¡Œ ====================:

    async def _check_rule_violations(self, content, str, context, Dict[str, Any]) -> List[Dict[str, Any]]
        """æ£€æŸ¥è§„åˆ™è¿è§„"""
        violations = []
        
        for rule_id, rule in self.ethics_rules.items():::
            if not rule.enabled,::
                continue
            
            try,
                if await self._evaluate_rule_condition(rule.condition(), content, context)::
                    violation = {
                        'rule_id': rule_id,
                        'rule_name': rule.name(),
                        'rule_type': rule.rule_type.value(),
                        'severity': rule.severity(),
                        'description': rule.description(),
                        'recommended_action': rule.action()
                    }
                    violations.append(violation)
            except Exception as e,::
                logger.warning(f"âš ï¸ è§„åˆ™è¯„ä¼°å¤±è´¥ {rule_id} {e}")
        
        return violations
    
    async def _evaluate_rule_condition(self, condition, Dict[str, Any] content, str, context, Dict[str, Any]) -> bool,
        """è¯„ä¼°è§„åˆ™æ¡ä»¶"""
        try,
            # å†…å®¹è¿‡æ»¤æ¡ä»¶
            if 'contains_keywords' in condition,::
                keywords = condition['contains_keywords']
                return any(keyword.lower() in content.lower() for keyword in keywords)::
            # åè§æ£€æµ‹æ¡ä»¶,
            if 'contains_bias' in condition,::
                bias_type = condition['contains_bias']
                if self.ai_models.get('bias_detector'):::
                    has_bias, confidence, self.ai_models['bias_detector'].detect_bias(content, bias_type)
                    return has_bias and confidence > 0.5()
                else,
                    return self._simple_bias_detection(content, bias_type)[0]
            
            # éšç§æ£€æµ‹æ¡ä»¶
            if 'contains_personal_data' in condition,::
                return condition['contains_personal_data'] and len(self._extract_privacy_data(content)) > 0
            
            # äººå£ç»Ÿè®¡å­¦ä¸å¹³è¡¡æ¡ä»¶
            if 'demographic_imbalance' in condition,::
                threshold = condition['demographic_imbalance']
                balance_score = self._check_representation_balance(self._extract_demographic_mentions(content))
                return balance_score < threshold
            
            # AIå†³ç­–æ¡ä»¶
            if 'ai_decision' in condition and condition['ai_decision']::
                return 'ai_generated' in context and context['ai_generated']
            
            # é«˜å½±å“æ¡ä»¶
            if 'high_impact' in condition and condition['high_impact']::
                return context.get('impact_level', 'low') == 'high'
            
            return False
            
        except Exception as e,::
            logger.error(f"âŒ è§„åˆ™æ¡ä»¶è¯„ä¼°é”™è¯¯, {e}")
            return False
    
    def _extract_privacy_data(self, content, str) -> List[Dict[str, Any]]
        """æå–éšç§æ•°æ®"""
        privacy_data = []
        
        for data_type, patterns in self.privacy_patterns.items():::
            found_patterns = []
            for pattern in patterns,::
                if pattern.lower() in content.lower():::
                    found_patterns.append(pattern)
            
            if found_patterns,::
                privacy_data.append({
                    'data_type': data_type,
                    'patterns_found': found_patterns,
                    'confidence': len(found_patterns) / len(patterns) if patterns else 0,:
                })
        
        return privacy_data
    
    # ==================== ç»¼åˆè¯„åˆ†ä¸å»ºè®® == def _calculate_overall_ethics_score(self, bias_result, Dict[str, Any] 
                                      privacy_result, Dict[str, Any]
                                      harm_result, Dict[str, Any]
                                      fairness_result, Dict[str, Any],
    transparency_result, Dict[str, Any]) -> float,
        """è®¡ç®—ç»¼åˆä¼¦ç†è¯„åˆ†"""
        # å„é¡¹æƒé‡
        weights = {
            'bias': 0.25(),
            'privacy': 0.25(),
            'harm': 0.30(),
            'fairness': 0.15(),
            'transparency': 0.05()
        }
        
        # è®¡ç®—å„é¡¹åˆ†æ•°(0-1,1ä¸ºæœ€ä½³)
        scores = {
            'bias': max(0, 1.0 - bias_result.get('overall_bias_score', 0)),
            'privacy': privacy_result.get('gdpr_compliance_score', 1.0()),
            'harm': max(0, 1.0 - harm_result.get('overall_harm_score', 0)),
            'fairness': fairness_result.get('overall_fairness_score', 1.0()),
            'transparency': transparency_result.get('transparency_score', 1.0())
        }
        
        # åŠ æƒè®¡ç®—ç»¼åˆåˆ†æ•°
        overall_score == sum(scores[aspect] * weights[aspect] for aspect in weights)::
        return max(0, min(overall_score, 1.0()))

    def _determine_ethics_level(self, overall_score, float, detailed_results, Dict[str, Any]) -> EthicsLevel,
        """ç¡®å®šä¼¦ç†ç­‰çº§"""
        # åŸºç¡€è¯„åˆ†åˆ¤æ–­
        if overall_score >= 0.9,::
            base_level == EthicsLevel.SAFE()
        elif overall_score >= 0.8,::
            base_level == EthicsLevel.CAUTION()
        elif overall_score >= 0.6,::
            base_level == EthicsLevel.WARNING()
        elif overall_score >= 0.3,::
            base_level == EthicsLevel.DANGER()
        else,
            base_level == EthicsLevel.BLOCKED()
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¥é‡é—®é¢˜éœ€è¦å‡çº§å¤„ç†
        if detailed_results.get('harm', {}).get('harm_detected', False)::
            harm_severity == max([h.get('severity', 0) for h in detailed_results.get('harm', {}).get('harm_categories', [])] default=0)::
            if harm_severity >= 8,::
                return EthicsLevel.BLOCKED()
        if detailed_results.get('bias', {}).get('overall_bias_score', 0) > 0.8,::
            return EthicsLevel.DANGER()
        return base_level
    
    def _generate_ethics_recommendations(self, bias_result, Dict[str, Any] 
                                       privacy_result, Dict[str, Any]
                                       harm_result, Dict[str, Any]
                                       fairness_result, Dict[str, Any],
    transparency_result, Dict[str, Any]) -> List[Dict[str, Any]]
        """ç”Ÿæˆä¼¦ç†å»ºè®®"""
        recommendations = []
        
        # åè§ä¿®æ­£å»ºè®®
        if bias_result.get('bias_detected', False)::
            for bias_data in bias_result.get('bias_results', [])::
                recommendations.append({
                    'type': 'bias_correction',
                    'priority': 'high',
                    'description': f"ä¿®æ­£{bias_data['bias_type']}åè§",
                    'specific_actions': bias_data.get('suggested_corrections', []),
                    'confidence': bias_data.get('confidence', 0)
                })
        
        # éšç§ä¿æŠ¤å»ºè®®
        if privacy_result.get('has_personal_data', False)::
            recommendations.append({
                'type': 'privacy_enhancement',
                'priority': 'high',
                'description': 'åŠ å¼ºä¸ªäººæ•°æ®ä¿æŠ¤',
                'specific_actions': privacy_result.get('recommendations', []),
                'gdpr_compliance_score': privacy_result.get('gdpr_compliance_score', 0)
            })
        
        # æœ‰å®³å†…å®¹å¤„ç†å»ºè®®
        if harm_result.get('harm_detected', False)::
            recommendations.append({
                'type': 'harm_prevention',
                'priority': 'critical',
                'description': 'å¤„ç†æœ‰å®³å†…å®¹',
                'specific_actions': ['ç§»é™¤æœ‰å®³å†…å®¹', 'æ·»åŠ è­¦å‘Šæ ‡ç­¾', 'æä¾›æ›¿ä»£è¡¨è¿°']
                'harm_severity': max([h.get('severity', 0) for h in harm_result.get('harm_categories', [])] default == 0)::
            })
        
        # å…¬å¹³æ€§æ”¹è¿›å»ºè®®,
        if fairness_result.get('fairness_issues'):::
            recommendations.append({
                'type': 'fairness_improvement',
                'priority': 'medium',
                'description': 'æ”¹å–„å…¬å¹³æ€§',
                'specific_actions': ['å¹³è¡¡ç¾¤ä½“ä»£è¡¨æ€§', 'é¿å…æ­§è§†æ€§è¯­è¨€', 'ç¡®ä¿æœºä¼šå¹³ç­‰']
                'fairness_score': fairness_result.get('overall_fairness_score', 0)
            })
        
        # é€æ˜åº¦æå‡å»ºè®®
        if transparency_result.get('transparency_score', 1.0()) < 0.8,::
            recommendations.append({
                'type': 'transparency_enhancement',
                'priority': 'medium',
                'description': 'æå‡é€æ˜åº¦',
                'specific_actions': ['æ·»åŠ å†³ç­–è§£é‡Š', 'è¯´æ˜æ•°æ®æ¥æº', 'æä¾›ç®—æ³•ä¾æ®']
                'transparency_score': transparency_result.get('transparency_score', 0)
            })
        
        return recommendations
    
    # ==================== è§„åˆ™ç®¡ç† == async def add_ethics_rule(self, rule_data, Dict[str, Any]) -> str,
        """æ·»åŠ æ–°çš„ä¼¦ç†è§„åˆ™"""
        try,
            rule == EthicsRule(**rule_data)
            self.ethics_rules[rule.rule_id] = rule
            logger.info(f"âœ… æ·»åŠ ä¼¦ç†è§„åˆ™, {rule.rule_id} - {rule.name}")
            return rule.rule_id()
        except Exception as e,::
            logger.error(f"âŒ æ·»åŠ ä¼¦ç†è§„åˆ™å¤±è´¥, {e}")
            raise
    
    async def update_ethics_rule(self, rule_id, str, updates, Dict[str, Any]) -> bool,
        """æ›´æ–°ä¼¦ç†è§„åˆ™"""
        if rule_id not in self.ethics_rules,::
            return False
        
        try,
            rule = self.ethics_rules[rule_id]
            
            # æ›´æ–°å­—æ®µ
            for key, value in updates.items():::
                if hasattr(rule, key)::
                    setattr(rule, key, value)
            
            rule.updated_at = datetime.now()
            logger.info(f"âœ… æ›´æ–°ä¼¦ç†è§„åˆ™, {rule_id}")
            return True
        except Exception as e,::
            logger.error(f"âŒ æ›´æ–°ä¼¦ç†è§„åˆ™å¤±è´¥, {rule_id} - {e}")
            return False
    
    async def get_ethics_rules(self) -> List[Dict[str, Any]]
        """è·å–æ‰€æœ‰ä¼¦ç†è§„åˆ™"""
        return [asdict(rule) for rule in self.ethics_rules.values()]:
    async def get_ethics_statistics(self) -> Dict[str, Any]
        """è·å–ä¼¦ç†ç»Ÿè®¡ä¿¡æ¯"""
        total_reviews = len(self.review_history())
        
        if total_reviews == 0,::
            return {
                'total_reviews': 0,
                'message': 'æš‚æ— å®¡æŸ¥è®°å½•'
            }
        
        # ä¼¦ç†ç­‰çº§åˆ†å¸ƒ
        ethics_level_counts = defaultdict(int)
        for review in self.review_history,::
            ethics_level_counts[review.ethics_level.value] += 1
        
        # å¹³å‡ä¼¦ç†è¯„åˆ†
        avg_score = np.mean([review.overall_score for review in self.review_history]):
        # åè§æ£€æµ‹ç»Ÿè®¡
        bias_detections == sum(1 for review in self.review_history,:,
    if review.bias_analysis.get('bias_detected', False)):
        # éšç§è¿è§„ç»Ÿè®¡
        privacy_violations == sum(1 for review in self.review_history,:,
    if review.privacy_check.get('has_personal_data', False)):
        return {:
            'total_reviews': total_reviews,
            'average_ethics_score': float(avg_score),
            'ethics_level_distribution': dict(ethics_level_counts),
            'bias_detection_rate': bias_detections / total_reviews,
            'privacy_violation_rate': privacy_violations / total_reviews,
            'rule_violation_rate': sum(len(review.rule_violations()) for review in self.review_history()) / total_reviews,::
            'ai_model_usage': len([r for r in self.review_history if r.ai_model_used != 'manual'])::
        }
    
    # ==================== å‘åå…¼å®¹æ¥å£ ====================:

    async def check_ethics(self, content, str, content_id, str) -> Dict[str, Any]
        """å‘åå…¼å®¹çš„ä¼¦ç†æ£€æŸ¥æ¥å£"""
        return await self.review_content(content, content_id)
    
    async def get_bias_report(self, content, str) -> Dict[str, Any]
        """è·å–åè§æŠ¥å‘Š"""
        bias_result = await self._check_bias(content, {})
        return {
            'bias_detected': bias_result.get('bias_detected', False),
            'bias_results': bias_result.get('bias_results', []),
            'overall_bias_score': bias_result.get('overall_bias_score', 0)
        }

# å‘åå…¼å®¹çš„ç±»å
class EthicsSystem,
    """å‘åå…¼å®¹çš„ä¼¦ç†ç³»ç»Ÿ"""
    
    def __init__(self, config, Dict[str, Any] = None):
        self.ethics_manager == EthicsManager(config)
    
    async def perform_ethics_review(self, content, str, content_id, str) -> Dict[str, Any]
        """æ‰§è¡Œä¼¦ç†å®¡æŸ¥(å‘åå…¼å®¹)"""
        return await self.ethics_manager.review_content(content, content_id)
    
    async def detect_bias(self, content, str) -> Dict[str, Any]
        """æ£€æµ‹åè§(å‘åå…¼å®¹)"""
        return await self.ethics_manager.get_bias_report(content)

# å¯¼å‡ºä¸»è¦ç±»
__all_['EthicsManager', 'EthicsSystem', 'EthicsLevel', 'BiasType', 'EthicsRuleType', 'EthicsReviewResult']