#!/usr/bin/env python3
"""
ç»Ÿä¸€çŸ¥è¯†å›¾è°± (Unified Knowledge Graph)
Level 5 AGIæ ¸å¿ƒç»„ä»¶ - å®ç°è·¨é¢†åŸŸçŸ¥è¯†è¡¨ç¤ºä¸æ¨ç†

åŠŸèƒ½ï¼š
- å®ä½“è¯†åˆ«ä¸é“¾æ¥ (NER + Entity Linking)
- å…³ç³»æŠ½å–ä¸éªŒè¯ (Relation Extraction)
- çŸ¥è¯†èåˆä¸æ¶ˆæ­§ (Knowledge Fusion)
- æ—¶åºçŸ¥è¯†æ›´æ–° (Temporal Knowledge Updates)
- è·¨é¢†åŸŸçŸ¥è¯†è¿ç§» (Cross - domain Knowledge Transfer)
"""

import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from collections import defaultdict
from pathlib import Path

# å°è¯•å¯¼å…¥å¯é€‰çš„AIåº“
try:
    import numpy as np
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.cluster import DBSCAN
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class Entity:
    """å®ä½“å®šä¹‰"""
    entity_id: str
    name: str
    entity_type: str
    confidence: float
    properties: Dict[str, Any]
    aliases: List[str]
    source: str
    timestamp: datetime
    
    def __post_init__(self):
        if not isinstance(self.timestamp, datetime):
            self.timestamp = datetime.fromisoformat(self.timestamp)

@dataclass
class Relation:
    """å…³ç³»å®šä¹‰"""
    relation_id: str
    source_entity: str
    target_entity: str
    relation_type: str
    confidence: float
    properties: Dict[str, Any]
    source: str
    timestamp: datetime
    is_temporal: bool = False
    temporal_properties: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        if not isinstance(self.timestamp, datetime):
            self.timestamp = datetime.fromisoformat(self.timestamp)

@dataclass
class KnowledgeTriple:
    """çŸ¥è¯†ä¸‰å…ƒç»„"""
    subject: str
    predicate: str
    object: str
    confidence: float
    source: str
    timestamp: datetime
    metadata: Dict[str, Any]

@dataclass
class DomainKnowledge:
    """é¢†åŸŸçŸ¥è¯†"""
    domain: str
    entities: Dict[str, Entity]
    relations: Dict[str, Relation]
    patterns: List[Dict[str, Any]]
    last_updated: datetime

class UnifiedKnowledgeGraph:
    """ç»Ÿä¸€çŸ¥è¯†å›¾è°± - Level 5 AGIæ ¸å¿ƒç»„ä»¶"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        
        # çŸ¥è¯†å­˜å‚¨
        self.entities: Dict[str, Entity] = {}
        self.relations: Dict[str, Relation] = {}
        self.knowledge_triples: List[KnowledgeTriple] = []
        self.domain_knowledge: Dict[str, DomainKnowledge] = {}
        
        # å®ä½“é“¾æ¥æ˜ å°„
        self.entity_linking_map: Dict[str, Set[str]] = defaultdict(set)
        self.type_hierarchy: Dict[str, Set[str]] = defaultdict(set)
        
        # æ—¶åºçŸ¥è¯†ç®¡ç†
        self.temporal_knowledge: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        self.knowledge_evolution: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        
        # è·¨é¢†åŸŸæ˜ å°„
        self.cross_domain_mappings: Dict[str, Dict[str, float]] = defaultdict(dict)
        
        # AIç»„ä»¶ï¼ˆå¯é€‰ï¼‰
        self.entity_embeddings: Dict[str, Any] = {}
        self.relation_embeddings: Dict[str, Any] = {}
        
        # é…ç½®å‚æ•°
        self.similarity_threshold = self.config.get('similarity_threshold', 0.85)
        self.confidence_threshold = self.config.get('confidence_threshold', 0.7)
        self.max_search_depth = self.config.get('max_search_depth', 3)
        
        # åˆå§‹åŒ–AIç»„ä»¶
        self._initialize_ai_components()
        
        logger.info("ğŸ§  ç»Ÿä¸€çŸ¥è¯†å›¾è°±åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_ai_components(self):
        """åˆå§‹åŒ–AIç»„ä»¶"""
        try:
            if SKLEARN_AVAILABLE:
                # åˆå§‹åŒ–å®ä½“åµŒå…¥æ¨¡å‹
                self.entity_vectorizer = TfidfVectorizer(
                    max_features=1000,
                    ngram_range=(1, 2),
                    analyzer='char'
                )
                
                # åˆå§‹åŒ–å…³ç³»åµŒå…¥æ¨¡å‹
                self.relation_vectorizer = TfidfVectorizer(
                    max_features=500,
                    ngram_range=(1, 3)
                )
                
                logger.info("âœ… AIç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("âš ï¸ scikit-learnä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç®—æ³•")
        except Exception as e:
            logger.error(f"âŒ AIç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def add_entity(self, entity: Entity) -> bool:
        """æ·»åŠ å®ä½“åˆ°çŸ¥è¯†å›¾è°±"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if entity.entity_id in self.entities:
                logger.warning(f"å®ä½“ {entity.entity_id} å·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–")
            
            # å®ä½“æ¶ˆæ­§
            resolved_entity = await self._resolve_entity_ambiguity(entity)
            
            # å¦‚æœæ‰¾åˆ°ç›¸ä¼¼å®ä½“ï¼Œåˆå¹¶
            if resolved_entity and resolved_entity.entity_id != entity.entity_id:
                logger.info(f"åˆå¹¶å®ä½“: {entity.entity_id} -> {resolved_entity.entity_id}")
                merged = await self._merge_entities(resolved_entity, entity)
                self.entities[resolved_entity.entity_id] = merged
            else:
                self.entities[entity.entity_id] = entity
            
            # æ›´æ–°å®ä½“é“¾æ¥æ˜ å°„
            await self._update_entity_linking(entity)
            
            logger.info(f"âœ… æˆåŠŸæ·»åŠ å®ä½“: {entity.entity_id}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ å®ä½“å¤±è´¥: {e}")
            return False
    
    async def _resolve_entity_ambiguity(self, entity: Entity) -> Optional[Entity]:
        """å®ä½“æ¶ˆæ­§"""
        # æ£€æŸ¥åŒåå®ä½“
        for existing_id, existing_entity in self.entities.items():
            if existing_entity.name.lower() == entity.name.lower():
                return existing_entity
            
            # æ£€æŸ¥åˆ«ååŒ¹é…
            if entity.name.lower() in [alias.lower() for alias in existing_entity.aliases]:
                return existing_entity
            
            # æ£€æŸ¥ç›¸ä¼¼æ€§(åŸºäºåµŒå…¥å‘é‡)
            if SKLEARN_AVAILABLE and existing_id in self.entity_embeddings:
                similarity = await self._calculate_entity_similarity(entity, existing_entity)
                if similarity > self.similarity_threshold:
                    return existing_entity
        
        return None
    
    async def _merge_entities(self, entity1: Entity, entity2: Entity) -> Entity:
        """åˆå¹¶å®ä½“ä¿¡æ¯"""
        # ä¿ç•™ç½®ä¿¡åº¦æ›´é«˜çš„ä¿¡æ¯
        if entity2.confidence > entity1.confidence:
            merged_properties = {**entity1.properties, **entity2.properties}
            merged_aliases = list(set(entity1.aliases + entity2.aliases))

            return Entity(
                entity_id=entity1.entity_id,
                name=entity2.name,
                entity_type=entity2.entity_type,
                confidence=max(entity1.confidence, entity2.confidence),
                properties=merged_properties,
                aliases=merged_aliases,
                source=f"{entity1.source}|{entity2.source}",
                timestamp=datetime.now()
            )
        else:
            # ä¿ç•™entity1çš„ä¸»è¦ä¿¡æ¯, åˆå¹¶å…¶ä»–å±æ€§
            merged_properties = {**entity2.properties, **entity1.properties}
            merged_aliases = list(set(entity1.aliases + entity2.aliases))

            entity1.properties = merged_properties
            entity1.aliases = merged_aliases
            entity1.source = f"{entity1.source}|{entity2.source}"
            entity1.timestamp = datetime.now()
            return entity1
    
    async def _update_entity_linking(self, entity: Entity):
        """æ›´æ–°å®ä½“é“¾æ¥æ˜ å°„"""
        # æ·»åŠ å®ä½“IDåˆ°æ˜ å°„
        self.entity_linking_map[entity.name.lower()].add(entity.entity_id)
        
        # æ·»åŠ åˆ«ååˆ°æ˜ å°„
        for alias in entity.aliases:
            self.entity_linking_map[alias.lower()].add(entity.entity_id)
        
        # æ·»åŠ ç±»å‹åˆ°å±‚æ¬¡ç»“æ„
        self.type_hierarchy[entity.entity_type].add(entity.entity_id)
    
    async def _calculate_entity_similarity(self, entity1: Entity, entity2: Entity) -> float:
        """è®¡ç®—å®ä½“ç›¸ä¼¼åº¦"""
        try:
            if not SKLEARN_AVAILABLE:
                # ä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
                return self._string_similarity(entity1.name, entity2.name)
            
            # æ„å»ºå®ä½“æè¿°æ–‡æœ¬
            entity1_text = f"{entity1.name} {' '.join(entity1.aliases)} {entity1.entity_type}"
            for key, value in entity1.properties.items():
                entity1_text += f" {key} {value}"
            
            entity2_text = f"{entity2.name} {' '.join(entity2.aliases)} {entity2.entity_type}"
            for key, value in entity2.properties.items():
                entity2_text += f" {key} {value}"
            
            # ä½¿ç”¨TF-IDFå‘é‡åŒ–
            vec1 = self.entity_vectorizer.fit_transform([entity1_text])
            vec2 = self.entity_vectorizer.transform([entity2_text])
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = cosine_similarity(vec1, vec2)[0][0]
            
            return float(similarity)
        except Exception as e:
            logger.warning(f"è®¡ç®—å®ä½“ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0
    
    def _string_similarity(self, str1: str, str2: str) -> float:
        """ç®€å•çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦è®¡ç®—"""
        str1, str2 = str1.lower(), str2.lower()
        if str1 == str2:
            return 1.0
        
        # ç®€å•çš„Jaccardç›¸ä¼¼åº¦
        set1, set2 = set(str1.split()), set(str2.split())
        intersection = set1 & set2
        union = set1 | set2
        
        if not union:
            return 0.0
        
        return len(intersection) / len(union)
    
    async def add_relation(self, relation: Relation) -> bool:
        """æ·»åŠ å…³ç³»åˆ°çŸ¥è¯†å›¾è°±"""
        try:
            if relation.relation_id in self.relations:
                logger.warning(f"å…³ç³» {relation.relation_id} å·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–")
            
            self.relations[relation.relation_id] = relation
            
            # åˆ›å»ºçŸ¥è¯†ä¸‰å…ƒç»„
            triple = KnowledgeTriple(
                subject=relation.source_entity,
                predicate=relation.relation_type,
                object=relation.target_entity,
                confidence=relation.confidence,
                source=relation.source,
                timestamp=relation.timestamp,
                metadata=relation.properties
            )
            
            self.knowledge_triples.append(triple)
            
            logger.info(f"âœ… æˆåŠŸæ·»åŠ å…³ç³»: {relation.relation_id}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ å…³ç³»å¤±è´¥: {e}")
            return False
    
    async def query_knowledge(self, query: str, query_type: str = "entity") -> List[Dict[str, Any]]:
        """æŸ¥è¯¢çŸ¥è¯†å›¾è°±"""
        try:
            if query_type == "entity":
                return await self._query_entities(query)
            elif query_type == "relation":
                return await self._query_relations(query)
            elif query_type == "triple":
                return await self._query_triples(query)
            else:
                logger.warning(f"æœªçŸ¥çš„æŸ¥è¯¢ç±»å‹: {query_type}")
                return []
        except Exception as e:
            logger.error(f"æŸ¥è¯¢çŸ¥è¯†å¤±è´¥: {e}")
            return []
    
    async def _query_entities(self, query: str) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢å®ä½“"""
        results = []
        query_lower = query.lower()
        
        for entity_id, entity in self.entities.items():
            # æ£€æŸ¥åç§°åŒ¹é…
            if query_lower in entity.name.lower():
                results.append({
                    "entity_id": entity.entity_id,
                    "name": entity.name,
                    "entity_type": entity.entity_type,
                    "confidence": entity.confidence,
                    "source": entity.source
                })
                continue
            
            # æ£€æŸ¥åˆ«ååŒ¹é…
            for alias in entity.aliases:
                if query_lower in alias.lower():
                    results.append({
                        "entity_id": entity.entity_id,
                        "name": entity.name,
                        "entity_type": entity.entity_type,
                        "confidence": entity.confidence,
                        "source": entity.source
                    })
                    break
        
        return results
    
    async def _query_relations(self, query: str) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢å…³ç³»"""
        results = []
        query_lower = query.lower()
        
        for relation_id, relation in self.relations.items():
            if (query_lower in relation.source_entity.lower() or
                query_lower in relation.target_entity.lower() or
                query_lower in relation.relation_type.lower()):
                results.append({
                    "relation_id": relation.relation_id,
                    "source_entity": relation.source_entity,
                    "target_entity": relation.target_entity,
                    "relation_type": relation.relation_type,
                    "confidence": relation.confidence,
                    "source": relation.source
                })
        
        return results
    
    async def _query_triples(self, query: str) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢çŸ¥è¯†ä¸‰å…ƒç»„"""
        results = []
        query_lower = query.lower()
        
        for triple in self.knowledge_triples:
            if (query_lower in triple.subject.lower() or
                query_lower in triple.predicate.lower() or
                query_lower in triple.object.lower()):
                results.append({
                    "subject": triple.subject,
                    "predicate": triple.predicate,
                    "object": triple.object,
                    "confidence": triple.confidence,
                    "source": triple.source
                })
        
        return results
    
    async def get_statistics(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_entities": len(self.entities),
            "total_relations": len(self.relations),
            "total_triples": len(self.knowledge_triples),
            "total_domains": len(self.domain_knowledge),
            "entity_types": list(set(e.entity_type for e in self.entities.values())),
            "relation_types": list(set(r.relation_type for r in self.relations.values()))
        }