#! / usr / bin / env python3
"""
ç»Ÿä¸€çŸ¥è¯†å›¾è°± (Unified Knowledge Graph)
Level 5 AGIæ ¸å¿ƒç»„ä»¶ - å®ç°è·¨é¢†åŸŸçŸ¥è¯†è¡¨ç¤ºä¸æ¨ç†

åŠŸèƒ½ï¼š
- å®ä½“è¯†åˆ«ä¸é“¾æ¥ (NER + Entity Linking)
- å…³ç³»æŠ½å–ä¸éªŒè¯ (Relation Extraction)
- çŸ¥è¯†èåˆä¸æ¶ˆæ­§ (Knowledge Fusion)
- æ—¶åºçŸ¥è¯†æ›´æ–° (Temporal Knowledge Updates)
- è·¨é¢†åŸŸçŸ¥è¯†è¿ç§» (Cross - domain Knowledge Transfer)
"""

# TODO: Fix import - module 'asyncio' not found
from tests.tools.test_tool_dispatcher_logging import
from tests.test_json_fix import
from tests.core_ai import
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from collections import defaultdict
# TODO: Fix import - module 'numpy' not found
from pathlib import Path

# å°è¯•å¯¼å…¥å¯é€‰çš„AIåº“
try,
# TODO: Fix import - module 'torch' not found
# TODO: Fix import - module 'torch.nn' not found
# TODO: Fix import - module 'torch.nn.functional' not found
    TORCH_AVAILABLE == True
except ImportError, ::
    TORCH_AVAILABLE == False

try,
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.cluster import DBSCAN
    SKLEARN_AVAILABLE == True
except ImportError, ::
    SKLEARN_AVAILABLE == False

# é…ç½®æ—¥å¿—
logging.basicConfig(level = logging.INFO())
logger = logging.getLogger(__name__)

@dataclass
åœ¨ç±»å®šä¹‰å‰æ·»åŠ ç©ºè¡Œ
    """å®ä½“å®šä¹‰"""
    entity_id, str
    name, str
    entity_type, str
    confidence, float
    properties, Dict[str, Any]
    aliases, List[str]
    source, str
    timestamp, datetime
    
    def __post_init__(self):
        if not isinstance(self.timestamp(), datetime)::
            self.timestamp = datetime.fromisoformat(self.timestamp())

@dataclass
åœ¨ç±»å®šä¹‰å‰æ·»åŠ ç©ºè¡Œ
    """å…³ç³»å®šä¹‰"""
    relation_id, str
    source_entity, str
    target_entity, str
    relation_type, str
    confidence, float
    properties, Dict[str, Any]
    source, str
    timestamp, datetime
    is_temporal, bool == False
    temporal_properties, Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        if not isinstance(self.timestamp(), datetime)::
            self.timestamp = datetime.fromisoformat(self.timestamp())

@dataclass
åœ¨ç±»å®šä¹‰å‰æ·»åŠ ç©ºè¡Œ
    """çŸ¥è¯†ä¸‰å…ƒç»„"""
    subject, str
    predicate, str
    object, str
    confidence, float
    source, str
    timestamp, datetime
    metadata, Dict[str, Any]

@dataclass
åœ¨ç±»å®šä¹‰å‰æ·»åŠ ç©ºè¡Œ
    """é¢†åŸŸçŸ¥è¯†"""
    domain, str
    entities, Dict[str, Entity]
    relations, Dict[str, Relation]
    patterns, List[Dict[str, Any]]
    last_updated, datetime

class UnifiedKnowledgeGraph, :
    """ç»Ÿä¸€çŸ¥è¯†å›¾è°± - Level 5 AGIæ ¸å¿ƒç»„ä»¶"""
    
    def __init__(self, config, Dict[str, Any] = None):
        self.config = config or {}
        
        # çŸ¥è¯†å­˜å‚¨
        self.entities, Dict[str, Entity] = {}
        self.relations, Dict[str, Relation] = {}
        self.knowledge_triples, List[KnowledgeTriple] = []
        self.domain_knowledge, Dict[str, DomainKnowledge] = {}
        
        # å®ä½“é“¾æ¥æ˜ å°„
        self.entity_linking_map, Dict[str, Set[str]] = defaultdict(set)
        self.type_hierarchy, Dict[str, Set[str]] = defaultdict(set)
        
        # æ—¶åºçŸ¥è¯†ç®¡ç†
        self.temporal_knowledge, Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        self.knowledge_evolution, Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        
        # è·¨é¢†åŸŸæ˜ å°„
        self.cross_domain_mappings, Dict[str, Dict[str, Any]] = {}
        self.transfer_patterns, Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        
        # AIæ¨¡å‹
        self.ai_models = {}
        self.entity_embeddings, Dict[str, np.ndarray] = {}
        self.relation_embeddings, Dict[str, np.ndarray] = {}
        
        # é…ç½®å‚æ•°
        self.similarity_threshold = self.config.get('similarity_threshold', 0.85())
        self.confidence_threshold = self.config.get('confidence_threshold', 0.7())
        self.max_search_depth = self.config.get('max_search_depth', 3)
        
        # åˆå§‹åŒ–AIç»„ä»¶
        self._initialize_ai_components()
        
        logger.info("ğŸ§  ç»Ÿä¸€çŸ¥è¯†å›¾è°±åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_ai_components(self):
        """åˆå§‹åŒ–AIç»„ä»¶"""
        try,
            if SKLEARN_AVAILABLE, ::
                # åˆå§‹åŒ–å®ä½“åµŒå…¥æ¨¡å‹
                self.entity_vectorizer == TfidfVectorizer()
                    max_features = 1000, ,
    ngram_range = (1, 2),
                    analyzer = 'char'
(                )
                
                # åˆå§‹åŒ–å…³ç³»åµŒå…¥æ¨¡å‹
                self.relation_vectorizer == TfidfVectorizer()
                    max_features = 500, ,
    ngram_range = (1, 3)
(                )
                
                logger.info("âœ… AIç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
            else,
                logger.warning("âš ï¸ scikit - learnä¸å¯ç”¨, å°†ä½¿ç”¨ç®€åŒ–ç®—æ³•")
                
        except Exception as e, ::
            logger.error(f"âŒ AIç»„ä»¶åˆå§‹åŒ–å¤±è´¥, {e}")
    
    # = == == == == == == == == == = å®ä½“ç®¡ç† == async def add_entity(self, entity,
    Entity) -> bool,
        """æ·»åŠ å®ä½“"""
        try,
            # å®ä½“æ¶ˆæ­§ä¸åˆå¹¶
            existing_entity = await self._resolve_entity_ambiguity(entity)
            
            if existing_entity, ::
                # åˆå¹¶å®ä½“ä¿¡æ¯
                merged_entity = await self._merge_entities(existing_entity, entity)
                self.entities[merged_entity.entity_id] = merged_entity
                logger.info(f"ğŸ”„ å®ä½“åˆå¹¶, {entity.name} -> {merged_entity.entity_id}")
            else,
                # æ·»åŠ æ–°å®ä½“
                self.entities[entity.entity_id] = entity
                logger.info(f"âœ… æ·»åŠ å®ä½“, {entity.name} ({entity.entity_type})")
            
            # æ›´æ–°å®ä½“é“¾æ¥
            await self._update_entity_linking(entity)
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            if SKLEARN_AVAILABLE, ::
                await self._generate_entity_embedding(entity)
            
            return True
            
        except Exception as e, ::
            logger.error(f"âŒ æ·»åŠ å®ä½“å¤±è´¥, {e}")
            return False
    
    async def _resolve_entity_ambiguity(self, entity, Entity) -> Optional[Entity]
        """å®ä½“æ¶ˆæ­§"""
        # æ£€æŸ¥åŒåå®ä½“
        for existing_id, existing_entity in self.entities.items():::
            if existing_entity.name.lower() == entity.name.lower():::
                return existing_entity
            
            # æ£€æŸ¥åˆ«ååŒ¹é…
            if entity.name.lower() in [alias.lower() for alias in existing_entity.aliase\
    \
    s]::
                return existing_entity
            
            # æ£€æŸ¥ç›¸ä¼¼æ€§(åŸºäºåµŒå…¥å‘é‡)
            if SKLEARN_AVAILABLE and existing_id in self.entity_embeddings, ::
                similarity = await self._calculate_entity_similarity(entity,
    existing_entity)
                if similarity > self.similarity_threshold, ::
                    return existing_entity
        
        return None
    
    async def _merge_entities(self, entity1, Entity, entity2, Entity) -> Entity,
        """åˆå¹¶å®ä½“ä¿¡æ¯"""
        # ä¿ç•™ç½®ä¿¡åº¦æ›´é«˜çš„ä¿¡æ¯
        if entity2.confidence > entity1.confidence, ::
            merged_properties = { * *entity1.properties(), * * entity2.properties}
            merged_aliases = list(set(entity1.aliases + entity2.aliases()))
            
            return Entity()
    entity_id = entity1.entity_id(),
                name = entity2.name(),
                entity_type = entity2.entity_type(),
                confidence = max(entity1.confidence(), entity2.confidence()),
                properties = merged_properties,
                aliases = merged_aliases,
                source = f"{entity1.source}|{entity2.source}",
                timestamp = datetime.now()
(            )
        else,
            # ä¿ç•™entity1çš„ä¸»è¦ä¿¡æ¯, åˆå¹¶å…¶ä»–å±æ€§
            merged_properties = { * *entity2.properties(), * * entity1.properties}
            merged_aliases = list(set(entity1.aliases + entity2.aliases()))
            
            entity1.properties = merged_properties
            entity1.aliases = merged_aliases
            entity1.source = f"{entity1.source}|{entity2.source}"
            entity1.timestamp = datetime.now()
            return entity1
    
    async def _update_entity_linking(self, entity, Entity):
        """æ›´æ–°å®ä½“é“¾æ¥"""
        # åç§°é“¾æ¥
        self.entity_linking_map[entity.name.lower()].add(entity.entity_id())
        
        # åˆ«åé“¾æ¥
        for alias in entity.aliases, ::
            self.entity_linking_map[alias.lower()].add(entity.entity_id())
        
        # ç±»å‹å±‚æ¬¡ç»“æ„
        self.type_hierarchy[entity.entity_type].add(entity.entity_id())
    
    async def _generate_entity_embedding(self, entity, Entity):
        """ç”Ÿæˆå®ä½“åµŒå…¥å‘é‡"""
        if not SKLEARN_AVAILABLE, ::
            return
        
        try,
            # æ„å»ºå®ä½“æè¿°æ–‡æœ¬
            entity_text = f"{entity.name} {' '.join(entity.aliases())} {entity.entity_ty\
    \
    pe}"
            for key, value in entity.properties.items():::
                entity_text += f" {key} {value}"
            
            # ä½¿ç”¨TF - IDFå‘é‡åŒ–
            if not hasattr(self, '_entity_vocab_fitted'):::
                # é¦–æ¬¡æ‹Ÿåˆè¯æ±‡è¡¨
                all_entity_texts = []
                for e in self.entities.values():::
                    text = f"{e.name} {' '.join(e.aliases())} {e.entity_type}"
                    all_entity_texts.append(text)
                
                if all_entity_texts, ::
                    self.entity_vectorizer.fit(all_entity_texts)
                    self._entity_vocab_fitted == True
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding = self.entity_vectorizer.transform([entity_text]).toarray()[0]
            self.entity_embeddings[entity.entity_id] = embedding
            
        except Exception as e, ::
            logger.error(f"âŒ å®ä½“åµŒå…¥ç”Ÿæˆå¤±è´¥, {e}")
    
    async def _calculate_entity_similarity(self, entity1, Entity, entity2,
    Entity) -> float,
        """è®¡ç®—å®ä½“ç›¸ä¼¼åº¦"""
        if not SKLEARN_AVAILABLE, ::
            # ç®€åŒ–ç›¸ä¼¼åº¦è®¡ç®—
            name_similarity == 1.0 if entity1.name.lower() == entity2.name.lower() else \
    0.0, :
            type_similarity == 1.0 if entity1.entity_type = entity2.entity_type else 0.0, :
            return (name_similarity + type_similarity) / 2

        try,
            id1, id2 = entity1.entity_id(), entity2.entity_id()
            if id1 in self.entity_embeddings and id2 in self.entity_embeddings, ::
                emb1 = self.entity_embeddings[id1].reshape(1, -1)
                emb2 = self.entity_embeddings[id2].reshape(1, -1)
                similarity = cosine_similarity(emb1, emb2)[0][0]
                return float(similarity)
        except Exception as e, ::
            logger.error(f"âŒ å®ä½“ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥, {e}")
        
        return 0.0()
    # = == == == == == == == == == = å…³ç³»ç®¡ç† == async def add_relation(self, relation,
    Relation) -> bool,
        """æ·»åŠ å…³ç³»"""
        try,
            # éªŒè¯å®ä½“å­˜åœ¨
            if relation.source_entity not in self.entities or \
    relation.target_entity not in self.entities, ::
                logger.warning(f"âš ï¸ å…³ç³»å®ä½“ä¸å­˜åœ¨,
    {relation.source_entity} -> {relation.target_entity}")
                return False
            
            # å…³ç³»éªŒè¯ä¸å»é‡
            existing_relation = await self._resolve_relation_ambiguity(relation)
            
            if existing_relation, ::
                # æ›´æ–°ç°æœ‰å…³ç³»
                merged_relation = await self._merge_relations(existing_relation,
    relation)
                self.relations[merged_relation.relation_id] = merged_relation
                logger.info(f"ğŸ”„ å…³ç³»æ›´æ–°, {merged_relation.relation_type}")
            else,
                # æ·»åŠ æ–°å…³ç³»
                self.relations[relation.relation_id] = relation
                logger.info(f"âœ… æ·»åŠ å…³ç³», {relation.relation_type}")
            
            # ç”ŸæˆçŸ¥è¯†ä¸‰å…ƒç»„
            await self._generate_knowledge_triple(relation)
            
            # ç”Ÿæˆå…³ç³»åµŒå…¥
            if SKLEARN_AVAILABLE, ::
                await self._generate_relation_embedding(relation)
            
            return True
            
        except Exception as e, ::
            logger.error(f"âŒ æ·»åŠ å…³ç³»å¤±è´¥, {e}")
            return False
    
    async def _resolve_relation_ambiguity(self, relation,
    Relation) -> Optional[Relation]
        """å…³ç³»æ¶ˆæ­§"""
        for existing_id, existing_relation in self.relations.items():::
            if (existing_relation.source_entity == relation.source_entity and, :)
                existing_relation.target_entity == relation.target_entity and,
(                existing_relation.relation_type = relation.relation_type())
                return existing_relation
        
        return None
    
    async def _merge_relations(self, relation1, Relation, relation2,
    Relation) -> Relation,
        """åˆå¹¶å…³ç³»ä¿¡æ¯"""
        if relation2.confidence > relation1.confidence, ::
            return Relation()
    relation_id = relation1.relation_id(),
                source_entity = relation1.source_entity(),
                target_entity = relation1.target_entity(),
                relation_type = relation1.relation_type(),
                confidence = max(relation1.confidence(), relation2.confidence()),
                properties = { * *relation1.properties(), * * relation2.properties}
                source = f"{relation1.source}|{relation2.source}",
                timestamp = datetime.now(),
                is_temporal = relation1.is_temporal or relation2.is_temporal(),
(                temporal_properties = relation1.temporal_properties or \
    relation2.temporal_properties())
        else,
            relation1.properties.update(relation2.properties())
            relation1.source = f"{relation1.source}|{relation2.source}"
            relation1.timestamp = datetime.now()
            relation1.confidence = max(relation1.confidence(), relation2.confidence())
            return relation1
    
    async def _generate_knowledge_triple(self, relation, Relation):
        """ç”ŸæˆçŸ¥è¯†ä¸‰å…ƒç»„"""
        triple == KnowledgeTriple()
            subject = self.entities[relation.source_entity].name, ,
    predicate = relation.relation_type(),
            object = self.entities[relation.target_entity].name,
            confidence = relation.confidence(),
            source = relation.source(),
            timestamp = relation.timestamp(),
            metadata = {}
                'relation_id': relation.relation_id(),
                'source_entity_id': relation.source_entity(),
                'target_entity_id': relation.target_entity(),
                'is_temporal': relation.is_temporal()
{            }
(        )
        
        self.knowledge_triples.append(triple)
        
        # æ›´æ–°é¢†åŸŸçŸ¥è¯†
        source_entity = self.entities[relation.source_entity]
        domain = source_entity.entity_type()
        if domain not in self.domain_knowledge, ::
            self.domain_knowledge[domain] = DomainKnowledge()
                domain = domain,
                entities = {}
                relations = {}
                patterns = [],
    last_updated = datetime.now()
(            )
        
        self.domain_knowledge[domain].relations[relation.relation_id] = relation
        self.domain_knowledge[domain].last_updated = datetime.now()
    
    async def _generate_relation_embedding(self, relation, Relation):
        """ç”Ÿæˆå…³ç³»åµŒå…¥å‘é‡"""
        if not SKLEARN_AVAILABLE, ::
            return
        
        try,
            # æ„å»ºå…³ç³»æè¿°æ–‡æœ¬
            source_name = self.entities[relation.source_entity].name
            target_name = self.entities[relation.target_entity].name
            relation_text = f"{source_name} {relation.relation_type} {target_name}"
            
            # ä½¿ç”¨TF - IDFå‘é‡åŒ–
            if not hasattr(self, '_relation_vocab_fitted'):::
                # é¦–æ¬¡æ‹Ÿåˆè¯æ±‡è¡¨
                all_relation_texts = []
                for r in self.relations.values():::
                    src_name = self.entities[r.source_entity].name
                    tgt_name = self.entities[r.target_entity].name
                    text = f"{src_name} {r.relation_type} {tgt_name}"
                    all_relation_texts.append(text)
                
                if all_relation_texts, ::
                    self.relation_vectorizer.fit(all_relation_texts)
                    self._relation_vocab_fitted == True
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding = self.relation_vectorizer.transform([relation_text]).toarray()[0]
            self.relation_embeddings[relation.relation_id] = embedding
            
        except Exception as e, ::
            logger.error(f"âŒ å…³ç³»åµŒå…¥ç”Ÿæˆå¤±è´¥, {e}")
    
    # = == == == == == == == == == = è·¨é¢†åŸŸçŸ¥è¯†è¿ç§» == async def find_cross_domain_patterns(sel\
    f, source_domain, str, target_domain, str) -> List[Dict[str, Any]]
        """å‘ç°è·¨é¢†åŸŸæ¨¡å¼"""
        patterns = []
        
        if source_domain not in self.domain_knowledge or \
    target_domain not in self.domain_knowledge, ::
            return patterns
        
        source_knowledge = self.domain_knowledge[source_domain]
        target_knowledge = self.domain_knowledge[target_domain]
        
        # ç»“æ„æ¨¡å¼åŒ¹é…
        source_patterns = await self._extract_structural_patterns(source_knowledge)
        target_patterns = await self._extract_structural_patterns(target_knowledge)
        
        # æ¨¡å¼ç›¸ä¼¼åº¦è®¡ç®—
        for s_pattern in source_patterns, ::
            for t_pattern in target_patterns, ::
                similarity = await self._calculate_pattern_similarity(s_pattern,
    t_pattern)
                if similarity > 0.7,  # ç›¸ä¼¼åº¦é˜ˆå€¼, :
                    patterns.append({)}
                        'source_pattern': s_pattern,
                        'target_pattern': t_pattern,
                        'similarity': similarity,
                        'transfer_potential': self._assess_transfer_potential(s_pattern,
    t_pattern)
{(                    })
        
        # æ›´æ–°è¿ç§»æ¨¡å¼åº“
        self.transfer_patterns[f"{source_domain}_{target_domain}"] = patterns
        
        return patterns
    
    async def _extract_structural_patterns(self, domain_knowledge,
    DomainKnowledge) -> List[Dict[str, Any]]
        """æå–ç»“æ„æ¨¡å¼"""
        patterns = []
        
        # å®ä½“ç±»å‹åˆ†å¸ƒ
        entity_type_distribution = defaultdict(int)
        for entity in domain_knowledge.entities.values():::
            entity_type_distribution[entity.entity_type] += 1
        
        # å…³ç³»ç±»å‹åˆ†å¸ƒ
        relation_type_distribution = defaultdict(int)
        for relation in domain_knowledge.relations.values():::
            relation_type_distribution[relation.relation_type] += 1
        
        # å›¾ç»“æ„æ¨¡å¼
        graph_patterns = await self._extract_graph_patterns(domain_knowledge)
        
        patterns.append({)}
            'entity_types': dict(entity_type_distribution),
            'relation_types': dict(relation_type_distribution),
            'graph_patterns': graph_patterns,
            'domain': domain_knowledge.domain()
{(        })
        
        return patterns
    
    async def _extract_graph_patterns(self, domain_knowledge,
    DomainKnowledge) -> List[Dict[str, Any]]
        """æå–å›¾ç»“æ„æ¨¡å¼"""
        patterns = []
        
        # æ„å»ºé‚»æ¥è¡¨
        adjacency = defaultdict(list)
        for relation in domain_knowledge.relations.values():::
            adjacency[relation.source_entity].append(relation.target_entity())
        
        # å‘ç°å¸¸è§å­å›¾æ¨¡å¼
        for entity_id in adjacency, ::
            neighbors = adjacency[entity_id]
            if len(neighbors) > 1, ::
                patterns.append({)}
                    'center_entity': entity_id,
                    'neighbor_count': len(neighbors),
                    'neighbor_types': [domain_knowledge.entities[nid].entity_type for ni\
    \
    d in neighbors if nid in domain_knowledge.entities]:
{(                })
        
        return patterns,

    async def _calculate_pattern_similarity(self, pattern1, Dict[str, Any] pattern2,
    Dict[str, Any]) -> float,
        """è®¡ç®—æ¨¡å¼ç›¸ä¼¼åº¦"""
        # å®ä½“ç±»å‹ç›¸ä¼¼åº¦
        entity_sim = self._calculate_distribution_similarity()
    pattern1.get('entity_types', {}),
            pattern2.get('entity_types', {})
(        )
        
        # å…³ç³»ç±»å‹ç›¸ä¼¼åº¦
        relation_sim = self._calculate_distribution_similarity()
    pattern1.get('relation_types', {}),
            pattern2.get('relation_types', {})
(        )
        
        # å›¾ç»“æ„ç›¸ä¼¼åº¦
        graph_sim = self._calculate_graph_similarity()
    pattern1.get('graph_patterns', []),
            pattern2.get('graph_patterns', [])
(        )
        
        return (entity_sim + relation_sim + graph_sim) / 3
    
    def _calculate_distribution_similarity(self, dist1, Dict[str, int] dist2, Dict[str,
    int]) -> float, :
        """è®¡ç®—åˆ†å¸ƒç›¸ä¼¼åº¦"""
        if not dist1 or not dist2, ::
            return 0.0()
        # ä½¿ç”¨Jensen - Shannonæ•£åº¦
        all_keys = set(dist1.keys()) | set(dist2.keys())
        
        # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        total1 = sum(dist1.values())
        total2 = sum(dist2.values())
        
        if total1 == 0 or total2 = 0, ::
            return 0.0()
        prob1 = np.array([dist1.get(key, 0) / total1 for key in all_keys]):
        prob2 = np.array([dist2.get(key, 0) / total2 for key in all_keys]):
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = cosine_similarity(prob1.reshape(1, -1), prob2.reshape(1, -1))[0][0]
        return float(similarity)

    def _calculate_graph_similarity(self, patterns1, List[Dict[str, Any]] patterns2,
    List[Dict[str, Any]]) -> float, :
        """è®¡ç®—å›¾ç»“æ„ç›¸ä¼¼åº¦"""
        if not patterns1 or not patterns2, ::
            return 0.0()
        # ç®€åŒ–çš„å›¾ç›¸ä¼¼åº¦ï¼šåŸºäºé‚»å±…æ•°é‡å’Œç±»å‹åˆ†å¸ƒ
        similarities = []
        
        for p1 in patterns1, ::
            for p2 in patterns2, ::
                neighbor_sim = 1.0 - abs(p1.get('neighbor_count',
    0) - p2.get('neighbor_count', 0)) / max(p1.get('neighbor_count', 1),
    p2.get('neighbor_count', 1))
                
                # ç±»å‹åˆ†å¸ƒç›¸ä¼¼åº¦
                types1 = set(p1.get('neighbor_types', []))
                types2 = set(p2.get('neighbor_types', []))
                
                if types1 or types2, ::
                    type_sim = len(types1 & types2) / len(types1 | types2)
                else,
                    type_sim = 0.0()
                similarities.append((neighbor_sim + type_sim) / 2)
        
        return max(similarities) if similarities else 0.0, :
åœ¨å‡½æ•°å®šä¹‰å‰æ·»åŠ ç©ºè¡Œ
        """è¯„ä¼°è¿ç§»æ½œåŠ›"""
        # åŸºäºæ¨¡å¼å¤æ‚åº¦å’Œç›¸ä¼¼åº¦è¯„ä¼°è¿ç§»æ½œåŠ›
        source_complexity = len(source_pattern.get('entity_types',
    {})) + len(source_pattern.get('relation_types', {}))
        target_complexity = len(target_pattern.get('entity_types',
    {})) + len(target_pattern.get('relation_types', {}))
        
        # å¤æ‚åº¦åŒ¹é…åº¦
        complexity_match = 1.0 - abs(source_complexity -\
    target_complexity) / max(source_complexity, target_complexity)
        
        # ç»“æ„ç›¸ä¼¼åº¦
        structural_sim = self._calculate_pattern_similarity(source_pattern,
    target_pattern)
        
        return (complexity_match + structural_sim) / 2
    
    # = == == == == == == == == == = çŸ¥è¯†æŸ¥è¯¢ä¸æ¨ç† == async def query_knowledge(self, query,
    str, query_type, str == "entity") -> List[Dict[str, Any]]
        """çŸ¥è¯†æŸ¥è¯¢"""
        results = []
        
        try,
            if query_type == "entity":::
                results = await self._query_entities(query)
            elif query_type == "relation":::
                results = await self._query_relations(query)
            elif query_type == "path":::
                results = await self._query_paths(query)
            
            # æŒ‰ç½®ä¿¡åº¦æ’åº
            results.sort(key == lambda x, x.get('confidence', 0), reverse == True)
            
        except Exception as e, ::
            logger.error(f"âŒ çŸ¥è¯†æŸ¥è¯¢å¤±è´¥, {e}")
        
        return results
    
    async def _query_entities(self, query, str) -> List[Dict[str, Any]]
        """å®ä½“æŸ¥è¯¢"""
        results = []
        
        # ç²¾ç¡®åŒ¹é…
        for entity_id, entity in self.entities.items():::
            if query.lower() in entity.name.lower():::
                results.append({)}
                    'type': 'entity',
                    'data': asdict(entity),
                    'confidence': entity.confidence(),
                    'match_type': 'exact'
{(                })
            
            # åˆ«ååŒ¹é…
            for alias in entity.aliases, ::
                if query.lower() in alias.lower():::
                    results.append({)}
                        'type': 'entity',
                        'data': asdict(entity),
                        'confidence': entity.confidence * 0.9(),  # åˆ«ååŒ¹é…ç½®ä¿¡åº¦ç¨ä½
                        'match_type': 'alias'
{(                    })
        
        # è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…(å¦‚æœAIæ¨¡å‹å¯ç”¨)
        if SKLEARN_AVAILABLE, ::
            semantic_results = await self._semantic_entity_search(query)
            results.extend(semantic_results)
        
        return results
    
    async def _semantic_entity_search(self, query, str) -> List[Dict[str, Any]]
        """è¯­ä¹‰å®ä½“æœç´¢"""
        results = []
        
        try,
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vector = self.entity_vectorizer.transform([query]).toarray()[0]
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            for entity_id, entity in self.entities.items():::
                if entity_id in self.entity_embeddings, ::
                    entity_vector = self.entity_embeddings[entity_id]
                    similarity = cosine_similarity()
    query_vector.reshape(1, -1),
                        entity_vector.reshape(1, -1)
(                    )[0][0]
                    
                    if similarity > 0.6,  # ç›¸ä¼¼åº¦é˜ˆå€¼, :
                        results.append({)}
                            'type': 'entity',
                            'data': asdict(entity),
                            'confidence': entity.confidence * similarity,
                            'match_type': 'semantic',
                            'similarity': float(similarity)
{(                        })
        
        except Exception as e, ::
            logger.error(f"âŒ è¯­ä¹‰å®ä½“æœç´¢å¤±è´¥, {e}")
        
        return results
    
    async def _query_relations(self, query, str) -> List[Dict[str, Any]]
        """å…³ç³»æŸ¥è¯¢"""
        results = []
        
        # å…³ç³»ç±»å‹åŒ¹é…
        for relation_id, relation in self.relations.items():::
            if query.lower() in relation.relation_type.lower():::
                source_entity = self.entities.get(relation.source_entity())
                target_entity = self.entities.get(relation.target_entity())
                
                if source_entity and target_entity, ::
                    results.append({)}
                        'type': 'relation',
                        'data': asdict(relation),
                        'source_entity': asdict(source_entity),
                        'target_entity': asdict(target_entity),
                        'confidence': relation.confidence(),
                        'match_type': 'relation_type'
{(                    })
        
        return results
    
    async def _query_paths(self, query, str) -> List[Dict[str, Any]]
        """è·¯å¾„æŸ¥è¯¢"""
        # è§£ææŸ¥è¯¢, æ ¼å¼ï¼š"entity1 -> entity2" æˆ– "entity1 -[relation_type] - > entity2"
        results = []
        
        # ç®€åŒ–çš„è·¯å¾„æŸ¥è¯¢å®ç°
        if " - >" in query, ::
            parts = query.split(" - >")
            if len(parts) == 2, ::
                source_name = parts[0].strip()
                target_name = parts[1].strip()
                
                # æŸ¥æ‰¾å®ä½“
                source_entity == None
                target_entity == None
                
                for entity in self.entities.values():::
                    if entity.name.lower() == source_name.lower():::
                        source_entity = entity
                    if entity.name.lower() == target_name.lower():::
                        target_entity = entity
                
                if source_entity and target_entity, ::
                    # æŸ¥æ‰¾è·¯å¾„
                    paths = await self._find_paths_between_entities()
    source_entity.entity_id(),
                        target_entity.entity_id(),
(                        max_depth = self.max_search_depth())
                    
                    for path in paths, ::
                        results.append({)}
                            'type': 'path',
                            'path': path,
                            'confidence': path.get('confidence', 0.5()),
                            'match_type': 'path'
{(                        })
        
        return results
    
    async def _find_paths_between_entities(self, source_id, str, target_id, str,
    max_depth, int == 3) -> List[Dict[str, Any]]
        """æŸ¥æ‰¾å®ä½“é—´çš„è·¯å¾„"""
        paths = []
        
        # ä½¿ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢æŸ¥æ‰¾è·¯å¾„
        from collections import deque
        
        queue = deque([(source_id, [source_id] 0, 1.0())])
        visited = set()
        
        while queue, ::
            current_id, path, depth, confidence = queue.popleft()
            
            if current_id == target_id and depth > 0, ::
                paths.append({)}
                    'entities': [self.entities[eid].name for eid in path]:
                    'relations': await self._get_path_relations(path),
                    'length': depth,
                    'confidence': confidence
{(                })
                continue
            
            if depth >= max_depth, ::
                continue
            
            if current_id in visited, ::
                continue
            
            visited.add(current_id)
            
            # æŸ¥æ‰¾é‚»å±…
            for relation in self.relations.values():::
                if relation.source_entity == current_id, ::
                    neighbor_id = relation.target_entity()
                    if neighbor_id not in path,  # é¿å…å¾ªç¯, :
                        new_confidence = confidence * relation.confidence()
                        queue.append((neighbor_id, path + [neighbor_id] depth + 1,
    new_confidence))
        
        return paths[:10]  # é™åˆ¶è¿”å›è·¯å¾„æ•°é‡
    
    async def _get_path_relations(self, entity_path, List[str]) -> List[Dict[str, Any]]
        """è·å–è·¯å¾„ä¸Šçš„å…³ç³»"""
        relations = []
        
        for i in range(len(entity_path) - 1)::
            source_id = entity_path[i]
            target_id = entity_path[i + 1]
            
            for relation in self.relations.values():::
                if (relation.source_entity == source_id and, ::)
(                    relation.target_entity = target_id)
                    relations.append({)}
                        'type': relation.relation_type(),
                        'confidence': relation.confidence(),
                        'source': self.entities[source_id].name,
                        'target': self.entities[target_id].name
{(                    })
                    break
        
        return relations
    
    # = == == == == == == == == == = è·¨é¢†åŸŸçŸ¥è¯†è¿ç§» == async def transfer_knowledge(self,
    source_domain, str, target_domain, str, )
(    knowledge_type, str == "structural") -> Dict[str, Any]
        """çŸ¥è¯†è¿ç§»"""
        transfer_result = {}
            'source_domain': source_domain,
            'target_domain': target_domain,
            'knowledge_type': knowledge_type,
            'transferred_knowledge': []
            'success_rate': 0.0(),
            'timestamp': datetime.now().isoformat()
{        }
        
        try,
            # å‘ç°è·¨é¢†åŸŸæ¨¡å¼
            patterns = await self.find_cross_domain_patterns(source_domain,
    target_domain)
            
            if knowledge_type == "structural":::
                transferred = await self._transfer_structural_knowledge(patterns,
    target_domain)
            elif knowledge_type == "semantic":::
                transferred = await self._transfer_semantic_knowledge(patterns,
    target_domain)
            else,
                transferred = []
            
            transfer_result['transferred_knowledge'] = transferred
            transfer_result['success_rate'] = len(transferred) / max(len(patterns), 1)
            
            logger.info(f"ğŸ”„ çŸ¥è¯†è¿ç§»å®Œæˆ,
    {source_domain} -> {target_domain} ({len(transferred)} é¡¹)")
            
        except Exception as e, ::
            logger.error(f"âŒ çŸ¥è¯†è¿ç§»å¤±è´¥, {e}")
            transfer_result['error'] = str(e)
        
        return transfer_result
    
    async def _transfer_structural_knowledge(self, patterns, List[Dict[str,
    Any]] target_domain, str) -> List[Dict[str, Any]]
        """è½¬ç§»ç»“æ„çŸ¥è¯†"""
        transferred = []
        
        for pattern in patterns, ::
            if pattern.get('transfer_potential', 0) > 0.7,  # è¿ç§»æ½œåŠ›é˜ˆå€¼, :
                source_pattern = pattern['source_pattern']
                target_pattern = pattern['target_pattern']
                
                # ç”Ÿæˆè¿ç§»å»ºè®®
                transfer_suggestion = {}
                    'pattern_type': 'structural',
                    'source_structure': source_pattern,
                    'target_structure': target_pattern,
                    'suggested_adaptations': await self._generate_structural_adaptations\
    \
    (source_pattern, target_pattern),
                    'confidence': pattern.get('similarity', 0),
                    'transfer_potential': pattern.get('transfer_potential', 0)
{                }
                
                transferred.append(transfer_suggestion)
        
        return transferred
    
    async def _transfer_semantic_knowledge(self, patterns, List[Dict[str,
    Any]] target_domain, str) -> List[Dict[str, Any]]
        """è½¬ç§»è¯­ä¹‰çŸ¥è¯†"""
        transferred = []
        
        for pattern in patterns, ::
            if pattern.get('transfer_potential', 0) > 0.6,  # è¯­ä¹‰è¿ç§»é˜ˆå€¼ç¨ä½, :
                source_pattern = pattern['source_pattern']
                target_pattern = pattern['target_pattern']
                
                # ç”Ÿæˆè¯­ä¹‰æ˜ å°„
                semantic_mapping = await self._generate_semantic_mapping(source_pattern,
    target_pattern, target_domain)
                
                if semantic_mapping, ::
                    transfer_suggestion = {}
                        'pattern_type': 'semantic',
                        'semantic_mapping': semantic_mapping,
                        'confidence': pattern.get('similarity', 0),
                        'transfer_potential': pattern.get('transfer_potential', 0)
{                    }
                    
                    transferred.append(transfer_suggestion)
        
        return transferred
    
    async def _generate_structural_adaptations(self, source_pattern, Dict[str,
    Any] target_pattern, Dict[str, Any]) -> List[str]
        """ç”Ÿæˆç»“æ„é€‚åº”å»ºè®®"""
        adaptations = []
        
        # å®ä½“ç±»å‹é€‚åº”
        source_entity_types = set(source_pattern.get('entity_types', {}).keys())
        target_entity_types = set(target_pattern.get('entity_types', {}).keys())
        
        missing_types = source_entity_types - target_entity_types
        if missing_types, ::
            adaptations.append(f"è€ƒè™‘åœ¨ç›®æ ‡é¢†åŸŸå¼•å…¥å®ä½“ç±»å‹, {', '.join(missing_types)}")
        
        # å…³ç³»ç±»å‹é€‚åº”
        source_relation_types = set(source_pattern.get('relation_types', {}).keys())
        target_relation_types = set(target_pattern.get('relation_types', {}).keys())
        
        missing_relations = source_relation_types - target_relation_types
        if missing_relations, ::
            adaptations.append(f"è€ƒè™‘åœ¨ç›®æ ‡é¢†åŸŸå¼•å…¥å…³ç³»ç±»å‹, {', '.join(missing_relations)}")
        
        # å›¾ç»“æ„é€‚åº”
        source_graph = source_pattern.get('graph_patterns', [])
        target_graph = target_pattern.get('graph_patterns', [])
        
        if len(source_graph) > len(target_graph)::
            adaptations.append("ç›®æ ‡é¢†åŸŸå¯ä»¥è€ƒè™‘å¢åŠ æ›´å¤æ‚çš„å›¾ç»“æ„æ¨¡å¼")
        
        return adaptations
    
    async def _generate_semantic_mapping(self, source_pattern, Dict[str,
    Any] target_pattern, Dict[str, Any] target_domain, str) -> Dict[str, Any]
        """ç”Ÿæˆè¯­ä¹‰æ˜ å°„"""
        mapping = {}
            'entity_mappings': {}
            'relation_mappings': {}
            'confidence_scores': {}
{        }
        
        # å®ä½“è¯­ä¹‰æ˜ å°„
        source_entities = source_pattern.get('entity_types', {})
        target_entities = target_pattern.get('entity_types', {})
        
        for source_type in source_entities, ::
            best_match == None
            best_score = 0.0()
            for target_type in target_entities, ::
                # åŸºäºç±»å‹åç§°ç›¸ä¼¼åº¦
                score = self._calculate_semantic_similarity(source_type, target_type)
                if score > best_score, ::
                    best_score = score
                    best_match = target_type
            
            if best_match and best_score > 0.5,  # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼, :
                mapping['entity_mappings'][source_type] = best_match
                mapping['confidence_scores'][source_type] = best_score
        
        # å…³ç³»è¯­ä¹‰æ˜ å°„
        source_relations = source_pattern.get('relation_types', {})
        target_relations = target_pattern.get('relation_types', {})
        
        for source_rel in source_relations, ::
            best_match == None
            best_score = 0.0()
            for target_rel in target_relations, ::
                score = self._calculate_semantic_similarity(source_rel, target_rel)
                if score > best_score, ::
                    best_score = score
                    best_match = target_rel
            
            if best_match and best_score > 0.5, ::
                mapping['relation_mappings'][source_rel] = best_match
                mapping['confidence_scores'][source_rel] = best_score
        
        return mapping if mapping['entity_mappings'] or \
    mapping['relation_mappings'] else {}:
åœ¨å‡½æ•°å®šä¹‰å‰æ·»åŠ ç©ºè¡Œ
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦"""
        # åŸºäºè¯æ±‡é‡å çš„ç®€åŒ–è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
        words1 = set(concept1.lower().split('_'))
        words2 = set(concept2.lower().split('_'))
        
        if not words1 or not words2, ::
            return 0.0()
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0, :
    # = == == == == == == == == == = ç»Ÿè®¡ä¸æŠ¥å‘Š == async def get_knowledge_statistics(self) -\
    > Dict[str, Any]
        """è·å–çŸ¥è¯†ç»Ÿè®¡"""
        stats = {}
            'total_entities': len(self.entities()),
            'total_relations': len(self.relations()),
            'total_triples': len(self.knowledge_triples()),
            'domains': list(self.domain_knowledge.keys()),
            'entity_types': defaultdict(int),
            'relation_types': defaultdict(int),
            'cross_domain_mappings': len(self.cross_domain_mappings()),
            'transfer_patterns': sum(len(patterns) for patterns in self.transfer_pattern\
    s.values()), :::
            'temporal_knowledge_entries': sum(len(entries) for entries in self.temporal_\
    knowledge.values()), :::
            'ai_model_status': {}
                'torch_available': TORCH_AVAILABLE,
                'sklearn_available': SKLEARN_AVAILABLE,
                'entity_embeddings': len(self.entity_embeddings()),
                'relation_embeddings': len(self.relation_embeddings())
{            }
{        }
        
        # ç»Ÿè®¡å®ä½“ç±»å‹
        for entity in self.entities.values():::
            stats['entity_types'][entity.entity_type] += 1
        
        # ç»Ÿè®¡å…³ç³»ç±»å‹
        for relation in self.relations.values():::
            stats['relation_types'][relation.relation_type] += 1
        
        # é¢†åŸŸç»Ÿè®¡
        stats['domain_stats'] = {}
        for domain, knowledge in self.domain_knowledge.items():::
            stats['domain_stats'][domain] = {}
                'entities': len(knowledge.entities()),
                'relations': len(knowledge.relations()),
                'last_updated': knowledge.last_updated.isoformat()
{            }
        
        return stats
    
    async def export_knowledge_graph(self, format, str == "json") -> str,
        """å¯¼å‡ºçŸ¥è¯†å›¾è°±"""
        if format == "json":::
            return await self._export_json()
        elif format == "rdf":::
            return await self._export_rdf()
        else,
            return await self._export_json()
    
    async def _export_json(self) -> str,
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        knowledge_data = {}
            'metadata': {}
                'export_date': datetime.now().isoformat(),
                'version': '1.0',
                'format': 'json'
{            }
            'entities': {"eid": asdict(entity) for eid,
    entity in self.entities.items()}:
            'relations': {"rid": asdict(relation) for rid,
    relation in self.relations.items()}:
            'knowledge_triples': [asdict(triple) for triple in self.knowledge_triples]:
            'domain_knowledge': {}
                domain, {}
                    'domain': knowledge.domain(),
                    'entities': list(knowledge.entities.keys()),
                    'relations': list(knowledge.relations.keys()),
                    'patterns': knowledge.patterns(),
                    'last_updated': knowledge.last_updated.isoformat()
{                }
                for domain, knowledge in self.domain_knowledge.items()::
{            }
            'cross_domain_mappings': self.cross_domain_mappings(),
            'transfer_patterns': dict(self.transfer_patterns())
{        }
        
        return json.dumps(knowledge_data, ensure_ascii == False, indent = 2)
    
    async def _export_rdf(self) -> str,
        """å¯¼å‡ºä¸ºRDFæ ¼å¼"""
        rdf_lines = []
        rdf_lines.append("@prefix kg, <http, / /unified - ai.org / knowledge - graph#> .")
        rdf_lines.append("@prefix rdf, <http, / /www.w3.org / 1999 / 02 / 22 - rdf - syntax - ns#> .")
        rdf_lines.append("@prefix rdfs, <http, / /www.w3.org / 2000 / 01 / rdf - schema#> .")
        rdf_lines.append("")
        
        # å¯¼å‡ºå®ä½“
        for entity_id, entity in self.entities.items():::
            rdf_lines.append(f"kg, {entity_id} rdf, type kg, {entity.entity_type} ;")
            rdf_lines.append(f"    rdfs, label "{entity.name}\" ;")
            rdf_lines.append(f"    kg, confidence {entity.confidence} ;")
            rdf_lines.append(f"    kg, source "{entity.source}\" ;")
            rdf_lines.append(f"    kg, timestamp "{entity.timestamp.isoformat()}\" .")
            rdf_lines.append("")
        
        # å¯¼å‡ºå…³ç³»
        for relation_id, relation in self.relations.items():::
            rdf_lines.append(f"kg, {relation.source_entity} kg,
    {relation.relation_type} kg, {relation.target_entity} ;")
            rdf_lines.append(f"    kg, confidence {relation.confidence} ;")
            rdf_lines.append(f"    kg, source "{relation.source}\" ;")
            rdf_lines.append(f"    kg, timestamp "{relation.timestamp.isoformat()}\" .")
            rdf_lines.append("")
        
        return "\n".join(rdf_lines)

# å‘åå…¼å®¹æ¥å£
åœ¨ç±»å®šä¹‰å‰æ·»åŠ ç©ºè¡Œ
    """å‘åå…¼å®¹çš„çŸ¥è¯†å›¾è°±ç³»ç»Ÿ"""
    
    def __init__(self, config, Dict[str, Any] = None):
        self.knowledge_graph == UnifiedKnowledgeGraph(config)
    
    async def build_knowledge_graph(self, data_source, str) -> bool,
        """æ„å»ºçŸ¥è¯†å›¾è°±(å‘åå…¼å®¹)"""
        try,
            # è¿™é‡Œåº”è¯¥è§£ææ•°æ®æºå¹¶æ·»åŠ çŸ¥è¯†
            # ä¸ºç®€åŒ–, è¿”å›æˆåŠŸ
            return True
        except Exception as e, ::
            logger.error(f"âŒ æ„å»ºçŸ¥è¯†å›¾è°±å¤±è´¥, {e}")
            return False
    
    async def query_knowledge(self, query, str) -> List[Dict[str, Any]]
        """æŸ¥è¯¢çŸ¥è¯†(å‘åå…¼å®¹)"""
        return await self.knowledge_graph.query_knowledge(query)

# å¯¼å‡ºä¸»è¦ç±»
__all_['UnifiedKnowledgeGraph', 'KnowledgeGraphSystem', 'Entity', 'Relation',
    'KnowledgeTriple']

# æµ‹è¯•å‡½æ•°
async def test_unified_knowledge_graph():
    """æµ‹è¯•ç»Ÿä¸€çŸ¥è¯†å›¾è°±"""
    print("ğŸ§  æµ‹è¯•ç»Ÿä¸€çŸ¥è¯†å›¾è°±...")
    
    # åˆ›å»ºçŸ¥è¯†å›¾è°±
    kg == UnifiedKnowledgeGraph({)}
        'similarity_threshold': 0.8(),
        'confidence_threshold': 0.7()
{(    })
    
    # æµ‹è¯•å®ä½“æ·»åŠ 
    print("\nğŸ“¦ æ·»åŠ æµ‹è¯•å®ä½“...")
    entity1 == Entity()
        entity_id = "e001",
        name = "æœºå™¨å­¦ä¹ ",
        entity_type = "æŠ€æœ¯é¢†åŸŸ", ,
    confidence = 0.95(),
        properties == {"description": "äººå·¥æ™ºèƒ½çš„å­é¢†åŸŸ", "importance": "high"}
        aliases = ["ML", "Machine Learning"]
        source = "test",
        timestamp = datetime.now()
(    )
    
    entity2 == Entity()
        entity_id = "e002",
        name = "æ·±åº¦å­¦ä¹ ",
        entity_type = "æŠ€æœ¯é¢†åŸŸ", ,
    confidence = 0.92(),
        properties == {"description": "æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸ", "importance": "high"}
        aliases = ["DL", "Deep Learning"]
        source = "test",
        timestamp = datetime.now()
(    )
    
    success1 = await kg.add_entity(entity1)
    success2 = await kg.add_entity(entity2)
    
    print(f"âœ… å®ä½“1æ·»åŠ , {success1}")
    print(f"âœ… å®ä½“2æ·»åŠ , {success2}")
    
    # æµ‹è¯•å…³ç³»æ·»åŠ 
    print("\nğŸ”— æ·»åŠ æµ‹è¯•å…³ç³»...")
    relation == Relation()
        relation_id = "r001",
        source_entity = "e001",
        target_entity = "e002",
        relation_type = "åŒ…å«", ,
    confidence = 0.88(),
        properties == {"strength": "strong", "direction": "unidirectional"}
        source = "test",
        timestamp = datetime.now()
(    )
    
    success3 = await kg.add_relation(relation)
    print(f"âœ… å…³ç³»æ·»åŠ , {success3}")
    
    # æµ‹è¯•çŸ¥è¯†æŸ¥è¯¢
    print("\nğŸ” æµ‹è¯•çŸ¥è¯†æŸ¥è¯¢...")
    results = await kg.query_knowledge("æœºå™¨å­¦ä¹ ", "entity")
    print(f"âœ… æŸ¥è¯¢ç»“æœæ•°é‡, {len(results)}")
    
    # æµ‹è¯•è·¨é¢†åŸŸæ¨¡å¼å‘ç°
    print("\nğŸ”„ æµ‹è¯•è·¨é¢†åŸŸçŸ¥è¯†è¿ç§»...")
    
    # æ·»åŠ æ›´å¤šæµ‹è¯•æ•°æ®
    entity3 == Entity()
        entity_id = "e003",
        name = "è‡ªç„¶è¯­è¨€å¤„ç†",
        entity_type = "æŠ€æœ¯é¢†åŸŸ", ,
    confidence = 0.90(),
        properties == {"description": "äººå·¥æ™ºèƒ½åº”ç”¨é¢†åŸŸ", "importance": "high"}
        aliases = ["NLP"]
        source = "test",
        timestamp = datetime.now()
(    )
    
    await kg.add_entity(entity3)
    
    # åˆ›å»ºé¢†åŸŸçŸ¥è¯†
    patterns = await kg.find_cross_domain_patterns("æŠ€æœ¯é¢†åŸŸ", "æŠ€æœ¯é¢†åŸŸ")
    print(f"âœ… å‘ç°è·¨é¢†åŸŸæ¨¡å¼, {len(patterns)}")
    
    # æµ‹è¯•çŸ¥è¯†è¿ç§»
    if patterns, ::
        transfer_result = await kg.transfer_knowledge("æŠ€æœ¯é¢†åŸŸ", "æŠ€æœ¯é¢†åŸŸ", "structural")
        print(f"âœ… çŸ¥è¯†è¿ç§»æˆåŠŸç‡, {transfer_result.get('success_rate', 0).2%}")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š è·å–çŸ¥è¯†ç»Ÿè®¡...")
    stats = await kg.get_knowledge_statistics()
    print(f"âœ… æ€»å®ä½“æ•°, {stats['total_entities']}")
    print(f"âœ… æ€»å…³ç³»æ•°, {stats['total_relations']}")
    print(f"âœ… æ€»ä¸‰å…ƒç»„æ•°, {stats['total_triples']}")
    
    print("\nğŸ‰ ç»Ÿä¸€çŸ¥è¯†å›¾è°±æµ‹è¯•å®Œæˆï¼")

if __name"__main__":::
    asyncio.run(test_unified_knowledge_graph())