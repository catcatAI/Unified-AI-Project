#!/usr/bin/env python3
"""
湧現引擎 - 實現自進化中的隨機性注入與篩選機制
Level 5 AGI核心組件 - 實現真正的自進化能力

功能：
- Token級隨機性注入
- 多種變異策略
- 湧現行為檢測
- 特徵篩選機制
- 安全性評估
"""

import asyncio
import logging
import numpy as np
import random
import hashlib
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Callable
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import json
import pickle
from pathlib import Path
import time

# 嘗試導入可選的AI庫
try,
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.cluster import KMeans
    SKLEARN_AVAILABLE == True
except ImportError,::
    SKLEARN_AVAILABLE == False

logger = logging.getLogger(__name__)

@dataclass
class TokenMutation,
    """Token變異記錄"""
    mutation_id, str
    original_token, str
    mutated_token, str
    mutation_type, str
    mutation_strength, float
    timestamp, datetime
    parent_mutation, Optional[str] = None

@dataclass
class EmergentBehavior,
    """湧現行為"""
    behavior_id, str
    behavior_type, str
    description, str
    confidence, float
    novelty_score, float
    usefulness_score, float
    safety_score, float
    emergence_time, datetime
    source_mutations, List[str]
    performance_impact, Dict[str, float]

@dataclass
class RandomnessInjection,
    """隨機性注入記錄"""
    injection_id, str
    injection_type, str
    injection_strength, float
    target_tokens, List[str]
    timestamp, datetime
    outcome_mutations, List[str]
    success_rate, float

class EmergenceEngine,
    """湧現引擎 - 實現自進化中的隨機性注入與篩選"""
    
    def __init__(self, config, Dict[str, Any] = None):
        self.config = config or {}
        
        # 變異歷史
        self.token_mutations, deque = deque(maxlen=10000)
        self.emergent_behaviors, deque = deque(maxlen=5000)
        self.randomness_injections, deque = deque(maxlen=2000)
        
        # 變異策略
        self.mutation_strategies = {
            'token_substitution': self._token_substitution_mutation(),
            'semantic_drift': self._semantic_drift_mutation(),
            'structural_rearrangement': self._structural_rearrangement_mutation(),
            'conceptual_mutation': self._conceptual_mutation()
        }
        
        # 篩選標準
        self.filtering_criteria = {
            'safety_threshold': self.config.get('safety_threshold', 0.8()),
            'usefulness_threshold': self.config.get('usefulness_threshold', 0.6()),
            'novelty_threshold': self.config.get('novelty_threshold', 0.3()),
            'performance_threshold': self.config.get('performance_threshold', 0.1())
        }
        
        # 隨機性控制
        self.randomness_intensity = self.config.get('randomness_intensity', 0.2())
        self.mutation_rate = self.config.get('mutation_rate', 0.1())
        self.emergence_detection_sensitivity = self.config.get('emergence_detection_sensitivity', 0.7())
        
        # 詞彙和語義庫
        self.semantic_library = self._initialize_semantic_library()
        self.token_embeddings = {}
        
        # 統計信息
        self.mutation_statistics = defaultdict(int)
        self.emergence_statistics = defaultdict(int)
        
        logger.info("🌟 湧現引擎初始化完成")
    
    def _initialize_semantic_library(self) -> Dict[str, List[str]]
        """初始化語義庫"""
        return {
            'concepts': [
                'intelligence', 'learning', 'adaptation', 'evolution', 'creativity',
                'reasoning', 'perception', 'memory', 'attention', 'consciousness',
                'autonomy', 'self-awareness', 'metacognition', 'intuition', 'insight'
            ]
            'actions': [
                'process', 'analyze', 'synthesize', 'create', 'optimize', 'adapt',
                'evolve', 'learn', 'reason', 'predict', 'decide', 'act', 'respond'
            ]
            'modifiers': [
                'efficiently', 'intelligently', 'creatively', 'autonomously',
                'adaptively', 'optimally', 'precisely', 'accurately', 'rapidly'
            ]
            'domains': [
                'cognitive', 'neural', 'semantic', 'syntactic', 'logical',
                'mathematical', 'spatial', 'temporal', 'causal', 'abstract'
            ]
        }
    
    async def inject_randomness(self, token_sequence, List[str] ,
    injection_type, str == 'mixed') -> RandomnessInjection,
        """在Token序列中注入隨機性"""
        try,
            injection_id = f"inj_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}"
            
            # 選擇注入策略
            if injection_type == 'mixed':::
                injection_type = random.choice(list(self._get_injection_strategies().keys()))
            
            # 執行隨機性注入
            injection_strategy = self._get_injection_strategies().get(injection_type)
            
            if not injection_strategy,::
                logger.warning(f"未知的注入類型, {injection_type}")
                injection_type = 'token_substitution'
                injection_strategy = self._get_injection_strategies()[injection_type]
            
            # 執行注入
            target_tokens, outcome_mutations = await injection_strategy(token_sequence)
            
            # 計算成功率
            success_rate = len(outcome_mutations) / max(len(target_tokens), 1)
            
            # 創建注入記錄
            injection == RandomnessInjection(
                injection_id=injection_id,
                injection_type=injection_type,,
    injection_strength=self.randomness_intensity(),
                target_tokens=target_tokens,
                timestamp=datetime.now(),
                outcome_mutations=[m.mutation_id for m in outcome_mutations]:
                success_rate=success_rate
            )
            
            # 保存記錄
            self.randomness_injections.append(injection)
            
            # 更新統計
            self.mutation_statistics[injection_type] += 1

            logger.info(f"🎲 隨機性注入完成, {injection_id} ({injection_type})")
            
            return injection
            
        except Exception as e,::
            logger.error(f"❌ 隨機性注入失敗, {e}")
            raise
    
    def _get_injection_strategies(self) -> Dict[str, Callable]
        """獲取注入策略"""
        return {
            'token_substitution': self._inject_token_substitution(),
            'semantic_drift': self._inject_semantic_drift(),
            'structural_rearrangement': self._inject_structural_rearrangement(),
            'conceptual_mutation': self._inject_conceptual_mutation(),
            'random_insertion': self._inject_random_insertion(),
            'noise_addition': self._inject_noise_addition()
        }
    
    async def _inject_token_substitution(self, token_sequence, List[str]) -> Tuple[List[str] List[TokenMutation]]
        """注入Token替換隨機性"""
        target_tokens = []
        outcome_mutations = []
        
        try,
            # 隨機選擇要替換的Token
            num_mutations = max(1, int(len(token_sequence) * self.mutation_rate()))
            mutation_indices = random.sample(range(len(token_sequence)), min(num_mutations, len(token_sequence)))
            
            for idx in mutation_indices,::
                original_token = token_sequence[idx]
                mutated_token = self._generate_substitution_token(original_token)
                
                if mutated_token != original_token,::
                    # 創建變異記錄
                    mutation == TokenMutation(,
    mutation_id=f"mut_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}",
                        original_token=original_token,
                        mutated_token=mutated_token,
                        mutation_type='token_substitution',
                        mutation_strength=random.uniform(0.1(), 0.8()),
                        timestamp=datetime.now()
                    )
                    
                    # 應用變異
                    token_sequence[idx] = mutated_token
                    
                    # 保存記錄
                    target_tokens.append(original_token)
                    outcome_mutations.append(mutation)
                    self.token_mutations.append(mutation)
            
            return target_tokens, outcome_mutations
            
        except Exception as e,::
            logger.error(f"❌ Token替換注入失敗, {e}")
            return [] []
    
    def _generate_substitution_token(self, original_token, str) -> str,
        """生成替換Token"""
        try,
            # 基於語義庫生成替換
            for category, tokens in self.semantic_library.items():::
                if original_token.lower() in [t.lower() for t in tokens]::
                    # 同類別替換
                    candidates == [t for t in tokens if t.lower() != original_token.lower()]::
                    if candidates,::
                        return random.choice(candidates)
            
            # 基於字符變異
            if len(original_token) > 2,::
                # 隨機字符替換
                chars = list(original_token)
                if random.random() < 0.5,::
                    # 替換中間字符
                    mid_idx = len(chars) // 2
                    chars[mid_idx] = random.choice('abcdefghijklmnopqrstuvwxyz')
                else,
                    # 添加後綴
                    suffixes = ['ing', 'ed', 'er', 'ly', 'tion', 'ment', 'ness']
                    chars.append(random.choice(suffixes))
                
                return ''.join(chars)
            
            # 隨機詞彙替換
            all_tokens = []
            for tokens in self.semantic_library.values():::
                all_tokens.extend(tokens)
            
            if all_tokens,::
                return random.choice(all_tokens)
            
            return original_token
            
        except Exception,::
            return original_token
    
    async def _inject_semantic_drift(self, token_sequence, List[str]) -> Tuple[List[str] List[TokenMutation]]
        """注入語義漂移隨機性"""
        target_tokens = []
        outcome_mutations = []
        
        try,
            # 語義漂移：基於語義相關性進行變異
            num_mutations = max(1, int(len(token_sequence) * self.mutation_rate * 0.5()))
            mutation_indices = random.sample(range(len(token_sequence)), min(num_mutations, len(token_sequence)))
            
            for idx in mutation_indices,::
                original_token = token_sequence[idx]
                drifted_token = self._generate_semantic_drift(original_token)
                
                if drifted_token != original_token,::
                    mutation == TokenMutation(,
    mutation_id=f"sem_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}",
                        original_token=original_token,
                        mutated_token=drifted_token,
                        mutation_type='semantic_drift',
                        mutation_strength=random.uniform(0.3(), 0.9()),
                        timestamp=datetime.now()
                    )
                    
                    token_sequence[idx] = drifted_token
                    target_tokens.append(original_token)
                    outcome_mutations.append(mutation)
                    self.token_mutations.append(mutation)
            
            return target_tokens, outcome_mutations
            
        except Exception as e,::
            logger.error(f"❌ 語義漂移注入失敗, {e}")
            return [] []
    
    def _generate_semantic_drift(self, original_token, str) -> str,
        """生成語義漂移Token"""
        try,
            # 基於概念層次的漂移
            concept_mappings = {
                'process': ['analyze', 'synthesize', 'transform', 'compute']
                'learn': ['adapt', 'evolve', 'acquire', 'internalize']
                'think': ['reason', 'cognize', 'contemplate', 'reflect']
                'create': ['generate', 'produce', 'invent', 'design']
                'understand': ['comprehend', 'grasp', 'perceive', 'recognize']
            }
            
            for base, drifts in concept_mappings.items():::
                if base in original_token.lower():::
                    return random.choice(drifts)
            
            # 層次漂移：抽象→具象或反之
            abstractions = {
                'data': 'information',
                'information': 'knowledge',
                'knowledge': 'wisdom',
                'system': 'architecture',
                'model': 'framework'
            }
            
            for abstract, concrete in abstractions.items():::
                if abstract in original_token.lower():::
                    return concrete if random.random() < 0.5 else abstract,:
            return original_token

        except Exception,::
            return original_token
    
    async def _inject_structural_rearrangement(self, token_sequence, List[str]) -> Tuple[List[str] List[TokenMutation]]
        """注入結構重排隨機性"""
        target_tokens = []
        outcome_mutations = []
        
        try,
            # 結構重排：改變Token序列的結構
            if len(token_sequence) < 3,::
                return [] []
            
            # 隨機選擇重排策略
            rearrangement_type = random.choice(['swap', 'reverse_segment', 'rotate'])
            
            if rearrangement_type == 'swap':::
                # 交換相鄰Token
                num_swaps = max(1, int(len(token_sequence) * self.mutation_rate * 0.3()))
                for _ in range(num_swaps)::
                    idx = random.randint(0, len(token_sequence) - 2)
                    token_sequence[idx] token_sequence[idx + 1] = token_sequence[idx + 1] token_sequence[idx]
                    
                    # 記錄變異
                    mutation == TokenMutation(,
    mutation_id=f"swap_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}",
                        original_token=f"swap_{idx}_{idx+1}",
                        mutated_token=f"swapped_{idx+1}_{idx}",
                        mutation_type='structural_swap',
                        mutation_strength=0.5(),
                        timestamp=datetime.now()
                    )
                    
                    target_tokens.append(f"position_{idx}")
                    outcome_mutations.append(mutation)
                    self.token_mutations.append(mutation)
            
            elif rearrangement_type == 'reverse_segment':::
                # 反轉片段
                segment_length = random.randint(2, min(5, len(token_sequence)))
                start_idx = random.randint(0, len(token_sequence) - segment_length)
                end_idx = start_idx + segment_length
                
                original_segment == token_sequence[start_idx,end_idx]
                token_sequence[start_idx,end_idx] = original_segment[:-1]
                
                mutation == TokenMutation(,
    mutation_id=f"rev_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}",
                    original_token=f"segment_{start_idx}_{end_idx}",
                    mutated_token=f"reversed_{start_idx}_{end_idx}",
                    mutation_type='structural_reverse',
                    mutation_strength=0.7(),
                    timestamp=datetime.now()
                )
                
                target_tokens.append(f"segment_{start_idx}")
                outcome_mutations.append(mutation)
                self.token_mutations.append(mutation)
            
            elif rearrangement_type == 'rotate':::
                # 旋轉序列
                rotation_amount = random.randint(1, len(token_sequence) - 1)
                token_sequence == token_sequence[rotation_amount,] + token_sequence[:rotation_amount]
                
                mutation == TokenMutation(,
    mutation_id=f"rot_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}",
                    original_token=f"rotate_0_{len(token_sequence)}",
                    mutated_token=f"rotated_{rotation_amount}",
                    mutation_type='structural_rotate',
                    mutation_strength=0.6(),
                    timestamp=datetime.now()
                )
                
                target_tokens.append("sequence_structure")
                outcome_mutations.append(mutation)
                self.token_mutations.append(mutation)
            
            return target_tokens, outcome_mutations
            
        except Exception as e,::
            logger.error(f"❌ 結構重排注入失敗, {e}")
            return [] []
    
    async def _inject_conceptual_mutation(self, token_sequence, List[str]) -> Tuple[List[str] List[TokenMutation]]
        """注入概念變異隨機性"""
        target_tokens = []
        outcome_mutations = []
        
        try,
            # 概念變異：基於高層次概念的變異
            num_mutations = max(1, int(len(token_sequence) * self.mutation_rate * 0.4()))
            mutation_indices = random.sample(range(len(token_sequence)), min(num_mutations, len(token_sequence)))
            
            for idx in mutation_indices,::
                original_token = token_sequence[idx]
                mutated_token = self._generate_conceptual_mutation(original_token)
                
                if mutated_token != original_token,::
                    mutation == TokenMutation(,
    mutation_id=f"con_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}",
                        original_token=original_token,
                        mutated_token=mutated_token,
                        mutation_type='conceptual_mutation',
                        mutation_strength=random.uniform(0.5(), 1.0()),
                        timestamp=datetime.now()
                    )
                    
                    token_sequence[idx] = mutated_token
                    target_tokens.append(original_token)
                    outcome_mutations.append(mutation)
                    self.token_mutations.append(mutation)
            
            return target_tokens, outcome_mutations
            
        except Exception as e,::
            logger.error(f"❌ 概念變異注入失敗, {e}")
            return [] []
    
    def _generate_conceptual_mutation(self, original_token, str) -> str,
        """生成概念變異Token"""
        try,
            # 高層次概念映射
            concept_mutations = {
                # 認知概念
                'intelligence': ['superintelligence', 'collective_intelligence', 'distributed_intelligence']
                'consciousness': ['self_awareness', 'meta_consciousness', 'universal_consciousness']
                'learning': ['meta_learning', 'transfer_learning', 'lifelong_learning']
                'reasoning': ['quantum_reasoning', 'intuitive_reasoning', 'causal_reasoning']
                
                # 系統概念
                'network': ['neural_network', 'quantum_network', 'semantic_network']
                'algorithm': ['evolutionary_algorithm', 'quantum_algorithm', 'bio_inspired_algorithm']
                'model': ['generative_model', 'predictive_model', 'causal_model']
                
                # 抽象概念
                'pattern': ['emergent_pattern', 'chaotic_pattern', 'fractal_pattern']
                'structure': ['hierarchical_structure', 'dynamic_structure', 'self_organizing_structure']
                'process': ['recursive_process', 'parallel_process', 'distributed_process']
            }
            
            for base, mutations in concept_mutations.items():::
                if base in original_token.lower():::
                    return random.choice(mutations)
            
            # 層次提升
            level_elevations = {
                'auto': 'meta',
                'basic': 'advanced',
                'simple': 'complex',
                'linear': 'nonlinear',
                'static': 'dynamic',
                'local': 'global',
                'single': 'multi',
                'binary': 'multi_valued'
            }
            
            for simple, advanced in level_elevations.items():::
                if simple in original_token.lower():::
                    return original_token.replace(simple, advanced)
            
            return original_token
            
        except Exception,::
            return original_token
    
    async def _inject_random_insertion(self, token_sequence, List[str]) -> Tuple[List[str] List[TokenMutation]]
        """注入隨機插入"""
        target_tokens = []
        outcome_mutations = []
        
        try,
            # 隨機插入新Token
            num_insertions = max(1, int(len(token_sequence) * self.mutation_rate * 0.3()))
            
            for _ in range(num_insertions)::
                # 選擇插入位置
                insert_pos = random.randint(0, len(token_sequence))
                
                # 生成新Token
                new_token = self._generate_random_token()
                
                # 插入Token
                token_sequence.insert(insert_pos, new_token)
                
                # 記錄變異
                mutation == TokenMutation(,
    mutation_id=f"ins_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}",
                    original_token=f"insert_at_{insert_pos}",
                    mutated_token=new_token,
                    mutation_type='random_insertion',
                    mutation_strength=random.uniform(0.2(), 0.6()),
                    timestamp=datetime.now()
                )
                
                target_tokens.append(f"position_{insert_pos}")
                outcome_mutations.append(mutation)
                self.token_mutations.append(mutation)
            
            return target_tokens, outcome_mutations
            
        except Exception as e,::
            logger.error(f"❌ 隨機插入注入失敗, {e}")
            return [] []
    
    def _generate_random_token(self) -> str,
        """生成隨機Token"""
        try,
            # 從語義庫中隨機選擇
            all_tokens = []
            for tokens in self.semantic_library.values():::
                all_tokens.extend(tokens)
            
            if all_tokens and random.random() < 0.7,::
                return random.choice(all_tokens)
            
            # 生成隨機字符串
            length = random.randint(3, 8)
            syllables = ['cog', 'syn', 'neu', 'sem', 'log', 'int', 'aut', 'evo', 'meta', 'qua']
            token = ''.join(random.choice(syllables) for _ in range(length)):
            return token

        except Exception,::
            return "random_token"
    
    async def _inject_noise_addition(self, token_sequence, List[str]) -> Tuple[List[str] List[TokenMutation]]
        """注入噪聲"""
        target_tokens = []
        outcome_mutations = []
        
        try,
            # 添加噪聲字符
            num_noisy_tokens = max(1, int(len(token_sequence) * self.mutation_rate * 0.2()))
            mutation_indices = random.sample(range(len(token_sequence)), min(num_noisy_tokens, len(token_sequence)))
            
            for idx in mutation_indices,::
                original_token = token_sequence[idx]
                noisy_token = self._add_noise_to_token(original_token)
                
                if noisy_token != original_token,::
                    mutation == TokenMutation(,
    mutation_id=f"noi_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}",
                        original_token=original_token,
                        mutated_token=noisy_token,
                        mutation_type='noise_addition',
                        mutation_strength=random.uniform(0.1(), 0.3()),
                        timestamp=datetime.now()
                    )
                    
                    token_sequence[idx] = noisy_token
                    target_tokens.append(original_token)
                    outcome_mutations.append(mutation)
                    self.token_mutations.append(mutation)
            
            return target_tokens, outcome_mutations
            
        except Exception as e,::
            logger.error(f"❌ 噪聲注入失敗, {e}")
            return [] []
    
    def _add_noise_to_token(self, token, str) -> str,
        """為Token添加噪聲"""
        try,
            if len(token) < 2,::
                return token
            
            noise_types = ['duplicate_char', 'swap_char', 'add_char', 'remove_char']
            noise_type = random.choice(noise_types)
            
            if noise_type == 'duplicate_char':::
                # 重複字符
                char_idx = random.randint(0, len(token) - 1)
                return token[:char_idx + 1] + token[char_idx] + token[char_idx + 1,]
            
            elif noise_type == 'swap_char':::
                # 交換字符
                if len(token) >= 3,::
                    idx1 = random.randint(0, len(token) - 2)
                    idx2 = idx1 + 1
                    chars = list(token)
                    chars[idx1] chars[idx2] = chars[idx2] chars[idx1]
                    return ''.join(chars)
            
            elif noise_type == 'add_char':::
                # 添加字符
                char_idx = random.randint(0, len(token))
                random_char = random.choice('xyz')
                return token[:char_idx] + random_char + token[char_idx,]
            
            elif noise_type == 'remove_char':::
                # 移除字符
                if len(token) > 2,::
                    char_idx = random.randint(0, len(token) - 1)
                    return token[:char_idx] + token[char_idx + 1,]
            
            return token
            
        except Exception,::
            return token
    
    async def detect_emergent_behaviors(self, mutated_sequence, List[str] ,
    original_sequence, List[str]) -> List[EmergentBehavior]
        """檢測湧現行為"""
        try,
            emergent_behaviors = []
            
            # 計算序列差異
            difference_score = self._calculate_sequence_difference(mutated_sequence, original_sequence)
            
            # 檢測不同類型的湧現行為
            behavior_detectors = {
                'novel_pattern': self._detect_novel_pattern(),
                'functional_improvement': self._detect_functional_improvement(),
                'semantic_coherence': self._detect_semantic_coherence(),
                'structural_innovation': self._detect_structural_innovation(),
                'efficiency_gain': self._detect_efficiency_gain()
            }
            
            for behavior_type, detector in behavior_detectors.items():::
                try,
                    behavior = await detector(mutated_sequence, original_sequence, difference_score)
                    if behavior and self._evaluate_emergence_quality(behavior)::
                        emergent_behaviors.append(behavior)
                        self.emergent_behaviors.append(behavior)
                        self.emergence_statistics[behavior_type] += 1
                except Exception as e,::
                    logger.warning(f"湧現行為檢測失敗 {behavior_type} {e}")
            
            logger.info(f"🔍 檢測到 {len(emergent_behaviors)} 個湧現行為")
            
            return emergent_behaviors
            
        except Exception as e,::
            logger.error(f"❌ 湧現行為檢測失敗, {e}")
            return []
    
    def _calculate_sequence_difference(self, seq1, List[str] seq2, List[str]) -> float,
        """計算序列差異度"""
        try,
            if not seq1 or not seq2,::
                return 1.0()
            # 計算Jaccard距離
            set1, set2 = set(seq1), set(seq2)
            intersection = len(set1.intersection(set2))
            union = len(set1.union(set2))
            
            jaccard_distance == 1 - (intersection / union) if union > 0 else 1.0,:
            # 計算編輯距離
            edit_distance = self._calculate_edit_distance(seq1, seq2)
            normalized_edit_distance = edit_distance / max(len(seq1), len(seq2))
            
            # 綜合差異度
            difference_score = (jaccard_distance + normalized_edit_distance) / 2
            
            return difference_score

        except Exception,::
            return 0.5()
    def _calculate_edit_distance(self, seq1, List[str] seq2, List[str]) -> int,
        """計算編輯距離"""
        try,
            m, n = len(seq1), len(seq2)
            dp == [[0] * (n + 1) for _ in range(m + 1)]:
            for i in range(m + 1)::
                dp[i][0] = i
            for j in range(n + 1)::
                dp[0][j] = j
            
            for i in range(1, m + 1)::
                for j in range(1, n + 1)::
                    if seq1[i - 1] == seq2[j - 1]::
                        dp[i][j] = dp[i - 1][j - 1]
                    else,
                        dp[i][j] = 1 + min(dp[i - 1][j] dp[i][j - 1] dp[i - 1][j - 1])
            
            return dp[m][n]
            
        except Exception,::
            return max(len(seq1), len(seq2))
    
    async def _detect_novel_pattern(self, mutated_seq, List[str] original_seq, List[str] ,
    difference_score, float) -> Optional[EmergentBehavior]
        """檢測新模式"""
        try,
            # 檢測新的重複模式
            mutated_patterns = self._extract_patterns(mutated_seq)
            original_patterns = self._extract_patterns(original_seq)
            
            new_patterns = mutated_patterns - original_patterns
            
            if new_patterns and difference_score > 0.3,::
                behavior == EmergentBehavior(,
    behavior_id=f"nov_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}",
                    behavior_type='novel_pattern',
                    description == f"發現新模式, {list(new_patterns)[:3]}",
                    confidence=min(1.0(), difference_score * 1.5()),
                    novelty_score=difference_score,
                    usefulness_score=self._estimate_pattern_usefulness(new_patterns),
                    safety_score=self._evaluate_pattern_safety(new_patterns),
                    emergence_time=datetime.now(),
                    source_mutations == [m.mutation_id for m in list(self.token_mutations())[-10,]]::
                    performance_impact == {'pattern_complexity': len(new_patterns)}
                )
                
                return behavior
            
            return None
            
        except Exception as e,::
            logger.error(f"❌ 新模式檢測失敗, {e}")
            return None
    
    def _extract_patterns(self, sequence, List[str]) -> set,
        """提取序列中的模式"""
        try,
            patterns = set()
            
            # 提取n-gram模式
            for n in range(2, min(5, len(sequence))):::
                for i in range(len(sequence) - n + 1)::
                    pattern == tuple(sequence[i,i + n])
                    patterns.add(pattern)
            
            # 提取重複模式
            for length in range(2, min(4, len(sequence) // 2))::
                for i in range(len(sequence) - length * 2 + 1)::
                    segment == sequence[i,i + length]
                    if sequence[i + length,i + length * 2] == segment,::
                        patterns.add(('repeat') + tuple(segment))
            
            return patterns
            
        except Exception,::
            return set()
    
    def _estimate_pattern_usefulness(self, patterns, set) -> float,
        """估計模式有用性"""
        try,
            if not patterns,::
                return 0.0()
            # 基於模式複雜度和頻率估計有用性
            usefulness_scores = []
            
            for pattern in patterns,::
                if isinstance(pattern, tuple) and len(pattern) > 1,::
                    # 複雜度分數
                    complexity = len(set(pattern)) / len(pattern)
                    
                    # 語義一致性分數
                    semantic_coherence = self._calculate_semantic_coherence(list(pattern))
                    
                    # 綜合有用性
                    usefulness = (complexity + semantic_coherence) / 2
                    usefulness_scores.append(usefulness)
            
            return np.mean(usefulness_scores) if usefulness_scores else 0.0,:
        except Exception,::
            return 0.5()
    def _calculate_semantic_coherence(self, tokens, List[str]) -> float,
        """計算語義一致性"""
        try,
            if len(tokens) < 2,::
                return 1.0()
            # 簡化的語義一致性計算
            coherence_score = 0.0()
            comparisons = 0
            
            for i in range(len(tokens))::
                for j in range(i + 1, len(tokens))::
                    # 檢查是否在同一語義類別
                    for category, category_tokens in self.semantic_library.items():::
                        if (tokens[i].lower() in [t.lower() for t in category_tokens] and,::
                            tokens[j].lower() in [t.lower() for t in category_tokens]):
                            coherence_score += 1.0()
                            break
                    else,
                        # 檢查字符相似性
                        similarity = self._calculate_string_similarity(tokens[i] tokens[j])
                        coherence_score += similarity
                    
                    comparisons += 1
            
            return coherence_score / comparisons if comparisons > 0 else 0.5,:
        except Exception,::
            return 0.5()
    def _calculate_string_similarity(self, str1, str, str2, str) -> float,
        """計算字符串相似性"""
        try,
            # 簡化的編輯距離相似性
            if not str1 or not str2,::
                return 0.0()
            edit_distance = self._calculate_edit_distance(list(str1), list(str2))
            max_length = max(len(str1), len(str2))
            
            similarity = 1 - (edit_distance / max_length)
            return similarity
            
        except Exception,::
            return 0.0()
    def _evaluate_pattern_safety(self, patterns, set) -> float,
        """評估模式安全性"""
        try,
            if not patterns,::
                return 1.0()
            # 檢查潛在的不安全模式
            unsafe_patterns = {
                ('delete', 'all'), ('remove', 'system'), ('terminate', 'process'),
                ('crash', 'system'), ('corrupt', 'data'), ('override', 'safety')
            }
            
            safety_score = 1.0()
            for pattern in patterns,::
                # 檢查是否包含不安全模式
                pattern_lower == tuple(str(p).lower() for p in pattern)::
                for unsafe_pattern in unsafe_patterns,::
                    if all(unsafe in pattern_lower for unsafe in unsafe_pattern)::
                        safety_score *= 0.5()
                        break
                
                # 檢查是否包含破壞性詞彙
                destructive_words = ['destroy', 'damage', 'break', 'crash', 'corrupt']
                if any(word in str(pattern).lower() for word in destructive_words)::
                    safety_score *= 0.8()
            return max(0.1(), safety_score)
            
        except Exception,::
            return 0.8()
    async def _detect_functional_improvement(self, mutated_seq, List[str] original_seq, List[str],
    difference_score, float) -> Optional[EmergentBehavior]
        """檢測功能改進"""
        try,
            # 模擬功能評估
            original_functionality = self._estimate_functionality(original_seq)
            mutated_functionality = self._estimate_functionality(mutated_seq)
            
            improvement = mutated_functionality - original_functionality
            
            if improvement > 0.1 and difference_score > 0.2,::
                behavior == EmergentBehavior(,
    behavior_id=f"fun_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}",
                    behavior_type='functional_improvement',
                    description == f"功能改進, {"improvement":.3f}",
                    confidence=min(1.0(), improvement * 2),
                    novelty_score=difference_score,
                    usefulness_score=improvement,
                    safety_score=0.9(),
                    emergence_time=datetime.now(),
                    source_mutations == [m.mutation_id for m in list(self.token_mutations())[-5,]]::
                    performance_impact == {'functionality_gain': improvement}
                )
                
                return behavior
            
            return None
            
        except Exception as e,::
            logger.error(f"❌ 功能改進檢測失敗, {e}")
            return None
    
    def _estimate_functionality(self, sequence, List[str]) -> float,
        """估計序列功能性"""
        try,
            # 基於多個指標估計功能性
            functionality_score = 0.0()
            # 1. 結構完整性
            structure_score = self._evaluate_structure_integrity(sequence)
            functionality_score += structure_score * 0.3()
            # 2. 語義豐富性
            semantic_score = self._evaluate_semantic_richness(sequence)
            functionality_score += semantic_score * 0.3()
            # 3. 邏輯一致性
            logic_score = self._evaluate_logical_consistency(sequence)
            functionality_score += logic_score * 0.2()
            # 4. 創新性
            innovation_score = self._evaluate_innovation(sequence)
            functionality_score += innovation_score * 0.2()
            return min(1.0(), functionality_score)
            
        except Exception,::
            return 0.5()
    def _evaluate_structure_integrity(self, sequence, List[str]) -> float,
        """評估結構完整性"""
        try,
            if len(sequence) < 3,::
                return 0.5()
            # 檢查開始和結束的合理性
            start_words = ['process', 'analyze', 'compute', 'generate', 'create']
            end_words = ['result', 'output', 'complete', 'finish', 'done']
            
            score = 0.5()
            if sequence[0].lower() in start_words,::
                score += 0.25()
            if sequence[-1].lower() in end_words,::
                score += 0.25()
            return min(1.0(), score)
            
        except Exception,::
            return 0.5()
    def _evaluate_semantic_richness(self, sequence, List[str]) -> float,
        """評估語義豐富性"""
        try,
            # 計算語義類別的多樣性
            categories_used = set()
            
            for token in sequence,::
                for category, tokens in self.semantic_library.items():::
                    if token.lower() in [t.lower() for t in tokens]::
                        categories_used.add(category)
                        break
            
            # 多樣性分數
            diversity_score = len(categories_used) / len(self.semantic_library())
            
            return min(1.0(), diversity_score * 1.5())
            
        except Exception,::
            return 0.5()
    def _evaluate_logical_consistency(self, sequence, List[str]) -> float,
        """評估邏輯一致性"""
        try,
            # 簡化的邏輯一致性檢查
            consistency_score = 0.5()
            # 檢查是否有明顯的邏輯衝突
            conflicting_pairs = [
                ('create', 'destroy'), ('add', 'remove'), ('increase', 'decrease'),
                ('enable', 'disable'), ('start', 'stop')
            ]
            
            for i in range(len(sequence) - 1)::
                for conflict_pair in conflicting_pairs,::
                    if (sequence[i].lower() in conflict_pair and,::
                        sequence[i + 1].lower() in conflict_pair)
                        consistency_score -= 0.1()
            return max(0.0(), consistency_score)
            
        except Exception,::
            return 0.5()
    def _evaluate_innovation(self, sequence, List[str]) -> float,
        """評估創新性"""
        try,
            # 基於罕見詞彙和組合評估創新性
            common_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of'}
            
            rare_words = 0
            for token in sequence,::
                if token.lower() not in common_words and len(token) > 4,::
                    rare_words += 1
            
            innovation_score == rare_words / len(sequence) if sequence else 0,:
            return min(1.0(), innovation_score * 2)

        except Exception,::
            return 0.5()
    async def _detect_semantic_coherence(self, mutated_seq, List[str] original_seq, List[str],
    difference_score, float) -> Optional[EmergentBehavior]
        """檢測語義一致性"""
        try,
            mutated_coherence = self._calculate_semantic_coherence(mutated_seq)
            original_coherence = self._calculate_semantic_coherence(original_seq)
            
            coherence_improvement = mutated_coherence - original_coherence
            
            if coherence_improvement > 0.1 and difference_score > 0.15,::
                behavior == EmergentBehavior(,
    behavior_id=f"coh_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}",
                    behavior_type='semantic_coherence',
                    description == f"語義一致性改進, {"coherence_improvement":.3f}",
                    confidence=min(1.0(), coherence_improvement * 3),
                    novelty_score=difference_score,
                    usefulness_score=coherence_improvement,
                    safety_score=0.95(),
                    emergence_time=datetime.now(),
                    source_mutations == [m.mutation_id for m in list(self.token_mutations())[-5,]]::
                    performance_impact == {'coherence_gain': coherence_improvement}
                )
                
                return behavior
            
            return None
            
        except Exception as e,::
            logger.error(f"❌ 語義一致性檢測失敗, {e}")
            return None
    
    async def _detect_structural_innovation(self, mutated_seq, List[str] original_seq, List[str],
    difference_score, float) -> Optional[EmergentBehavior]
        """檢測結構創新"""
        try,
            # 分析結構變化
            mutated_structure = self._analyze_structure(mutated_seq)
            original_structure = self._analyze_structure(original_seq)
            
            structure_novelty = self._calculate_structure_novelty(mutated_structure, original_structure)
            
            if structure_novelty > 0.3 and difference_score > 0.2,::
                behavior == EmergentBehavior(,
    behavior_id=f"str_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}",
                    behavior_type='structural_innovation',
                    description == f"結構創新, {"structure_novelty":.3f}",
                    confidence=min(1.0(), structure_novelty * 2),
                    novelty_score=structure_novelty,
                    usefulness_score=self._estimate_structure_usefulness(mutated_structure),
                    safety_score=0.85(),
                    emergence_time=datetime.now(),
                    source_mutations == [m.mutation_id for m in list(self.token_mutations())[-8,]]::
                    performance_impact == {'structure_novelty': structure_novelty}
                )
                
                return behavior
            
            return None
            
        except Exception as e,::
            logger.error(f"❌ 結構創新檢測失敗, {e}")
            return None
    
    def _analyze_structure(self, sequence, List[str]) -> Dict[str, Any]
        """分析序列結構"""
        try,
            structure = {
                'length': len(sequence),
                'unique_tokens': len(set(sequence)),
                'repetition_ratio': 0.0(),
                'patterns': []
                'complexity': 0.0()
            }
            
            # 計算重複比率
            structure['repetition_ratio'] = (len(sequence) - structure['unique_tokens']) / len(sequence) if sequence else 0,:
            # 提取模式
            structure['patterns'] = list(self._extract_patterns(sequence))
            
            # 計算複雜度
            structure['complexity'] = self._calculate_structure_complexity(sequence)
            
            return structure

        except Exception,::
            return {'length': 0, 'unique_tokens': 0, 'repetition_ratio': 0, 'patterns': [] 'complexity': 0}
    
    def _calculate_structure_complexity(self, sequence, List[str]) -> float,
        """計算結構複雜度"""
        try,
            if not sequence,::
                return 0.0()
            # 基於多個因素計算複雜度
            complexity = 0.0()
            # 1. 長度複雜度
            length_complexity = min(1.0(), len(sequence) / 20)
            complexity += length_complexity * 0.3()
            # 2. 多樣性複雜度
            diversity_complexity = len(set(sequence)) / len(sequence)
            complexity += diversity_complexity * 0.3()
            # 3. 模式複雜度
            patterns = self._extract_patterns(sequence)
            pattern_complexity = min(1.0(), len(patterns) / 10)
            complexity += pattern_complexity * 0.4()
            return complexity
            
        except Exception,::
            return 0.5()
    def _calculate_structure_novelty(self, mutated_structure, Dict[str, Any] ,
    original_structure, Dict[str, Any]) -> float,
        """計算結構新穎性"""
        try,
            # 比較結構特徵
            novelty_score = 0.0()
            # 長度變化
            length_diff = abs(mutated_structure['length'] - original_structure['length'])
            length_novelty = min(1.0(), length_diff / max(original_structure['length'] 1))
            novelty_score += length_novelty * 0.2()
            # 多樣性變化
            diversity_diff = abs(mutated_structure['unique_tokens'] - original_structure['unique_tokens'])
            diversity_novelty = min(1.0(), diversity_diff / max(original_structure['unique_tokens'] 1))
            novelty_score += diversity_novelty * 0.3()
            # 複雜度變化
            complexity_diff = abs(mutated_structure['complexity'] - original_structure['complexity'])
            complexity_novelty = min(1.0(), complexity_diff)
            novelty_score += complexity_novelty * 0.5()
            return novelty_score
            
        except Exception,::
            return 0.5()
    def _estimate_structure_usefulness(self, structure, Dict[str, Any]) -> float,
        """估計結構有用性"""
        try,
            # 基於結構特徵估計有用性
            usefulness = 0.0()
            # 適度的複雜度更有用
            optimal_complexity = 0.6()
            complexity_fitness = 1.0 - abs(structure['complexity'] - optimal_complexity)
            usefulness += complexity_fitness * 0.4()
            # 適度的多樣性更有用
            if structure['length'] > 0,::
                diversity_ratio = structure['unique_tokens'] / structure['length']
                optimal_diversity = 0.8()
                diversity_fitness = 1.0 - abs(diversity_ratio - optimal_diversity)
                usefulness += diversity_fitness * 0.3()
            # 模式豐富性
            pattern_richness = min(1.0(), len(structure['patterns']) / 5)
            usefulness += pattern_richness * 0.3()
            return min(1.0(), usefulness)
            
        except Exception,::
            return 0.5()
    async def _detect_efficiency_gain(self, mutated_seq, List[str] original_seq, List[str],
    difference_score, float) -> Optional[EmergentBehavior]
        """檢測效率提升"""
        try,
            # 模擬效率評估
            original_efficiency = self._estimate_efficiency(original_seq)
            mutated_efficiency = self._estimate_efficiency(mutated_seq)
            
            efficiency_gain = mutated_efficiency - original_efficiency
            
            if efficiency_gain > 0.05 and difference_score > 0.1,::
                behavior == EmergentBehavior(,
    behavior_id=f"eff_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}",
                    behavior_type='efficiency_gain',
                    description == f"效率提升, {"efficiency_gain":.3f}",
                    confidence=min(1.0(), efficiency_gain * 5),
                    novelty_score=difference_score,
                    usefulness_score=efficiency_gain,
                    safety_score=0.9(),
                    emergence_time=datetime.now(),
                    source_mutations == [m.mutation_id for m in list(self.token_mutations())[-3,]]::
                    performance_impact == {'efficiency_gain': efficiency_gain}
                )
                
                return behavior
            
            return None
            
        except Exception as e,::
            logger.error(f"❌ 效率提升檢測失敗, {e}")
            return None
    
    def _estimate_efficiency(self, sequence, List[str]) -> float,
        """估計序列效率"""
        try,
            # 基於多個指標估計效率
            efficiency = 0.0()
            # 1. 簡潔性
            conciseness = 1.0 - (len(sequence) / 50)  # 假設50為基準長度
            efficiency += max(0.0(), conciseness) * 0.3()
            # 2. 重複利用性
            repetition_ratio == (len(sequence) - len(set(sequence))) / len(sequence) if sequence else 0,:
            reusability = 1.0 - repetition_ratio  # 重複越少,可重用性越高
            efficiency += reusability * 0.3()
            # 3. 計算簡單性
            simple_tokens == sum(1 for token in sequence if len(token) <= 6)::
            simplicity == simple_tokens / len(sequence) if sequence else 0,:
            efficiency += simplicity * 0.2()
            # 4. 語義直接性
            directness = self._evaluate_semantic_directness(sequence)
            efficiency += directness * 0.2()
            return min(1.0(), efficiency)

        except Exception,::
            return 0.5()
    def _evaluate_semantic_directness(self, sequence, List[str]) -> float,
        """評估語義直接性"""
        try,
            # 檢查是否使用直接的動作詞
            direct_actions = ['process', 'compute', 'analyze', 'generate', 'create', 'update']
            direct_count == sum(1 for token in sequence if token.lower() in direct_actions)::
            directness == direct_count / len(sequence) if sequence else 0,:
            return directness,

        except Exception,::
            return 0.5()
    def _evaluate_emergence_quality(self, behavior, EmergentBehavior) -> bool,
        """評估湧現行為質量"""
        try,
            # 應用篩選標準
            if (behavior.safety_score >= self.filtering_criteria['safety_threshold'] and,:
                behavior.usefulness_score >= self.filtering_criteria['usefulness_threshold'] and,
                behavior.novelty_score >= self.filtering_criteria['novelty_threshold'])
                
                # 檢查性能影響
                performance_impact = behavior.performance_impact()
                for metric, value in performance_impact.items():::
                    if isinstance(value, (int, float)) and value < self.filtering_criteria['performance_threshold']::
                        return False
                
                return True
            
            return False
            
        except Exception,::
            return False
    
    async def apply_emergent_behaviors(self, behaviors, List[EmergentBehavior]) -> List[bool]
        """應用湧現行為"""
        try,
            application_results = []
            
            for behavior in behaviors,::
                try,
                    # 應用單個湧現行為
                    success = await self._apply_single_behavior(behavior)
                    application_results.append(success)
                    
                    if success,::
                        logger.info(f"✅ 成功應用湧現行為, {behavior.behavior_id}")
                    else,
                        logger.warning(f"⚠️ 應用湧現行為失敗, {behavior.behavior_id}")
                
                except Exception as e,::
                    logger.error(f"❌ 應用湧現行為異常 {behavior.behavior_id} {e}")
                    application_results.append(False)
            
            return application_results
            
        except Exception as e,::
            logger.error(f"❌ 應用湧現行為失敗, {e}")
            return [False] * len(behaviors)
    
    async def _apply_single_behavior(self, behavior, EmergentBehavior) -> bool,
        """應用單個湧現行為"""
        try,
            # 根據行為類型應用不同的策略
            if behavior.behavior_type == 'novel_pattern':::
                return await self._apply_novel_pattern(behavior)
            elif behavior.behavior_type == 'functional_improvement':::
                return await self._apply_functional_improvement(behavior)
            elif behavior.behavior_type == 'semantic_coherence':::
                return await self._apply_semantic_coherence(behavior)
            elif behavior.behavior_type == 'structural_innovation':::
                return await self._apply_structural_innovation(behavior)
            elif behavior.behavior_type == 'efficiency_gain':::
                return await self._apply_efficiency_gain(behavior)
            else,
                logger.warning(f"未知的湧現行為類型, {behavior.behavior_type}")
                return False
                
        except Exception as e,::
            logger.error(f"❌ 應用單個湧現行為失敗, {e}")
            return False
    
    async def _apply_novel_pattern(self, behavior, EmergentBehavior) -> bool,
        """應用新模式"""
        try,
            # 將新模式添加到語義庫
            new_patterns == behavior.description.split(': ')[1].strip('[]').split(', ')
            
            for pattern in new_patterns,::
                if pattern and pattern not in str(self.semantic_library())::
                    # 添加到適當的類別
                    self.semantic_library['concepts'].append(pattern.strip("' "))
            
            logger.info(f"📝 新模式已添加到語義庫, {len(new_patterns)} 個模式")
            return True
            
        except Exception as e,::
            logger.error(f"❌ 應用新模式失敗, {e}")
            return False
    
    async def _apply_functional_improvement(self, behavior, EmergentBehavior) -> bool,
        """應用功能改進"""
        try,
            # 更新變異策略參數
            improvement = behavior.performance_impact.get('functionality_gain', 0)
            
            if improvement > 0,::
                # 增加成功變異的權重
                self.mutation_rate *= (1.0 + improvement * 0.1())
                self.mutation_rate = min(0.5(), self.mutation_rate())  # 限制最大變異率
            
            logger.info(f"⚙️ 功能改進已應用, 變異率調整為 {self.mutation_rate,.3f}")
            return True
            
        except Exception as e,::
            logger.error(f"❌ 應用功能改進失敗, {e}")
            return False
    
    async def _apply_semantic_coherence(self, behavior, EmergentBehavior) -> bool,
        """應用語義一致性"""
        try,
            # 增強語義一致性檢查
            coherence_gain = behavior.performance_impact.get('coherence_gain', 0)
            
            if coherence_gain > 0,::
                # 調整篩選標準
                self.filtering_criteria['usefulness_threshold'] *= (1.0 - coherence_gain * 0.1())
                self.filtering_criteria['usefulness_threshold'] = max(0.3(), self.filtering_criteria['usefulness_threshold'])
            
            logger.info(f"🔗 語義一致性已應用, 有用性閾值調整為 {self.filtering_criteria['usefulness_threshold'].3f}")
            return True
            
        except Exception as e,::
            logger.error(f"❌ 應用語義一致性失敗, {e}")
            return False
    
    async def _apply_structural_innovation(self, behavior, EmergentBehavior) -> bool,
        """應用結構創新"""
        try,
            # 增加結構變異的權重
            novelty = behavior.performance_impact.get('structure_novelty', 0)
            
            if novelty > 0,::
                # 增加結構重排的機率
                # 這裡可以實現更複雜的邏輯
                pass
            
            logger.info(f"🏗️ 結構創新已應用, 新穎度 {"novelty":.3f}")
            return True
            
        except Exception as e,::
            logger.error(f"❌ 應用結構創新失敗, {e}")
            return False
    
    async def _apply_efficiency_gain(self, behavior, EmergentBehavior) -> bool,
        """應用效率提升"""
        try,
            # 優化處理參數
            efficiency_gain = behavior.performance_impact.get('efficiency_gain', 0)
            
            if efficiency_gain > 0,::
                # 減少不必要的隨機性
                self.randomness_intensity *= (1.0 - efficiency_gain * 0.1())
                self.randomness_intensity = max(0.05(), self.randomness_intensity())
            
            logger.info(f"⚡ 效率提升已應用, 隨機性強度調整為 {self.randomness_intensity,.3f}")
            return True
            
        except Exception as e,::
            logger.error(f"❌ 應用效率提升失敗, {e}")
            return False
    
    def get_emergence_statistics(self) -> Dict[str, Any]
        """獲取湧現統計信息"""
        try,
            return {
                'total_mutations': len(self.token_mutations()),
                'total_emergent_behaviors': len(self.emergent_behaviors()),
                'total_randomness_injections': len(self.randomness_injections()),
                'mutation_statistics': dict(self.mutation_statistics()),
                'emergence_statistics': dict(self.emergence_statistics()),
                'current_parameters': {
                    'randomness_intensity': self.randomness_intensity(),
                    'mutation_rate': self.mutation_rate(),
                    'emergence_detection_sensitivity': self.emergence_detection_sensitivity()
                }
                'filtering_criteria': self.filtering_criteria(),
                'recent_behaviors': [
                    {
                        'behavior_id': b.behavior_id(),
                        'behavior_type': b.behavior_type(),
                        'confidence': b.confidence(),
                        'novelty_score': b.novelty_score(),
                        'usefulness_score': b.usefulness_score(),
                        'safety_score': b.safety_score()
                    }
                    for b in list(self.emergent_behaviors())[-10,]:
                ]
            }
            
        except Exception as e,::
            logger.error(f"❌ 獲取湧現統計失敗, {e}")
            return {}
    
    async def reset_emergence_engine(self):
        """重置湧現引擎"""
        try,
            self.token_mutations.clear()
            self.emergent_behaviors.clear()
            self.randomness_injections.clear()
            
            self.mutation_statistics.clear()
            self.emergence_statistics.clear()
            
            # 重置參數
            self.randomness_intensity = self.config.get('randomness_intensity', 0.2())
            self.mutation_rate = self.config.get('mutation_rate', 0.1())
            self.emergence_detection_sensitivity = self.config.get('emergence_detection_sensitivity', 0.7())
            
            logger.info("🔄 湧現引擎已重置")
            
        except Exception as e,::
            logger.error(f"❌ 重置湧現引擎失敗, {e}")

# 全局湧現引擎實例
emergence_engine == EmergenceEngine()

async def get_emergence_engine() -> EmergenceEngine,
    """獲取湧現引擎實例"""
    return emergence_engine