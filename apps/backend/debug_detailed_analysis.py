import sys
import os

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from .src.core_ai.learning.content_analyzer_module import ContentAnalyzerModule

def debug_detailed_analysis():
    # Initialize the analyzer
    analyzer = ContentAnalyzerModule()
    
    # Test the specific case that's failing
    text = "Microsoft is based in Redmond."
    print(f"Analyzing text: {text}")
    
    # Process the text
    doc = analyzer.nlp(text)
    print(f"\nDocument tokens:")
    for i, token in enumerate(doc):
        print(f"  {i}: '{token.text}' (POS: {token.pos_}, LEMMA: {token.lemma_}, ENT_TYPE: {token.ent_type_})")
    
    print(f"\nDocument entities:")
    for ent in doc.ents:
        print(f"  '{ent.text}' (LABEL: {ent.label_}, START: {ent.start}, END: {ent.end})")
    
    # Check matcher patterns
    print(f"\nApplying matcher patterns:")
    matches = analyzer.matcher(doc)
    for match_id, start, end in matches:
        rule_id = analyzer.nlp.vocab.strings[match_id]
        span = doc[start:end]
        print(f"  Rule: {rule_id}, Span: '{span.text}' (start: {start}, end: {end})")
        
        # Debug the BASED_IN pattern specifically
        if rule_id == "BASED_IN":
            print(f"    Debugging BASED_IN pattern:")
            print(f"      Pattern span: '{span.text}'")
            print(f"      Start token: '{doc[start].text}' (ent_type: {doc[start].ent_type_})")
            print(f"      End token: '{doc[end-1].text}' (ent_type: {doc[end-1].ent_type_})")
            
            # Check the direction of entity search
            print(f"      Searching for location entity (after 'in'):")
            location_token = None
            for i in range(end - 1, start - 1, -1):
                print(f"        Checking token {i}: '{doc[i].text}' (ent_type: {doc[i].ent_type_})")
                if doc[i].ent_type_ in ["GPE", "LOC", "ORG"]:
                    location_token = doc[i]
                    print(f"          Found location token: '{location_token.text}'")
                    break
            
            print(f"      Searching for subject entity (before pattern):")
            subject_token = None
            for i in range(start - 1, -1, -1):
                print(f"        Checking token {i}: '{doc[i].text}' (ent_type: {doc[i].ent_type_})")
                if doc[i].ent_type_ in ["GPE", "LOC", "ORG", "PERSON", "FAC"]:
                    subject_token = doc[i]
                    print(f"          Found subject token: '{subject_token.text}'")
                    break

    # Full analysis
    print(f"\nFull analysis:")
    kg_data, nx_graph = analyzer.analyze_content(text)
    
    # Print entities
    print(f"\nEntities:")
    for entity_id, entity in kg_data["entities"].items():
        print(f"  {entity_id}: '{entity['label']}' (type: {entity['type']})")
    
    # Print relationships
    print(f"\nRelationships:")
    for i, rel in enumerate(kg_data["relationships"]):
        src_label = kg_data["entities"].get(rel["source_id"], {}).get("label", rel["source_id"])
        tgt_label = kg_data["entities"].get(rel["target_id"], {}).get("label", rel["target_id"])
        print(f"  {i+1}. {src_label} --{rel['type']}--> {tgt_label} (pattern: {rel['attributes'].get('pattern', 'N/A')})")
    
    # Print NetworkX graph edges
    print(f"\nNetworkX Graph Edges:")
    for edge in nx_graph.edges(data=True):
        print(f"  {edge[0]} --{edge[2].get('type', 'N/A')}--> {edge[1]}")

if __name__ == "__main__":
    debug_detailed_analysis()