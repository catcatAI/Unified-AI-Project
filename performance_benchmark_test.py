#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•
è¯„ä¼°AGIç³»ç»Ÿçš„æ€§èƒ½æŒ‡æ ‡å’ŒåŸºå‡†
"""

import asyncio
import time
import json
import psutil
import threading
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
sys.path.append('apps/backend/src')
sys.path.append('.')

from unified_scheduler_framework import (
    create_unified_scheduler, create_parallel_scheduler, 
    create_pipeline_scheduler, TaskConfig
)
from enhanced_input_validator import create_smart_validator
from enhanced_output_validator import create_enhanced_output_validator


class PerformanceBenchmarkTester:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.benchmark_results = []
        self.system_metrics = []
        self.monitoring_active = False
        self.monitoring_thread = None
        
    def start_system_monitoring(self):
        """å¯åŠ¨ç³»ç»Ÿç›‘æ§"""
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._monitor_system_resources)
        self.monitoring_thread.daemon = True
        self.monitoring_thread.start()
    
    def stop_system_monitoring(self):
        """åœæ­¢ç³»ç»Ÿç›‘æ§"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=2)
    
    def _monitor_system_resources(self):
        """ç›‘æ§ç³»ç»Ÿèµ„æº"""
        while self.monitoring_active:
            try:
                # CPUä½¿ç”¨ç‡
                cpu_percent = psutil.cpu_percent(interval=0.1)
                
                # å†…å­˜ä½¿ç”¨
                memory = psutil.virtual_memory()
                memory_percent = memory.percent
                memory_used_mb = memory.used / 1024 / 1024
                
                # ç£ç›˜IO
                disk_io = psutil.disk_io_counters()
                disk_read_mb = disk_io.read_bytes / 1024 / 1024 if disk_io else 0
                disk_write_mb = disk_io.write_bytes / 1024 / 1024 if disk_io else 0
                
                # ç½‘ç»œIO
                net_io = psutil.net_io_counters()
                net_sent_mb = net_io.bytes_sent / 1024 / 1024 if net_io else 0
                net_recv_mb = net_io.bytes_recv / 1024 / 1024 if net_io else 0
                
                self.system_metrics.append({
                    "timestamp": time.time(),
                    "cpu_percent": cpu_percent,
                    "memory_percent": memory_percent,
                    "memory_used_mb": memory_used_mb,
                    "disk_read_mb": disk_read_mb,
                    "disk_write_mb": disk_write_mb,
                    "net_sent_mb": net_sent_mb,
                    "net_recv_mb": net_recv_mb
                })
                
                time.sleep(0.5)  # æ¯0.5ç§’é‡‡æ ·ä¸€æ¬¡
                
            except Exception as e:
                print(f"ç›‘æ§çº¿ç¨‹é”™è¯¯: {e}")
                time.sleep(1)
    
    async def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹AGIç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        benchmark_start = time.time()
        results = {
            "test_name": "AGIç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•",
            "start_time": datetime.now().isoformat(),
            "benchmarks": {},
            "system_metrics": {},
            "overall_score": 0.0
        }
        
        try:
            # 1. å“åº”æ—¶é—´åŸºå‡†æµ‹è¯•
            print("\nğŸ“Š 1. å“åº”æ—¶é—´åŸºå‡†æµ‹è¯•")
            results["benchmarks"]["response_time"] = await self._benchmark_response_time()
            
            # 2. ååé‡åŸºå‡†æµ‹è¯•
            print("\nğŸ“ˆ 2. ååé‡åŸºå‡†æµ‹è¯•")
            results["benchmarks"]["throughput"] = await self._benchmark_throughput()
            
            # 3. å¹¶å‘æ€§èƒ½åŸºå‡†æµ‹è¯•
            print("\nâš¡ 3. å¹¶å‘æ€§èƒ½åŸºå‡†æµ‹è¯•")
            results["benchmarks"]["concurrency"] = await self._benchmark_concurrency()
            
            # 4. å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•
            print("\nğŸ’¾ 4. å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•")
            results["benchmarks"]["memory_usage"] = await self._benchmark_memory_usage()
            
            # 5. å¤„ç†å»¶è¿ŸåŸºå‡†æµ‹è¯•
            print("\nâ±ï¸ 5. å¤„ç†å»¶è¿ŸåŸºå‡†æµ‹è¯•")
            results["benchmarks"]["processing_latency"] = await self._benchmark_processing_latency()
            
            # 6. ç³»ç»Ÿç¨³å®šæ€§åŸºå‡†æµ‹è¯•
            print("\nğŸ›¡ï¸ 6. ç³»ç»Ÿç¨³å®šæ€§åŸºå‡†æµ‹è¯•")
            results["benchmarks"]["stability"] = await self._benchmark_stability()
            
            # è®¡ç®—æ€»ä½“è¯„åˆ†
            benchmark_end = time.time()
            results["total_execution_time"] = benchmark_end - benchmark_start
            results["overall_score"] = self._calculate_overall_score(results["benchmarks"])
            
            # åˆ†æç³»ç»ŸæŒ‡æ ‡
            results["system_metrics"] = self._analyze_system_metrics()
            
            print(f"\nâœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
            print(f"æ€»æ‰§è¡Œæ—¶é—´: {results['total_execution_time']:.2f}ç§’")
            print(f"æ€»ä½“æ€§èƒ½è¯„åˆ†: {results['overall_score']:.1f}/10.0")
            
            return results
            
        except Exception as e:
            print(f"\nâŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            results["error"] = str(e)
            return results
    
    async def _benchmark_response_time(self) -> Dict[str, Any]:
        """å“åº”æ—¶é—´åŸºå‡†æµ‹è¯•"""
        print("  æµ‹è¯•ä¸åŒå¤æ‚åº¦ä»»åŠ¡çš„å“åº”æ—¶é—´...")
        
        test_cases = [
            {
                "name": "ç®€å•ä»»åŠ¡",
                "command": "python -c \"print('ç®€å•ä»»åŠ¡å®Œæˆ')\"",
                "timeout": 5
            },
            {
                "name": "ä¸­ç­‰å¤æ‚åº¦ä»»åŠ¡",
                "command": "python -c \"import time; time.sleep(0.1); print('ä¸­ç­‰ä»»åŠ¡å®Œæˆ')\"",
                "timeout": 10
            },
            {
                "name": "å¤æ‚ä»»åŠ¡",
                "command": "python -c \"import time; [time.sleep(0.05) for _ in range(3)]; print('å¤æ‚ä»»åŠ¡å®Œæˆ')\"",
                "timeout": 15
            }
        ]
        
        response_times = []
        scheduler = create_unified_scheduler()
        
        for test_case in test_cases:
            times = []
            
            # è¿è¡Œå¤šæ¬¡å–å¹³å‡å€¼
            for i in range(5):
                task_config = TaskConfig(
                    name=f"response_test_{test_case['name']}_{i}",
                    command=test_case["command"],
                    timeout=test_case["timeout"]
                )
                scheduler.register_task(task_config)
                
                start_time = time.time()
                result = await scheduler.execute_task(task_config.name)
                end_time = time.time()
                
                if result.status.value == "completed":
                    response_time = end_time - start_time
                    times.append(response_time)
            
            if times:
                avg_time = sum(times) / len(times)
                max_time = max(times)
                min_time = min(times)
                
                response_times.append({
                    "task_complexity": test_case["name"],
                    "average_response_time": avg_time,
                    "max_response_time": max_time,
                    "min_response_time": min_time,
                    "test_count": len(times)
                })
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        if response_times:
            overall_avg = sum(rt["average_response_time"] for rt in response_times) / len(response_times)
            performance_score = max(0, 10 - overall_avg)  # å“åº”æ—¶é—´è¶ŠçŸ­ï¼Œåˆ†æ•°è¶Šé«˜
        else:
            overall_avg = 0
            performance_score = 0
        
        return {
            "success": len(response_times) > 0,
            "response_times": response_times,
            "overall_average_response_time": overall_avg,
            "performance_score": performance_score,
            "grade": self._get_performance_grade(performance_score)
        }
    
    async def _benchmark_throughput(self) -> Dict[str, Any]:
        """ååé‡åŸºå‡†æµ‹è¯•"""
        print("  æµ‹è¯•ç³»ç»Ÿå¤„ç†å¹¶å‘è¯·æ±‚çš„èƒ½åŠ›...")
        
        # åˆ›å»ºå¹¶è¡Œè°ƒåº¦å™¨
        parallel_scheduler = create_parallel_scheduler(max_concurrent_tasks=5)
        
        # åˆ›å»ºå¤šä¸ªç®€å•ä»»åŠ¡
        task_configs = []
        for i in range(10):
            task_config = TaskConfig(
                name=f"throughput_task_{i}",
                command=f"python -c \"print('Task {i}'); import time; time.sleep(0.1)\"",
                timeout=10
            )
            parallel_scheduler.register_task(task_config)
            task_configs.append(task_config)
        
        # å¯åŠ¨ç³»ç»Ÿç›‘æ§
        self.start_system_monitoring()
        self.system_metrics = []  # é‡ç½®ç›‘æ§æ•°æ®
        
        start_time = time.time()
        
        # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        task_names = [tc.name for tc in task_configs]
        results = await parallel_scheduler.execute_tasks(task_names)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # åœæ­¢ç›‘æ§
        self.stop_system_monitoring()
        
        # åˆ†æç»“æœ
        successful_tasks = sum(1 for r in results if r.status.value == "completed")
        total_tasks = len(results)
        throughput = successful_tasks / total_time if total_time > 0 else 0
        
        # è®¡ç®—å¹¶å‘æ•ˆç‡
        theoretical_max = len(task_configs) / 0.1  # æ¯ä¸ªä»»åŠ¡ç†è®ºè€—æ—¶0.1ç§’
        efficiency = (throughput / theoretical_max) * 100 if theoretical_max > 0 else 0
        
        # è¯„åˆ†
        performance_score = min(10, throughput * 2)  # æ¯ä»»åŠ¡/ç§’å¾—2åˆ†ï¼Œæœ€é«˜10åˆ†
        
        return {
            "success": successful_tasks > 0,
            "total_tasks": total_tasks,
            "successful_tasks": successful_tasks,
            "total_execution_time": total_time,
            "throughput_per_second": throughput,
            "concurrent_efficiency": efficiency,
            "performance_score": performance_score,
            "grade": self._get_performance_grade(performance_score)
        }
    
    async def _benchmark_concurrency(self) -> Dict[str, Any]:
        """å¹¶å‘æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("  æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«ä¸‹çš„ç³»ç»Ÿè¡¨ç°...")
        
        concurrency_levels = [1, 2, 4, 8]
        concurrency_results = []
        
        for level in concurrency_levels:
            print(f"    æµ‹è¯•å¹¶å‘çº§åˆ«: {level}")
            
            scheduler = create_parallel_scheduler(max_concurrent_tasks=level)
            
            # åˆ›å»ºä»»åŠ¡
            task_configs = []
            for i in range(level * 2):  # æ¯ä¸ªå¹¶å‘çº§åˆ«åˆ›å»º2å€ä»»åŠ¡
                task_config = TaskConfig(
                    name=f"concurrency_task_{level}_{i}",
                    command=f"python -c \"print('Task {level}_{i}'); import time; time.sleep(0.2)\"",
                    timeout=15
                )
                scheduler.register_task(task_config)
                task_configs.append(task_config)
            
            start_time = time.time()
            task_names = [tc.name for tc in task_configs]
            results = await scheduler.execute_tasks(task_names)
            end_time = time.time()
            
            successful_tasks = sum(1 for r in results if r.status.value == "completed")
            total_time = end_time - start_time
            
            concurrency_results.append({
                "concurrency_level": level,
                "total_tasks": len(task_configs),
                "successful_tasks": successful_tasks,
                "total_time": total_time,
                "tasks_per_second": successful_tasks / total_time if total_time > 0 else 0,
                "efficiency": (successful_tasks / len(task_configs)) * 100
            })
        
        # åˆ†æå¹¶å‘æ‰©å±•æ€§
        if len(concurrency_results) > 1:
            scalability_score = self._calculate_scalability_score(concurrency_results)
        else:
            scalability_score = 0
        
        return {
            "success": len(concurrency_results) > 0,
            "concurrency_results": concurrency_results,
            "scalability_score": scalability_score,
            "optimal_concurrency_level": self._find_optimal_concurrency(concurrency_results)
        }
    
    async def _benchmark_memory_usage(self) -> Dict[str, Any]:
        """å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•"""
        print("  æµ‹è¯•ç³»ç»Ÿå†…å­˜ä½¿ç”¨æƒ…å†µå’Œå†…å­˜æ•ˆç‡...")
        
        # è·å–åˆå§‹å†…å­˜çŠ¶æ€
        initial_memory = psutil.virtual_memory()
        initial_memory_mb = initial_memory.used / 1024 / 1024
        
        # åˆ›å»ºä¸åŒè§„æ¨¡çš„æµ‹è¯•
        memory_tests = []
        
        for i in range(5):
            # åˆ›å»ºåŒ…å«å¤§é‡æ•°æ®çš„ä»»åŠ¡
            large_data = "x" * (1000 * (i + 1))  # é€’å¢çš„æ•°æ®é‡
            
            task_config = TaskConfig(
                name=f"memory_test_{i}",
                command=f"python -c \"data='{large_data}'; print(f'Memory test {i}: {{len(data)}} bytes')\"",
                timeout=10
            )
            memory_tests.append(task_config)
        
        scheduler = create_unified_scheduler()
        
        memory_snapshots = []
        
        for i, task_config in enumerate(memory_tests):
            # è®°å½•æ‰§è¡Œå‰çš„å†…å­˜çŠ¶æ€
            before_memory = psutil.virtual_memory()
            before_memory_mb = before_memory.used / 1024 / 1024
            
            scheduler.register_task(task_config)
            result = await scheduler.execute_task(task_config.name)
            
            # è®°å½•æ‰§è¡Œåçš„å†…å­˜çŠ¶æ€
            after_memory = psutil.virtual_memory()
            after_memory_mb = after_memory.used / 1024 / 1024
            
            memory_snapshots.append({
                "test_index": i,
                "data_size_kb": len(task_config.command) / 1024,
                "memory_before_mb": before_memory_mb,
                "memory_after_mb": after_memory_mb,
                "memory_increase_mb": after_memory_mb - before_memory_mb,
                "execution_success": result.status.value == "completed"
            })
        
        # åˆ†æå†…å­˜ä½¿ç”¨è¶‹åŠ¿
        final_memory = psutil.virtual_memory()
        final_memory_mb = final_memory.used / 1024 / 1024
        
        memory_efficiency = self._calculate_memory_efficiency(memory_snapshots)
        
        return {
            "success": len(memory_snapshots) > 0,
            "memory_snapshots": memory_snapshots,
            "initial_memory_mb": initial_memory_mb,
            "final_memory_mb": final_memory_mb,
            "memory_increase_mb": final_memory_mb - initial_memory_mb,
            "memory_efficiency_score": memory_efficiency,
            "memory_leak_detected": final_memory_mb > initial_memory_mb + 50  # 50MBé˜ˆå€¼
        }
    
    async def _benchmark_processing_latency(self) -> Dict[str, Any]:
        """å¤„ç†å»¶è¿ŸåŸºå‡†æµ‹è¯•"""
        print("  æµ‹è¯•ä¸åŒå¤„ç†é˜¶æ®µçš„å»¶è¿Ÿ...")
        
        latency_tests = [
            {
                "stage": "input_validation",
                "description": "è¾“å…¥éªŒè¯å»¶è¿Ÿ",
                "test_function": self._test_input_validation_latency
            },
            {
                "stage": "task_scheduling",
                "description": "ä»»åŠ¡è°ƒåº¦å»¶è¿Ÿ",
                "test_function": self._test_task_scheduling_latency
            },
            {
                "stage": "output_generation",
                "description": "è¾“å‡ºç”Ÿæˆå»¶è¿Ÿ",
                "test_function": self._test_output_generation_latency
            }
        ]
        
        latency_results = []
        
        for test in latency_tests:
            print(f"    æµ‹è¯•: {test['description']}")
            result = await test["test_function"]()
            latency_results.append(result)
        
        # è®¡ç®—æ€»ä½“å»¶è¿Ÿè¯„åˆ†
        avg_latency = sum(r["average_latency_ms"] for r in latency_results) / len(latency_results)
        latency_score = max(0, 10 - avg_latency / 100)  # æ¯100mså»¶è¿Ÿæ‰£1åˆ†
        
        return {
            "success": len(latency_results) > 0,
            "latency_results": latency_results,
            "overall_average_latency_ms": avg_latency,
            "latency_score": latency_score,
            "grade": self._get_performance_grade(latency_score)
        }
    
    async def _test_input_validation_latency(self) -> Dict[str, Any]:
        """æµ‹è¯•è¾“å…¥éªŒè¯å»¶è¿Ÿ"""
        from enhanced_input_validator import create_smart_validator
        
        validator = create_smart_validator()
        test_inputs = [
            {"type": "text", "content": "ç®€å•çš„æ–‡æœ¬è¾“å…¥", "metadata": {}},
            {"type": "code", "content": "print('hello')", "metadata": {"language": "python"}},
            {"type": "structured", "content": {"key": "value"}, "metadata": {}}
        ]
        
        latencies = []
        
        for test_input in test_inputs:
            start_time = time.time()
            result = validator.validate_input(test_input, test_input["type"])
            end_time = time.time()
            
            latency_ms = (end_time - start_time) * 1000
            latencies.append(latency_ms)
        
        avg_latency = sum(latencies) / len(latencies)
        
        return {
            "stage": "input_validation",
            "test_count": len(latencies),
            "average_latency_ms": avg_latency,
            "min_latency_ms": min(latencies),
            "max_latency_ms": max(latencies),
            "meets_requirement": avg_latency < 100  # 100msè¦æ±‚
        }
    
    async def _test_task_scheduling_latency(self) -> Dict[str, Any]:
        """æµ‹è¯•ä»»åŠ¡è°ƒåº¦å»¶è¿Ÿ"""
        scheduler = create_unified_scheduler()
        
        # åˆ›å»ºç®€å•ä»»åŠ¡
        task_config = TaskConfig(
            name="scheduling_latency_test",
            command="python -c 'print(\"test\")'",
            timeout=5
        )
        scheduler.register_task(task_config)
        
        latencies = []
        
        for i in range(5):
            start_time = time.time()
            result = await scheduler.execute_task(task_config.name)
            end_time = time.time()
            
            if result.status.value == "completed":
                latency_ms = (end_time - start_time) * 1000
                latencies.append(latency_ms)
        
        avg_latency = sum(latencies) / len(latencies) if latencies else 0
        
        return {
            "stage": "task_scheduling",
            "test_count": len(latencies),
            "average_latency_ms": avg_latency,
            "min_latency_ms": min(latencies) if latencies else 0,
            "max_latency_ms": max(latencies) if latencies else 0,
            "meets_requirement": avg_latency < 500  # 500msè¦æ±‚
        }
    
    async def _test_output_generation_latency(self) -> Dict[str, Any]:
        """æµ‹è¯•è¾“å‡ºç”Ÿæˆå»¶è¿Ÿ"""
        from enhanced_output_validator import create_enhanced_output_validator
        
        validator = create_enhanced_output_validator()
        
        test_outputs = [
            {
                "content": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•è¾“å‡º",
                "quality_score": 0.8
            },
            {
                "content": "```python\nprint('hello')\n```",
                "format": "python",
                "quality_score": 0.9
            },
            {
                "content": {
                    "overview": "æµ‹è¯•æ€»ç»“",
                    "details": "è¯¦ç»†å†…å®¹",
                    "recommendations": ["å»ºè®®1", "å»ºè®®2"]
                },
                "completeness": 0.95,
                "quality_score": 0.85
            }
        ]
        
        latencies = []
        
        for i, test_output in enumerate(test_outputs):
            output_type = ["text_analysis", "code_suggestion", "summary_report"][i]
            requirements = {}
            
            start_time = time.time()
            result = validator.validate_output(test_output, output_type, requirements)
            end_time = time.time()
            
            latency_ms = (end_time - start_time) * 1000
            latencies.append(latency_ms)
        
        avg_latency = sum(latencies) / len(latencies)
        
        return {
            "stage": "output_generation",
            "test_count": len(latencies),
            "average_latency_ms": avg_latency,
            "min_latency_ms": min(latencies),
            "max_latency_ms": max(latencies),
            "meets_requirement": avg_latency < 200  # 200msè¦æ±‚
        }
    
    async def _benchmark_stability(self) -> Dict[str, Any]:
        """ç³»ç»Ÿç¨³å®šæ€§åŸºå‡†æµ‹è¯•"""
        print("  æµ‹è¯•ç³»ç»Ÿåœ¨å‹åŠ›ä¸‹çš„ç¨³å®šæ€§...")
        
        # åˆ›å»ºå‹åŠ›æµ‹è¯•åœºæ™¯
        stress_scheduler = create_parallel_scheduler(max_concurrent_tasks=8)
        
        # åˆ›å»ºå¤§é‡ä»»åŠ¡
        stress_tasks = []
        for i in range(20):
            task_config = TaskConfig(
                name=f"stress_task_{i}",
                command=f"python -c \"import time; time.sleep(0.1); print('Stress {i}')\"",
                timeout=30
            )
            stress_scheduler.register_task(task_config)
            stress_tasks.append(task_config)
        
        # å¯åŠ¨ç›‘æ§
        self.start_system_monitoring()
        self.system_metrics = []
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œå‹åŠ›æµ‹è¯•
            task_names = [tc.name for tc in stress_tasks]
            results = await stress_scheduler.execute_tasks(task_names)
            
            end_time = time.time()
            total_time = end_time - start_time
            
            # åˆ†æç»“æœ
            successful_tasks = sum(1 for r in results if r.status.value == "completed")
            failed_tasks = sum(1 for r in results if r.status.value == "failed")
            
            # æ£€æŸ¥é”™è¯¯æ¨¡å¼
            error_types = {}
            for result in results:
                if result.error_message:
                    error_type = result.error_message.split(":")[0] if ":" in result.error_message else "unknown"
                    error_types[error_type] = error_types.get(error_type, 0) + 1
            
            # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
            success_rate = successful_tasks / len(results) if results else 0
            stability_score = success_rate * 10  # è½¬æ¢ä¸º10åˆ†åˆ¶
            
            # åˆ†æç³»ç»Ÿèµ„æºä½¿ç”¨ç¨³å®šæ€§
            resource_stability = self._analyze_resource_stability()
            
            return {
                "success": True,
                "total_tasks": len(results),
                "successful_tasks": successful_tasks,
                "failed_tasks": failed_tasks,
                "success_rate": success_rate,
                "error_distribution": error_types,
                "total_execution_time": total_time,
                "stability_score": stability_score,
                "resource_stability": resource_stability,
                "grade": self._get_performance_grade(stability_score)
            }
            
        finally:
            self.stop_system_monitoring()
    
    def _calculate_overall_score(self, benchmarks: Dict[str, Any]) -> float:
        """è®¡ç®—æ€»ä½“æ€§èƒ½è¯„åˆ†"""
        scores = []
        
        for benchmark_name, benchmark_result in benchmarks.items():
            if "performance_score" in benchmark_result:
                scores.append(benchmark_result["performance_score"])
            elif "stability_score" in benchmark_result:
                scores.append(benchmark_result["stability_score"])
            elif "scalability_score" in benchmark_result:
                scores.append(benchmark_result["scalability_score"])
        
        return sum(scores) / len(scores) if scores else 0.0
    
    def _get_performance_grade(self, score: float) -> str:
        """è·å–æ€§èƒ½ç­‰çº§"""
        if score >= 9.0:
            return "A+ (ä¼˜ç§€)"
        elif score >= 8.0:
            return "A (è‰¯å¥½)"
        elif score >= 7.0:
            return "B (ä¸­ç­‰)"
        elif score >= 6.0:
            return "C (åŠæ ¼)"
        elif score >= 5.0:
            return "D (è¾ƒå·®)"
        else:
            return "F (å¤±è´¥)"
    
    def _calculate_scalability_score(self, concurrency_results: List[Dict]) -> float:
        """è®¡ç®—å¯æ‰©å±•æ€§è¯„åˆ†"""
        if len(concurrency_results) < 2:
            return 0.0
        
        # åˆ†æå¹¶å‘æ•ˆç‡éšå¹¶å‘çº§åˆ«å¢åŠ çš„å˜åŒ–
        efficiencies = [r["efficiency"] for r in concurrency_results]
        
        # ç†æƒ³æƒ…å†µä¸‹ï¼Œæ•ˆç‡åº”è¯¥ä¿æŒç¨³å®šæˆ–ç•¥æœ‰ä¸‹é™
        efficiency_trend = 0
        for i in range(1, len(efficiencies)):
            if efficiencies[i] >= efficiencies[i-1] * 0.8:  # å…è®¸20%çš„æ•ˆç‡ä¸‹é™
                efficiency_trend += 1
        
        return (efficiency_trend / (len(efficiencies) - 1)) * 10
    
    def _find_optimal_concurrency(self, concurrency_results: List[Dict]) -> int:
        """æ‰¾åˆ°æœ€ä¼˜å¹¶å‘çº§åˆ«"""
        if not concurrency_results:
            return 1
        
        # æ‰¾åˆ°æ•ˆç‡æœ€é«˜çš„å¹¶å‘çº§åˆ«
        best_result = max(concurrency_results, key=lambda x: x["tasks_per_second"])
        return best_result["concurrency_level"]
    
    def _calculate_memory_efficiency(self, memory_snapshots: List[Dict]) -> float:
        """è®¡ç®—å†…å­˜æ•ˆç‡è¯„åˆ†"""
        if not memory_snapshots:
            return 0.0
        
        # åˆ†æå†…å­˜å¢é•¿ä¸æ•°æ®å¤§å°çš„å…³ç³»
        memory_efficiencies = []
        for snapshot in memory_snapshots:
            if snapshot["data_size_kb"] > 0:
                efficiency = 1.0 / (snapshot["memory_increase_mb"] / (snapshot["data_size_kb"] / 1024))
                memory_efficiencies.append(min(efficiency, 10.0))  # é™åˆ¶æœ€å¤§åˆ†æ•°
        
        return sum(memory_efficiencies) / len(memory_efficiencies) if memory_efficiencies else 0.0
    
    def _analyze_resource_stability(self) -> Dict[str, Any]:
        """åˆ†æèµ„æºä½¿ç”¨ç¨³å®šæ€§"""
        if not self.system_metrics:
            return {"stability_score": 0.0, "analysis": "æ— ç›‘æ§æ•°æ®"}
        
        # è®¡ç®—CPUå’Œå†…å­˜ä½¿ç”¨çš„æ ‡å‡†å·®
        cpu_values = [m["cpu_percent"] for m in self.system_metrics]
        memory_values = [m["memory_percent"] for m in self.system_metrics]
        
        cpu_std = self._calculate_std_dev(cpu_values)
        memory_std = self._calculate_std_dev(memory_values)
        
        # ç¨³å®šæ€§åˆ†æ•°ï¼šæ ‡å‡†å·®è¶Šå°è¶Šç¨³å®š
        cpu_stability = max(0, 10 - cpu_std)
        memory_stability = max(0, 10 - memory_std)
        
        overall_stability = (cpu_stability + memory_stability) / 2
        
        return {
            "stability_score": overall_stability,
            "cpu_stability": cpu_stability,
            "memory_stability": memory_stability,
            "cpu_std": cpu_std,
            "memory_std": memory_std
        }
    
    def _calculate_std_dev(self, values: List[float]) -> float:
        """è®¡ç®—æ ‡å‡†å·®"""
        if len(values) < 2:
            return 0.0
        
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance ** 0.5
    
    def _analyze_system_metrics(self) -> Dict[str, Any]:
        """åˆ†æç³»ç»ŸæŒ‡æ ‡"""
        if not self.system_metrics:
            return {"analysis": "æ— ç³»ç»Ÿç›‘æ§æ•°æ®"}
        
        cpu_values = [m["cpu_percent"] for m in self.system_metrics]
        memory_values = [m["memory_percent"] for m in self.system_metrics]
        
        return {
            "peak_cpu_percent": max(cpu_values) if cpu_values else 0,
            "average_cpu_percent": sum(cpu_values) / len(cpu_values) if cpu_values else 0,
            "peak_memory_percent": max(memory_values) if memory_values else 0,
            "average_memory_percent": sum(memory_values) / len(memory_values) if memory_values else 0,
            "total_samples": len(self.system_metrics),
            "monitoring_duration": self.system_metrics[-1]["timestamp"] - self.system_metrics[0]["timestamp"] if len(self.system_metrics) > 1 else 0
        }


def create_performance_benchmark() -> PerformanceBenchmarkTester:
    """åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    return PerformanceBenchmarkTester()


async def run_performance_benchmark():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    tester = create_performance_benchmark()
    
    try:
        results = await tester.run_comprehensive_benchmark()
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = generate_performance_report(results)
        print("\n" + "="*60)
        print(report)
        print("="*60)
        
        # ä¿å­˜ç»“æœ
        results_file = "performance_benchmark_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“Š è¯¦ç»†æ€§èƒ½æ•°æ®å·²ä¿å­˜åˆ°: {results_file}")
        
        return results["overall_score"] >= 7.0  # 7åˆ†ä»¥ä¸Šè®¤ä¸ºæ€§èƒ½è‰¯å¥½
        
    except Exception as e:
        print(f"\nâŒ æ€§èƒ½åŸºå‡†æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return False


def generate_performance_report(results: Dict[str, Any]) -> str:
    """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
    report = []
    report.append("AGIç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
    report.append("=" * 50)
    report.append(f"æµ‹è¯•æ—¶é—´: {results.get('start_time', 'Unknown')}")
    report.append(f"æ€»æ‰§è¡Œæ—¶é—´: {results.get('total_execution_time', 0):.2f}ç§’")
    report.append(f"æ€»ä½“æ€§èƒ½è¯„åˆ†: {results.get('overall_score', 0):.1f}/10.0")
    report.append("")
    
    # è¯¦ç»†åŸºå‡†æµ‹è¯•ç»“æœ
    benchmarks = results.get("benchmarks", {})
    
    for benchmark_name, benchmark_result in benchmarks.items():
        report.append(f"ğŸ“Š {benchmark_name.upper()}:")
        
        if benchmark_name == "response_time":
            report.append(f"  æ€»ä½“å¹³å‡å“åº”æ—¶é—´: {benchmark_result.get('overall_average_response_time', 0):.3f}ç§’")
            report.append(f"  æ€§èƒ½è¯„åˆ†: {benchmark_result.get('performance_score', 0):.1f}/10.0")
            report.append(f"  ç­‰çº§: {benchmark_result.get('grade', 'Unknown')}")
            
            for rt in benchmark_result.get('response_times', []):
                report.append(f"    {rt['task_complexity']}: {rt['average_response_time']:.3f}s (max: {rt['max_response_time']:.3f}s)")
        
        elif benchmark_name == "throughput":
            report.append(f"  ååé‡: {benchmark_result.get('throughput_per_second', 0):.2f} ä»»åŠ¡/ç§’")
            report.append(f"  å¹¶å‘æ•ˆç‡: {benchmark_result.get('concurrent_efficiency', 0):.1f}%")
            report.append(f"  æ€§èƒ½è¯„åˆ†: {benchmark_result.get('performance_score', 0):.1f}/10.0")
            report.append(f"  ç­‰çº§: {benchmark_result.get('grade', 'Unknown')}")
        
        elif benchmark_name == "concurrency":
            report.append(f"  å¯æ‰©å±•æ€§è¯„åˆ†: {benchmark_result.get('scalability_score', 0):.1f}/10.0")
            report.append(f"  æœ€ä¼˜å¹¶å‘çº§åˆ«: {benchmark_result.get('optimal_concurrency_level', 1)}")
            
            for cr in benchmark_result.get('concurrency_results', []):
                report.append(f"    å¹¶å‘{cr['concurrency_level']}: {cr['tasks_per_second']:.2f}ä»»åŠ¡/ç§’, æ•ˆç‡{cr['efficiency']:.1f}%")
        
        elif benchmark_name == "stability":
            report.append(f"  ç¨³å®šæ€§è¯„åˆ†: {benchmark_result.get('stability_score', 0):.1f}/10.0")
            report.append(f"  æˆåŠŸç‡: {benchmark_result.get('success_rate', 0):.1%}")
            report.append(f"  ç­‰çº§: {benchmark_result.get('grade', 'Unknown')}")
        
        report.append("")
    
    # ç³»ç»ŸæŒ‡æ ‡
    system_metrics = results.get("system_metrics", {})
    if system_metrics:
        report.append("ğŸ’» ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ:")
        report.append(f"  å³°å€¼CPUä½¿ç”¨ç‡: {system_metrics.get('peak_cpu_percent', 0):.1f}%")
        report.append(f"  å¹³å‡CPUä½¿ç”¨ç‡: {system_metrics.get('average_cpu_percent', 0):.1f}%")
        report.append(f"  å³°å€¼å†…å­˜ä½¿ç”¨ç‡: {system_metrics.get('peak_memory_percent', 0):.1f}%")
        report.append(f"  å¹³å‡å†…å­˜ä½¿ç”¨ç‡: {system_metrics.get('average_memory_percent', 0):.1f}%")
        report.append("")
    
    # æ€§èƒ½å»ºè®®
    overall_score = results.get("overall_score", 0)
    if overall_score >= 9.0:
        report.append("ğŸ¯ æ€§èƒ½è¯„ä»·: ä¼˜ç§€ - ç³»ç»Ÿæ€§èƒ½è¡¨ç°å‡ºè‰²")
    elif overall_score >= 8.0:
        report.append("ğŸ¯ æ€§èƒ½è¯„ä»·: è‰¯å¥½ - ç³»ç»Ÿæ€§èƒ½æ»¡è¶³è¦æ±‚")
    elif overall_score >= 7.0:
        report.append("ğŸ¯ æ€§èƒ½è¯„ä»·: ä¸­ç­‰ - ç³»ç»Ÿæ€§èƒ½åŸºæœ¬æ»¡è¶³è¦æ±‚")
    else:
        report.append("ğŸ¯ æ€§èƒ½è¯„ä»·: éœ€è¦ä¼˜åŒ– - å»ºè®®è¿›è¡Œæ€§èƒ½è°ƒä¼˜")
    
    return "\n".join(report)


if __name__ == '__main__':
    import asyncio
    
    print("ğŸš€ å¯åŠ¨AGIç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
    success = asyncio.run(run_performance_benchmark())
    
    if success:
        print("\nğŸ‰ æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿæ€§èƒ½è¾¾åˆ°é¢„æœŸæ ‡å‡†")
        exit(0)
    else:
        print("\nâŒ æ€§èƒ½åŸºå‡†æµ‹è¯•æœªé€šè¿‡ï¼Œå»ºè®®è¿›è¡Œæ€§èƒ½ä¼˜åŒ–")
        exit(1)