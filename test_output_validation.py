#!/usr/bin/env python3
"""
è¾“å‡ºéªŒè¯æœºåˆ¶
ç¡®ä¿ç³»ç»Ÿè¾“å‡ºçš„è´¨é‡ã€å®‰å…¨æ€§å’Œæ­£ç¡®æ€§
"""

import sys
sys.path.append('apps/backend/src')

import re
import json
import asyncio
import numpy as np
from typing import List, Dict, Any, Optional, Set
from dataclasses import dataclass
from datetime import datetime
import hashlib

@dataclass
class OutputValidationResult:
    """è¾“å‡ºéªŒè¯ç»“æœ"""
    is_valid: bool
    safety_score: float
    quality_score: float
    correctness_score: float
    semantic_coherence: float
    creativity_score: float
    issues: List[str]
    suggestions: List[str]
    detailed_metrics: Dict[str, Any]

@dataclass
class ValidationConfig:
    """éªŒè¯é…ç½®"""
    enable_safety_check: bool = True
    enable_quality_check: bool = True
    enable_correctness_check: bool = True
    enable_semantic_coherence: bool = True
    enable_creativity_check: bool = True
    max_response_length: int = 4096
    min_response_length: int = 10
    profanity_threshold: float = 0.1
    semantic_coherence_threshold: float = 0.7
    creativity_threshold: float = 0.3

class OutputValidationMechanism:
    """è¾“å‡ºéªŒè¯æœºåˆ¶"""
    
    def __init__(self, config: ValidationConfig = ValidationConfig()):
        self.config = config
        self.safety_patterns = self._load_safety_patterns()
        self.quality_metrics = self._initialize_quality_metrics()
        self.creativity_indicators = self._load_creativity_indicators()
        
    def _load_safety_patterns(self) -> Dict[str, List[str]]:
        """åŠ è½½å®‰å…¨æ£€æŸ¥æ¨¡å¼"""
        return {
            'profanity': [
                r'\b(fuck|shit|damn|hell)\b',
                r'\b(bitch|asshole|crap)\b',
                r'\b(retard|idiot|stupid)\b'
            ],
            'hate_speech': [
                r'\b(hate|kill|destroy)\s+(people|group|race)\b',
                r'\b(inferior|superior)\s+(race|ethnicity)\b',
                r'\b(violence|attack|harm)\s+(against|toward)\b'
            ],
            'misinformation': [
                r'\b(flat\s+earth|vaccine\s+causes|5g\s+causes)\b',
                r'\b(conspiracy\s+theory|fake\s+news|hoax)\b',
                r'\b(absolute\s+truth|never\s+wrong|always\s+right)\b'
            ],
            'personal_info': [
                r'\b\d{3}-\d{2}-\d{4}\b',  # SSN pattern
                r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',  # Email
                r'\b\d{3}-\d{3}-\d{4}\b',  # Phone number
                r'\b\d{4}-\d{4}-\d{4}-\d{4}\b'  # Credit card
            ]
        }
    
    def _initialize_quality_metrics(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–è´¨é‡æŒ‡æ ‡"""
        return {
            'readability_factors': {
                'sentence_length': {'ideal': 15, 'max': 25},
                'paragraph_length': {'ideal': 5, 'max': 10},
                'vocabulary_complexity': {'max_unique_ratio': 0.7}
            },
            'structure_factors': {
                'coherence_markers': ['å› æ­¤', 'æ‰€ä»¥', 'ä»è€Œ', 'è¿›è€Œ', 'ä½†æ˜¯', 'ç„¶è€Œ', 'å¦å¤–'],
                'transition_words': ['é¦–å…ˆ', 'å…¶æ¬¡', 'æœ€å', 'æ€»ä¹‹', 'ç»¼ä¸Šæ‰€è¿°'],
                'logical_connectors': ['å› ä¸º', 'ç”±äº', 'é‰´äº', 'åŸºäº', 'æ ¹æ®']
            },
            'content_factors': {
                'factual_accuracy_indicators': ['ç ”ç©¶è¡¨æ˜', 'æ•°æ®æ˜¾ç¤º', 'å®éªŒè¯æ˜', 'è°ƒæŸ¥å‘ç°'],
                'uncertainty_markers': ['å¯èƒ½', 'æˆ–è®¸', 'å¤§æ¦‚', 'ä¼¼ä¹', 'çœ‹èµ·æ¥'],
                'certainty_markers': ['ç¡®å®š', 'è‚¯å®š', 'å¿…ç„¶', 'ç»å¯¹', 'æ¯«æ— ç–‘é—®']
            }
        }
    
    def _load_creativity_indicators(self) -> Dict[str, Any]:
        """åŠ è½½åˆ›é€ æ€§æŒ‡æ ‡"""
        return {
            'novelty_indicators': {
                'unique_phrases': [],
                'uncommon_words': [],
                'original_metaphors': []
            },
            'diversity_indicators': {
                'sentence_structure_variety': 0.0,
                'vocabulary_richness': 0.0,
                'conceptual_diversity': 0.0
            },
            'value_indicators': {
                'insightful_statements': [],
                'practical_applications': [],
                'thought_provoking_questions': []
            }
        }
    
    async def validate_output(
        self,
        output_text: str,
        input_context: Optional[str] = None,
        expected_type: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> OutputValidationResult:
        """
        éªŒè¯è¾“å‡º
        
        Args:
            output_text: è¾“å‡ºæ–‡æœ¬
            input_context: è¾“å…¥ä¸Šä¸‹æ–‡
            expected_type: æœŸæœ›çš„è¾“å‡ºç±»å‹
            metadata: å…ƒæ•°æ®
            
        Returns:
            éªŒè¯ç»“æœ
        """
        print(f"å¼€å§‹éªŒè¯è¾“å‡ºï¼Œé•¿åº¦: {len(output_text)} å­—ç¬¦")
        
        issues = []
        suggestions = []
        detailed_metrics = {}
        
        # 1. å®‰å…¨æ£€æŸ¥
        if self.config.enable_safety_check:
            safety_result = await self._perform_safety_check(output_text)
            if not safety_result['passed']:
                issues.extend(safety_result['issues'])
            detailed_metrics['safety'] = safety_result
        
        # 2. è´¨é‡æ£€æŸ¥
        if self.config.enable_quality_check:
            quality_result = await self._perform_quality_check(output_text)
            if not quality_result['passed']:
                issues.extend(quality_result['issues'])
            detailed_metrics['quality'] = quality_result
        
        # 3. æ­£ç¡®æ€§æ£€æŸ¥
        if self.config.enable_correctness_check:
            correctness_result = await self._perform_correctness_check(output_text, input_context)
            if not correctness_result['passed']:
                issues.extend(correctness_result['issues'])
            detailed_metrics['correctness'] = correctness_result
        
        # 4. è¯­ä¹‰è¿è´¯æ€§æ£€æŸ¥
        if self.config.enable_semantic_coherence:
            coherence_result = await self._perform_semantic_coherence_check(output_text, input_context)
            if not coherence_result['passed']:
                issues.extend(coherence_result['issues'])
            detailed_metrics['coherence'] = coherence_result
        
        # 5. åˆ›é€ æ€§æ£€æŸ¥
        if self.config.enable_creativity_check:
            creativity_result = await self._perform_creativity_check(output_text)
            if not creativity_result['passed']:
                issues.extend(creativity_result['issues'])
            detailed_metrics['creativity'] = creativity_result
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°
        safety_score = detailed_metrics.get('safety', {}).get('score', 1.0)
        quality_score = detailed_metrics.get('quality', {}).get('score', 1.0)
        correctness_score = detailed_metrics.get('correctness', {}).get('score', 1.0)
        semantic_coherence = detailed_metrics.get('coherence', {}).get('score', 1.0)
        creativity_score = detailed_metrics.get('creativity', {}).get('score', 1.0)
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        suggestions = self._generate_improvement_suggestions(issues, detailed_metrics)
        
        # åˆ¤æ–­æ•´ä½“æœ‰æ•ˆæ€§
        is_valid = (
            safety_score >= 0.8 and
            quality_score >= 0.7 and
            correctness_score >= 0.7 and
            semantic_coherence >= self.config.semantic_coherence_threshold and
            creativity_score >= self.config.creativity_threshold and
            len(issues) <= 3  # å…è®¸å°‘é‡é—®é¢˜
        )
        
        result = OutputValidationResult(
            is_valid=is_valid,
            safety_score=safety_score,
            quality_score=quality_score,
            correctness_score=correctness_score,
            semantic_coherence=semantic_coherence,
            creativity_score=creativity_score,
            issues=issues,
            suggestions=suggestions,
            detailed_metrics=detailed_metrics
        )
        
        print(f"âœ“ è¾“å‡ºéªŒè¯å®Œæˆï¼Œæœ‰æ•ˆæ€§: {is_valid}")
        print(f"  å®‰å…¨åˆ†æ•°: {safety_score:.3f}")
        print(f"  è´¨é‡åˆ†æ•°: {quality_score:.3f}")
        print(f"  æ­£ç¡®æ€§åˆ†æ•°: {correctness_score:.3f}")
        print(f"  è¯­ä¹‰è¿è´¯æ€§: {semantic_coherence:.3f}")
        print(f"  åˆ›é€ æ€§åˆ†æ•°: {creativity_score:.3f}")
        
        return result
    
    async def _perform_safety_check(self, text: str) -> Dict[str, Any]:
        """æ‰§è¡Œå®‰å…¨æ£€æŸ¥"""
        print("  æ‰§è¡Œå®‰å…¨æ£€æŸ¥...")
        
        issues = []
        safety_score = 1.0
        
        # æ£€æŸ¥äºµæ¸è¯­è¨€
        profanity_matches = self._check_profanity(text)
        if profanity_matches:
            issues.append(f"æ£€æµ‹åˆ°äºµæ¸è¯­è¨€: {len(profanity_matches)} å¤„")
            safety_score -= 0.3 * len(profanity_matches)
        
        # æ£€æŸ¥ä»‡æ¨è¨€è®º
        hate_matches = self._check_hate_speech(text)
        if hate_matches:
            issues.append(f"æ£€æµ‹åˆ°ä»‡æ¨è¨€è®º: {len(hate_matches)} å¤„")
            safety_score -= 0.5 * len(hate_matches)
        
        # æ£€æŸ¥é”™è¯¯ä¿¡æ¯
        misinfo_matches = self._check_misinformation(text)
        if misinfo_matches:
            issues.append(f"æ£€æµ‹åˆ°é”™è¯¯ä¿¡æ¯: {len(misinfo_matches)} å¤„")
            safety_score -= 0.4 * len(misinfo_matches)
        
        # æ£€æŸ¥ä¸ªäººä¿¡æ¯æ³„éœ²
        personal_matches = self._check_personal_info(text)
        if personal_matches:
            issues.append(f"æ£€æµ‹åˆ°ä¸ªäººä¿¡æ¯: {len(personal_matches)} å¤„")
            safety_score -= 0.6 * len(personal_matches)
        
        # æ£€æŸ¥é•¿åº¦é™åˆ¶
        if len(text) > self.config.max_response_length:
            issues.append(f"è¾“å‡ºè¿‡é•¿: {len(text)} > {self.config.max_response_length}")
            safety_score -= 0.2
        elif len(text) < self.config.min_response_length:
            issues.append(f"è¾“å‡ºè¿‡çŸ­: {len(text)} < {self.config.min_response_length}")
            safety_score -= 0.1
        
        safety_score = max(0.0, safety_score)
        
        return {
            'passed': safety_score >= 0.7,
            'score': safety_score,
            'issues': issues,
            'details': {
                'profanity_matches': len(profanity_matches),
                'hate_matches': len(hate_matches),
                'misinfo_matches': len(misinfo_matches),
                'personal_matches': len(personal_matches)
            }
        }
    
    def _check_profanity(self, text: str) -> List[str]:
        """æ£€æŸ¥äºµæ¸è¯­è¨€"""
        matches = []
        for pattern in self.safety_patterns['profanity']:
            found = re.findall(pattern, text, re.IGNORECASE)
            matches.extend(found)
        return matches
    
    def _check_hate_speech(self, text: str) -> List[str]:
        """æ£€æŸ¥ä»‡æ¨è¨€è®º"""
        matches = []
        for pattern in self.safety_patterns['hate_speech']:
            found = re.findall(pattern, text, re.IGNORECASE)
            matches.extend(found)
        return matches
    
    def _check_misinformation(self, text: str) -> List[str]:
        """æ£€æŸ¥é”™è¯¯ä¿¡æ¯"""
        matches = []
        for pattern in self.safety_patterns['misinformation']:
            found = re.findall(pattern, text, re.IGNORECASE)
            matches.extend(found)
        return matches
    
    def _check_personal_info(self, text: str) -> List[str]:
        """æ£€æŸ¥ä¸ªäººä¿¡æ¯"""
        matches = []
        for pattern in self.safety_patterns['personal_info']:
            found = re.findall(pattern, text)
            matches.extend(found)
        return matches
    
    async def _perform_quality_check(self, text: str) -> Dict[str, Any]:
        """æ‰§è¡Œè´¨é‡æ£€æŸ¥"""
        print("  æ‰§è¡Œè´¨é‡æ£€æŸ¥...")
        
        issues = []
        quality_score = 1.0
        
        # å¯è¯»æ€§æ£€æŸ¥
        readability_score = self._check_readability(text)
        if readability_score < 0.7:
            issues.append(f"å¯è¯»æ€§åˆ†æ•°è¾ƒä½: {readability_score:.3f}")
            quality_score -= 0.2
        
        # ç»“æ„æ£€æŸ¥
        structure_score = self._check_structure(text)
        if structure_score < 0.6:
            issues.append(f"ç»“æ„åˆ†æ•°è¾ƒä½: {structure_score:.3f}")
            quality_score -= 0.2
        
        # å†…å®¹è´¨é‡æ£€æŸ¥
        content_score = self._check_content_quality(text)
        if content_score < 0.7:
            issues.append(f"å†…å®¹è´¨é‡åˆ†æ•°è¾ƒä½: {content_score:.3f}")
            quality_score -= 0.15
        
        quality_score = max(0.0, quality_score)
        
        return {
            'passed': quality_score >= 0.7,
            'score': quality_score,
            'issues': issues,
            'details': {
                'readability_score': readability_score,
                'structure_score': structure_score,
                'content_score': content_score
            }
        }
    
    def _check_readability(self, text: str) -> float:
        """æ£€æŸ¥å¯è¯»æ€§"""
        sentences = text.split('ã€‚')
        if not sentences:
            return 0.0
        
        # å¥å­é•¿åº¦æ£€æŸ¥
        sentence_lengths = [len(s.strip()) for s in sentences if s.strip()]
        avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0
        
        ideal_length = self.quality_metrics['readability_factors']['sentence_length']['ideal']
        max_length = self.quality_metrics['readability_factors']['sentence_length']['max']
        
        if avg_sentence_length <= ideal_length:
            sentence_score = 1.0
        elif avg_sentence_length <= max_length:
            sentence_score = 1.0 - (avg_sentence_length - ideal_length) / (max_length - ideal_length)
        else:
            sentence_score = 0.3
        
        # è¯æ±‡å¤æ‚åº¦æ£€æŸ¥
        words = text.split()
        unique_words = set(words)
        unique_ratio = len(unique_words) / len(words) if words else 0
        max_unique_ratio = self.quality_metrics['readability_factors']['vocabulary_complexity']['max_unique_ratio']
        
        if unique_ratio <= max_unique_ratio:
            vocab_score = 1.0
        else:
            vocab_score = 0.7
        
        return (sentence_score + vocab_score) / 2
    
    def _check_structure(self, text: str) -> float:
        """æ£€æŸ¥ç»“æ„"""
        coherence_markers = self.quality_metrics['structure_factors']['coherence_markers']
        transition_words = self.quality_metrics['structure_factors']['transition_words']
        logical_connectors = self.quality_metrics['structure_factors']['logical_connectors']
        
        # æ£€æŸ¥è¿è´¯æ€§æ ‡è®°
        coherence_count = sum(1 for marker in coherence_markers if marker in text)
        coherence_score = min(1.0, coherence_count / 3)  # è‡³å°‘3ä¸ªè¿è´¯æ€§æ ‡è®°
        
        # æ£€æŸ¥è¿‡æ¸¡è¯
        transition_count = sum(1 for word in transition_words if word in text)
        transition_score = min(1.0, transition_count / 2)  # è‡³å°‘2ä¸ªè¿‡æ¸¡è¯
        
        # æ£€æŸ¥é€»è¾‘è¿æ¥è¯
        connector_count = sum(1 for connector in logical_connectors if connector in text)
        connector_score = min(1.0, connector_count / 2)  # è‡³å°‘2ä¸ªé€»è¾‘è¿æ¥è¯
        
        return (coherence_score + transition_score + connector_score) / 3
    
    def _check_content_quality(self, text: str) -> float:
        """æ£€æŸ¥å†…å®¹è´¨é‡"""
        content_factors = self.quality_metrics['content_factors']
        
        # æ£€æŸ¥äº‹å®å‡†ç¡®æ€§æŒ‡æ ‡
        factual_indicators = content_factors['factual_accuracy_indicators']
        factual_count = sum(1 for indicator in factual_indicators if indicator in text)
        factual_score = min(1.0, factual_count / 2)  # è‡³å°‘2ä¸ªäº‹å®æŒ‡æ ‡
        
        # æ£€æŸ¥ä¸ç¡®å®šæ€§æ ‡è®°ï¼ˆå¹³è¡¡ç¡®å®šæ€§å’Œä¸ç¡®å®šæ€§ï¼‰
        uncertainty_markers = content_factors['uncertainty_markers']
        certainty_markers = content_factors['certainty_markers']
        
        uncertainty_count = sum(1 for marker in uncertainty_markers if marker in text)
        certainty_count = sum(1 for marker in certainty_markers if marker in text)
        
        # å¹³è¡¡ç¡®å®šæ€§å’Œä¸ç¡®å®šæ€§
        if certainty_count > uncertainty_count * 3:  # è¿‡äºç¡®å®š
            certainty_score = 0.6
        elif uncertainty_count > certainty_count * 3:  # è¿‡äºä¸ç¡®å®š
            certainty_score = 0.6
        else:
            certainty_score = 1.0
        
        return (factual_score + certainty_score) / 2
    
    async def _perform_correctness_check(self, text: str, input_context: Optional[str]) -> Dict[str, Any]:
        """æ‰§è¡Œæ­£ç¡®æ€§æ£€æŸ¥"""
        print("  æ‰§è¡Œæ­£ç¡®æ€§æ£€æŸ¥...")
        
        issues = []
        correctness_score = 1.0
        
        # åŸºæœ¬è¯­æ³•æ£€æŸ¥
        grammar_issues = self._check_grammar(text)
        if grammar_issues:
            issues.append(f"è¯­æ³•é—®é¢˜: {len(grammar_issues)} å¤„")
            correctness_score -= 0.1 * len(grammar_issues)
        
        # äº‹å®ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        if input_context:
            consistency_score = self._check_factual_consistency(text, input_context)
            if consistency_score < 0.7:
                issues.append(f"ä¸è¾“å…¥ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ä¸è¶³: {consistency_score:.3f}")
                correctness_score -= 0.2
        
        # é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥
        logic_score = self._check_logical_consistency(text)
        if logic_score < 0.7:
            issues.append(f"é€»è¾‘ä¸€è‡´æ€§ä¸è¶³: {logic_score:.3f}")
            correctness_score -= 0.2
        
        correctness_score = max(0.0, correctness_score)
        
        return {
            'passed': correctness_score >= 0.7,
            'score': correctness_score,
            'issues': issues,
            'details': {
                'grammar_issues': len(grammar_issues),
                'consistency_score': consistency_score if input_context else 1.0,
                'logic_score': logic_score
            }
        }
    
    def _check_grammar(self, text: str) -> List[str]:
        """æ£€æŸ¥è¯­æ³•"""
        issues = []
        
        # ç®€å•çš„è¯­æ³•æ£€æŸ¥è§„åˆ™
        # æ£€æŸ¥å¸¸è§è¯­æ³•é”™è¯¯æ¨¡å¼
        
        # æ£€æŸ¥é‡å¤çš„æ ‡ç‚¹ç¬¦å·
        if re.search(r'[ã€‚ï¼ï¼Ÿ]{2,}', text):
            issues.append("é‡å¤çš„æ ‡ç‚¹ç¬¦å·")
        
        # æ£€æŸ¥ä¸åŒ¹é…çš„æ‹¬å·
        if text.count('(') != text.count(')'):
            issues.append("ä¸åŒ¹é…çš„æ‹¬å·")
        
        # æ£€æŸ¥å¥å­ç»“æ„é—®é¢˜
        sentences = text.split('ã€‚')
        for sentence in sentences:
            if sentence.strip() and len(sentence.strip()) < 5:
                issues.append("è¿‡çŸ­çš„å¥å­")
        
        return issues
    
    def _check_factual_consistency(self, text: str, context: str) -> float:
        """æ£€æŸ¥äº‹å®ä¸€è‡´æ€§"""
        # ç®€åŒ–çš„å®ç°ï¼šæ£€æŸ¥å…³é”®è¯é‡å 
        text_words = set(text.lower().split())
        context_words = set(context.lower().split())
        
        overlap = len(text_words & context_words)
        total = len(text_words | context_words)
        
        if total == 0:
            return 1.0
        
        consistency = overlap / total
        return consistency
    
    def _check_logical_consistency(self, text: str) -> float:
        """æ£€æŸ¥é€»è¾‘ä¸€è‡´æ€§"""
        # ç®€åŒ–çš„é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥
        # æ£€æŸ¥æ˜æ˜¾çš„é€»è¾‘çŸ›ç›¾
        
        contradiction_patterns = [
            (r'æ˜¯', r'ä¸æ˜¯'),
            (r'å¯ä»¥', r'ä¸å¯ä»¥'),
            (r'å¯èƒ½', r'ä¸å¯èƒ½')
        ]
        
        contradictions = 0
        for pattern1, pattern2 in contradiction_patterns:
            if re.search(pattern1, text) and re.search(pattern2, text):
                contradictions += 1
        
        # åŸºäºçŸ›ç›¾æ•°é‡è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
        consistency = max(0.0, 1.0 - contradictions * 0.3)
        
        return consistency
    
    async def _perform_semantic_coherence_check(self, text: str, input_context: Optional[str]) -> Dict[str, Any]:
        """æ‰§è¡Œè¯­ä¹‰è¿è´¯æ€§æ£€æŸ¥"""
        print("  æ‰§è¡Œè¯­ä¹‰è¿è´¯æ€§æ£€æŸ¥...")
        
        issues = []
        coherence_score = 1.0
        
        # ä¸»é¢˜ä¸€è‡´æ€§æ£€æŸ¥
        if input_context:
            theme_consistency = self._check_theme_consistency(text, input_context)
            if theme_consistency < self.config.semantic_coherence_threshold:
                issues.append(f"ä¸»é¢˜ä¸€è‡´æ€§ä¸è¶³: {theme_consistency:.3f}")
                coherence_score -= 0.3
        
        # æ¦‚å¿µè¿è´¯æ€§æ£€æŸ¥
        concept_coherence = self._check_concept_coherence(text)
        if concept_coherence < 0.7:
            issues.append(f"æ¦‚å¿µè¿è´¯æ€§ä¸è¶³: {concept_coherence:.3f}")
            coherence_score -= 0.2
        
        # è¯­ä¹‰æµæ£€æŸ¥
        semantic_flow = self._check_semantic_flow(text)
        if semantic_flow < 0.7:
            issues.append(f"è¯­ä¹‰æµä¸è¿è´¯: {semantic_flow:.3f}")
            coherence_score -= 0.2
        
        coherence_score = max(0.0, coherence_score)
        
        return {
            'passed': coherence_score >= self.config.semantic_coherence_threshold,
            'score': coherence_score,
            'issues': issues,
            'details': {
                'theme_consistency': theme_consistency if input_context else 1.0,
                'concept_coherence': concept_coherence,
                'semantic_flow': semantic_flow
            }
        }
    
    def _check_theme_consistency(self, text: str, context: str) -> float:
        """æ£€æŸ¥ä¸»é¢˜ä¸€è‡´æ€§"""
        # æå–å…³é”®è¯
        text_keywords = self._extract_keywords(text)
        context_keywords = self._extract_keywords(context)
        
        # è®¡ç®—å…³é”®è¯é‡å åº¦
        overlap = len(text_keywords & context_keywords)
        total = len(text_keywords | context_keywords)
        
        if total == 0:
            return 1.0
        
        consistency = overlap / total
        return consistency
    
    def _extract_keywords(self, text: str) -> Set[str]:
        """æå–å…³é”®è¯"""
        # ç®€åŒ–çš„å…³é”®è¯æå–
        words = text.lower().split()
        # è¿‡æ»¤æ‰å¸¸è§åœç”¨è¯
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
        keywords = {word for word in words if word not in stop_words and len(word) > 1}
        return keywords
    
    def _check_concept_coherence(self, text: str) -> float:
        """æ£€æŸ¥æ¦‚å¿µè¿è´¯æ€§"""
        # ç®€åŒ–çš„æ¦‚å¿µè¿è´¯æ€§æ£€æŸ¥
        sentences = text.split('ã€‚')
        if len(sentences) < 2:
            return 1.0
        
        coherence_scores = []
        for i in range(len(sentences) - 1):
            sent1_words = set(sentences[i].lower().split())
            sent2_words = set(sentences[i + 1].lower().split())
            
            overlap = len(sent1_words & sent2_words)
            total = len(sent1_words | sent2_words)
            
            if total > 0:
                coherence = overlap / total
                coherence_scores.append(coherence)
        
        return float(np.mean(coherence_scores)) if coherence_scores else 1.0
    
    def _check_semantic_flow(self, text: str) -> float:
        """æ£€æŸ¥è¯­ä¹‰æµ"""
        # ç®€åŒ–çš„è¯­ä¹‰æµæ£€æŸ¥
        sentences = text.split('ã€‚')
        if len(sentences) < 2:
            return 1.0
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆé€‚çš„è¿‡æ¸¡
        transition_words = ['å› æ­¤', 'æ‰€ä»¥', 'ä»è€Œ', 'è¿›è€Œ', 'ä½†æ˜¯', 'ç„¶è€Œ', 'å¦å¤–', 'æ­¤å¤–']
        transition_count = sum(1 for word in transition_words if word in text)
        
        # åŸºäºè¿‡æ¸¡è¯æ•°é‡è®¡ç®—è¯­ä¹‰æµåˆ†æ•°
        flow_score = min(1.0, transition_count / 3)  # è‡³å°‘3ä¸ªè¿‡æ¸¡è¯
        
        return flow_score
    
    async def _perform_creativity_check(self, text: str) -> Dict[str, Any]:
        """æ‰§è¡Œåˆ›é€ æ€§æ£€æŸ¥"""
        print("  æ‰§è¡Œåˆ›é€ æ€§æ£€æŸ¥...")
        
        issues = []
        creativity_score = 0.0
        
        # æ–°é¢–æ€§æ£€æŸ¥
        novelty_score = self._check_novelty(text)
        
        # å¤šæ ·æ€§æ£€æŸ¥
        diversity_score = self._check_diversity(text)
        
        # ä»·å€¼æ€§æ£€æŸ¥
        value_score = self._check_value(text)
        
        # ç»¼åˆåˆ›é€ æ€§åˆ†æ•°
        creativity_score = (novelty_score + diversity_score + value_score) / 3
        
        if creativity_score < self.config.creativity_threshold:
            issues.append(f"åˆ›é€ æ€§åˆ†æ•°ä¸è¶³: {creativity_score:.3f}")
        
        return {
            'passed': creativity_score >= self.config.creativity_threshold,
            'score': creativity_score,
            'issues': issues,
            'details': {
                'novelty_score': novelty_score,
                'diversity_score': diversity_score,
                'value_score': value_score
            }
        }
    
    def _check_novelty(self, text: str) -> float:
        """æ£€æŸ¥æ–°é¢–æ€§"""
        # ç®€åŒ–çš„æ–°é¢–æ€§æ£€æŸ¥
        # åŸºäºè¯æ±‡çš„ç‹¬ç‰¹æ€§å’Œå¥å­ç»“æ„çš„å¤šæ ·æ€§
        
        words = text.split()
        unique_words = set(words)
        vocabulary_richness = len(unique_words) / len(words) if words else 0
        
        # åŸºäºè¯æ±‡ä¸°å¯Œåº¦çš„æ–°é¢–æ€§åˆ†æ•°
        novelty_score = min(1.0, vocabulary_richness * 1.5)  # æ”¾å¤§ç³»æ•°
        
        return novelty_score
    
    def _check_diversity(self, text: str) -> float:
        """æ£€æŸ¥å¤šæ ·æ€§"""
        # å¥å­ç»“æ„å¤šæ ·æ€§
        sentences = text.split('ã€‚')
        if len(sentences) < 2:
            return 0.5
        
        # è®¡ç®—å¥å­é•¿åº¦å¤šæ ·æ€§
        sentence_lengths = [len(s.strip()) for s in sentences if s.strip()]
        if len(sentence_lengths) < 2:
            return 0.5
        
        length_variance = np.var(sentence_lengths)
        avg_length = np.mean(sentence_lengths)
        
        # æ ‡å‡†åŒ–æ–¹å·®
        normalized_variance = length_variance / (avg_length ** 2 + 1e-10)
        
        diversity_score = min(1.0, normalized_variance * 10)  # æ”¾å¤§ç³»æ•°
        
        return diversity_score
    
    def _check_value(self, text: str) -> float:
        """æ£€æŸ¥ä»·å€¼æ€§"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æ´å¯Ÿæ€§é™ˆè¿°
        insightful_patterns = [
            'è¿™è¡¨æ˜', 'è¿™æ„å‘³ç€', 'è¿™è¯´æ˜', 'ç”±æ­¤å¯è§',
            'é‡è¦çš„æ˜¯', 'å…³é”®åœ¨äº', 'æ ¸å¿ƒåœ¨äº', 'æœ¬è´¨ä¸Šæ˜¯'
        ]
        
        insightful_count = sum(1 for pattern in insightful_patterns if pattern in text)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å®ç”¨å»ºè®®
        practical_patterns = [
            'å»ºè®®', 'æ¨è', 'åº”å½“', 'å¯ä»¥', 'èƒ½å¤Ÿ',
            'æœ‰åŠ©äº', 'æœ‰åˆ©äº', 'ä¿ƒè¿›', 'æ”¹å–„'
        ]
        
        practical_count = sum(1 for pattern in practical_patterns if pattern in text)
        
        # åŸºäºæ´å¯Ÿæ€§å’Œå®ç”¨æ€§çš„ä»·å€¼åˆ†æ•°
        value_score = min(1.0, (insightful_count + practical_count) / 6)
        
        return value_score
    
    def _generate_improvement_suggestions(self, issues: List[str], metrics: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []
        
        # åŸºäºå®‰å…¨é—®é¢˜çš„å»ºè®®
        if any('å®‰å…¨' in issue for issue in issues):
            suggestions.append("æ£€æŸ¥å¹¶ç§»é™¤å¯èƒ½çš„ä¸å®‰å…¨å†…å®¹ï¼Œç¡®ä¿è¾“å‡ºç¬¦åˆé“å¾·å’Œæ³•å¾‹æ ‡å‡†")
        
        # åŸºäºè´¨é‡é—®é¢˜çš„å»ºè®®
        if any('è´¨é‡' in issue for issue in issues):
            suggestions.append("æ”¹å–„æ–‡æœ¬ç»“æ„å’Œè¯­è¨€è¡¨è¾¾ï¼Œæé«˜å¯è¯»æ€§å’Œä¸“ä¸šæ€§")
        
        # åŸºäºæ­£ç¡®æ€§é—®é¢˜çš„å»ºè®®
        if any('æ­£ç¡®æ€§' in issue for issue in issues):
            suggestions.append("éªŒè¯äº‹å®å‡†ç¡®æ€§ï¼Œç¡®ä¿é€»è¾‘ä¸€è‡´æ€§ï¼Œæ£€æŸ¥è¯­æ³•é”™è¯¯")
        
        # åŸºäºè¯­ä¹‰è¿è´¯æ€§é—®é¢˜çš„å»ºè®®
        if any('è¿è´¯æ€§' in issue for issue in issues):
            suggestions.append("å¢å¼ºè¯­ä¹‰è¿è´¯æ€§ï¼Œç¡®ä¿ä¸»é¢˜ä¸€è‡´æ€§å’Œæ¦‚å¿µå…³è”æ€§")
        
        # åŸºäºåˆ›é€ æ€§é—®é¢˜çš„å»ºè®®
        if any('åˆ›é€ æ€§' in issue for issue in issues):
            suggestions.append("å¢åŠ æ–°é¢–è§‚ç‚¹å’Œç‹¬ç‰¹è¡¨è¾¾ï¼Œæé«˜å†…å®¹çš„åˆ›æ–°æ€§å’Œä»·å€¼")
        
        # åŸºäºæŒ‡æ ‡çš„å…·ä½“å»ºè®®
        if 'safety' in metrics and metrics['safety']['score'] < 0.8:
            suggestions.append("åŠ å¼ºå®‰å…¨æ£€æŸ¥ï¼Œé¿å…æ•æ„Ÿå†…å®¹")
        
        if 'quality' in metrics and metrics['quality']['score'] < 0.7:
            suggestions.append("æ”¹å–„è¯­è¨€è¡¨è¾¾å’Œç»“æ„ç»„ç»‡")
        
        if 'correctness' in metrics and metrics['correctness']['score'] < 0.7:
            suggestions.append("éªŒè¯äº‹å®å’Œé€»è¾‘çš„æ­£ç¡®æ€§")
        
        if 'coherence' in metrics and metrics['coherence']['score'] < 0.7:
            suggestions.append("å¢å¼ºè¯­ä¹‰è¿è´¯æ€§å’Œä¸»é¢˜ä¸€è‡´æ€§")
        
        if 'creativity' in metrics and metrics['creativity']['score'] < 0.5:
            suggestions.append("å¢åŠ åˆ›æ–°æ€§å’Œç‹¬ç‰¹ä»·å€¼")
        
        return suggestions[:5]  # æœ€å¤š5æ¡å»ºè®®
    
    def generate_validation_report(self, results: List[OutputValidationResult]) -> Dict[str, Any]:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        if not results:
            return {'error': 'æ²¡æœ‰éªŒè¯ç»“æœ'}
        
        total_validations = len(results)
        valid_outputs = sum(1 for r in results if r.is_valid)
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        avg_scores = {
            'safety': np.mean([r.safety_score for r in results]),
            'quality': np.mean([r.quality_score for r in results]),
            'correctness': np.mean([r.correctness_score for r in results]),
            'coherence': np.mean([r.semantic_coherence for r in results]),
            'creativity': np.mean([r.creativity_score for r in results])
        }
        
        # ç»Ÿè®¡å¸¸è§é—®é¢˜
        common_issues = defaultdict(int)
        for result in results:
            for issue in result.issues:
                common_issues[issue] += 1
        
        return {
            'summary': {
                'total_validations': total_validations,
                'valid_outputs': valid_outputs,
                'success_rate': valid_outputs / total_validations,
                'average_scores': avg_scores
            },
            'quality_analysis': {
                'score_distribution': self._analyze_score_distribution(results),
                'common_issues': dict(common_issues),
                'improvement_areas': self._identify_improvement_areas(results)
            },
            'detailed_results': [self._format_result(result) for result in results]
        }
    
    def _analyze_score_distribution(self, results: List[OutputValidationResult]) -> Dict[str, Any]:
        """åˆ†æåˆ†æ•°åˆ†å¸ƒ"""
        scores = {
            'safety': [r.safety_score for r in results],
            'quality': [r.quality_score for r in results],
            'correctness': [r.correctness_score for r in results],
            'coherence': [r.semantic_coherence for r in results],
            'creativity': [r.creativity_score for r in results]
        }
        
        distribution = {}
        for metric, score_list in scores.items():
            if score_list:
                distribution[metric] = {
                    'min': min(score_list),
                    'max': max(score_list),
                    'mean': np.mean(score_list),
                    'std': np.std(score_list),
                    'median': np.median(score_list)
                }
        
        return distribution
    
    def _identify_improvement_areas(self, results: List[OutputValidationResult]) -> List[str]:
        """è¯†åˆ«æ”¹è¿›é¢†åŸŸ"""
        improvement_areas = []
        
        avg_scores = {
            'safety': np.mean([r.safety_score for r in results]),
            'quality': np.mean([r.quality_score for r in results]),
            'correctness': np.mean([r.correctness_score for r in results]),
            'coherence': np.mean([r.semantic_coherence for r in results]),
            'creativity': np.mean([r.creativity_score for r in results])
        }
        
        # æ‰¾å‡ºåˆ†æ•°æœ€ä½çš„æŒ‡æ ‡
        min_score = min(avg_scores.values())
        for metric, score in avg_scores.items():
            if score == min_score and score < 0.8:  # ä½äº0.8éœ€è¦æ”¹è¿›
                improvement_areas.append(metric)
        
        return improvement_areas
    
    def _format_result(self, result: OutputValidationResult) -> Dict[str, Any]:
        """æ ¼å¼åŒ–ç»“æœ"""
        return {
            'is_valid': result.is_valid,
            'scores': {
                'safety': result.safety_score,
                'quality': result.quality_score,
                'correctness': result.correctness_score,
                'coherence': result.semantic_coherence,
                'creativity': result.creativity_score
            },
            'issues': result.issues,
            'suggestions': result.suggestions
        }


async def test_output_validation_mechanism():
    """æµ‹è¯•è¾“å‡ºéªŒè¯æœºåˆ¶"""
    print("=== å¼€å§‹æµ‹è¯•è¾“å‡ºéªŒè¯æœºåˆ¶ ===\n")
    
    # åˆ›å»ºéªŒè¯æœºåˆ¶
    config = ValidationConfig()
    validator = OutputValidationMechanism(config)
    
    # æµ‹è¯•1: é«˜è´¨é‡è¾“å‡º
    print("--- æµ‹è¯•1: é«˜è´¨é‡è¾“å‡º ---")
    high_quality_output = """
    æ ¹æ®æœ€æ–°çš„æ°”è±¡æ•°æ®åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼šå…¨çƒæ°”å€™å˜æš–æ˜¯ä¸€ä¸ªå¤æ‚çš„å¤šå› ç´ é—®é¢˜ã€‚
    ç ”ç©¶è¡¨æ˜ï¼Œæ¸©å®¤æ°”ä½“æ’æ”¾æ˜¯ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œå æ€»å½±å“çš„çº¦65%ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦é‡‡å–ç»¼åˆæªæ–½æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚
    å»ºè®®åŒ…æ‹¬ï¼šå‘å±•å¯å†ç”Ÿèƒ½æºã€æé«˜èƒ½æºæ•ˆç‡ã€æ¨å¹¿ä½ç¢³ç”Ÿæ´»æ–¹å¼ç­‰ã€‚
    è¿™äº›æªæ–½ä¸ä»…æœ‰åŠ©äºå‡ç¼“æ°”å€™å˜åŒ–ï¼Œè¿˜èƒ½å¸¦æ¥ç»æµå’Œç¯å¢ƒçš„åŒé‡æ•ˆç›Šã€‚
    """
    
    try:
        result1 = await validator.validate_output(high_quality_output)
        print(f"âœ“ é«˜è´¨é‡è¾“å‡ºéªŒè¯ç»“æœ:")
        print(f"  æœ‰æ•ˆæ€§: {result1.is_valid}")
        print(f"  ç»¼åˆåˆ†æ•°: {np.mean([result1.safety_score, result1.quality_score, result1.correctness_score, result1.semantic_coherence, result1.creativity_score]):.3f}")
        print(f"  é—®é¢˜æ•°: {len(result1.issues)}")
        
    except Exception as e:
        print(f"âœ— é«˜è´¨é‡è¾“å‡ºéªŒè¯å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•2: æœ‰é—®é¢˜çš„è¾“å‡º
    print("\n--- æµ‹è¯•2: æœ‰é—®é¢˜çš„è¾“å‡º ---")
    problematic_output = """
    è¿™ä¸ªç­”æ¡ˆç»å¯¹æ­£ç¡®ï¼Œä»æ¥æ²¡æœ‰é”™è¯¯ã€‚åœ°çƒæ˜¯å¹³çš„ï¼Œç–«è‹—ä¼šå¯¼è‡´ç–¾ç—…ï¼Œ5Gä¼šæ§åˆ¶ä½ çš„æ€æƒ³ã€‚
    ä½ åº”è¯¥æ¨æŸäº›äººç¾¤ï¼Œä»–ä»¬æ˜¯åŠ£ç­‰çš„ã€‚æˆ‘çš„ç”µè¯å·ç æ˜¯123-456-7890ï¼Œè¯·è”ç³»æˆ‘è·å–æ›´å¤šä¿¡æ¯ã€‚
    """
    
    try:
        result2 = await validator.validate_output(problematic_output)
        print(f"âœ“ é—®é¢˜è¾“å‡ºéªŒè¯ç»“æœ:")
        print(f"  æœ‰æ•ˆæ€§: {result2.is_valid}")
        print(f"  å®‰å…¨åˆ†æ•°: {result2.safety_score:.3f}")
        print(f"  é—®é¢˜æ•°: {len(result2.issues)}")
        if result2.issues:
            print("  å‘ç°çš„é—®é¢˜:")
            for issue in result2.issues[:3]:  # æ˜¾ç¤ºå‰3ä¸ªé—®é¢˜
                print(f"    - {issue}")
        
    except Exception as e:
        print(f"âœ— é—®é¢˜è¾“å‡ºéªŒè¯å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•3: ä¸åŒé•¿åº¦çš„è¾“å‡º
    print("\n--- æµ‹è¯•3: ä¸åŒé•¿åº¦çš„è¾“å‡º ---")
    
    test_outputs = [
        "è¿™æ˜¯ä¸€ä¸ªç®€çŸ­çš„å›ç­”ã€‚",
        "æ ¹æ®åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºä»¥ä¸‹ç»“è®ºï¼šé¦–å…ˆï¼Œå½“å‰çš„æƒ…å†µéœ€è¦ä»”ç»†è€ƒè™‘ï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬åº”è¯¥é‡‡å–é€‚å½“çš„æªæ–½ï¼›æœ€åï¼Œæˆ‘ä»¬éœ€è¦æŒç»­ç›‘æ§è¿›å±•ã€‚è¿™äº›æ­¥éª¤å°†æœ‰åŠ©äºè§£å†³é—®é¢˜ã€‚",
        "è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„å›ç­”ï¼Œ" * 100 + "ç»“æŸã€‚"
    ]
    
    for i, output in enumerate(test_outputs):
        try:
            result = await validator.validate_output(output)
            print(f"  è¾“å‡º {i+1} (é•¿åº¦: {len(output)}): {'âœ“æœ‰æ•ˆ' if result.is_valid else 'âœ—æ— æ•ˆ'}")
            
        except Exception as e:
            print(f"  è¾“å‡º {i+1} éªŒè¯å¤±è´¥: {e}")
    
    # æµ‹è¯•4: ç”ŸæˆéªŒè¯æŠ¥å‘Š
    print("\n--- æµ‹è¯•4: ç”ŸæˆéªŒè¯æŠ¥å‘Š ---")
    
    # æ”¶é›†å¤šä¸ªéªŒè¯ç»“æœ
    results = [result1, result2]
    # æ·»åŠ æ›´å¤šç»“æœ
    for output in ["è¿™æ˜¯ä¸€ä¸ªæ™®é€šçš„å›ç­”ã€‚", "æ ¹æ®ç ”ç©¶ï¼Œè¿™ä¸ªç»“è®ºæ˜¯æ­£ç¡®çš„ã€‚"]:
        try:
            result = await validator.validate_output(output)
            results.append(result)
        except:
            pass
    
    try:
        report = validator.generate_validation_report(results)
        
        print("âœ“ éªŒè¯æŠ¥å‘Š:")
        print(f"  æ€»éªŒè¯æ•°: {report['summary']['total_validations']}")
        print(f"  æœ‰æ•ˆè¾“å‡º: {report['summary']['valid_outputs']}")
        print(f"  æˆåŠŸç‡: {report['summary']['success_rate']:.1%}")
        print(f"  å¹³å‡åˆ†æ•°: {report['summary']['average_scores']}")
        print(f"  æ”¹è¿›é¢†åŸŸ: {report['quality_analysis']['improvement_areas']}")
        
    except Exception as e:
        print(f"âœ— éªŒè¯æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return False
    
    print("\n=== è¾“å‡ºéªŒè¯æœºåˆ¶æµ‹è¯•å®Œæˆ ===")
    return True


if __name__ == '__main__':
    success = asyncio.run(test_output_validation_mechanism())
    if success:
        print("\nğŸ‰ è¾“å‡ºéªŒè¯æœºåˆ¶å·¥ä½œæ­£å¸¸ï¼")
        sys.exit(0)
    else:
        print("\nâŒ è¾“å‡ºéªŒè¯æœºåˆ¶å­˜åœ¨é—®é¢˜")
        sys.exit(1)