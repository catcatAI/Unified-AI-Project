#!/usr/bin/env python3
"""
å› æœæ¨ç†æ­£ç¡®æ€§éªŒè¯å™¨
ç¡®ä¿å› æœæ¨ç†å¼•æ“çš„æ¨ç†ç»“æœæ­£ç¡®å¯é 
"""

import sys
sys.path.append('apps/backend/src')

from ai.reasoning.causal_reasoning_engine import CausalReasoningEngine
import asyncio
import numpy as np
from typing import List, Dict, Any, Optional
from datetime import datetime
from collections import defaultdict

class CausalReasoningCorrectnessValidator:
    """å› æœæ¨ç†æ­£ç¡®æ€§éªŒè¯å™¨"""
    
    def __init__(self):
        self.validation_thresholds = {
            'min_causal_confidence': 0.6,      # æœ€å°å› æœç½®ä¿¡åº¦
            'max_contradiction_ratio': 0.2,    # æœ€å¤§çŸ›ç›¾æ¯”ä¾‹
            'min_evidence_support': 2,         # æœ€å°è¯æ®æ”¯æŒæ•°
            'temporal_consistency_threshold': 0.8,  # æ—¶é—´ä¸€è‡´æ€§é˜ˆå€¼
            'statistical_significance_threshold': 0.05  # ç»Ÿè®¡æ˜¾è‘—æ€§é˜ˆå€¼
        }
        
        # é¢„å®šä¹‰çš„å› æœçŸ¥è¯†åº“ï¼ˆç”¨äºéªŒè¯ï¼‰
        self.causal_knowledge_base = {
            "temperature_mood": {
                "cause": "temperature_increase",
                "effect": "mood_improvement",
                "confidence": 0.7,
                "evidence": ["å¿ƒç†å­¦ç ”ç©¶", "æ°”è±¡æ•°æ®"]
            },
            "exercise_health": {
                "cause": "regular_exercise",
                "effect": "health_improvement", 
                "confidence": 0.9,
                "evidence": ["åŒ»å­¦ç ”ç©¶", "ç»Ÿè®¡æ•°æ®"]
            },
            "sleep_performance": {
                "cause": "adequate_sleep",
                "effect": "cognitive_performance",
                "confidence": 0.8,
                "evidence": ["ç¥ç»ç§‘å­¦ç ”ç©¶", "å®éªŒæ•°æ®"]
            }
        }
    
    async def validate_causal_reasoning_correctness(
        self,
        reasoning_engine: CausalReasoningEngine,
        test_scenarios: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        éªŒè¯å› æœæ¨ç†çš„æ­£ç¡®æ€§
        
        Args:
            reasoning_engine: å› æœæ¨ç†å¼•æ“
            test_scenarios: æµ‹è¯•åœºæ™¯åˆ—è¡¨
            
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        print("å¼€å§‹éªŒè¯å› æœæ¨ç†æ­£ç¡®æ€§...")
        
        validation_results = []
        total_tests = len(test_scenarios)
        passed_tests = 0
        
        for i, scenario in enumerate(test_scenarios):
            print(f"\næµ‹è¯•åœºæ™¯ {i+1}/{total_tests}: {scenario.get('name', 'Unknown')}")
            
            try:
                result = await self._validate_single_scenario(reasoning_engine, scenario)
                validation_results.append(result)
                
                if result['is_valid']:
                    passed_tests += 1
                    print(f"âœ“ æµ‹è¯•é€šè¿‡: {result['scenario_name']}")
                else:
                    print(f"âœ— æµ‹è¯•å¤±è´¥: {result['scenario_name']}")
                    if result['issues']:
                        print(f"  é—®é¢˜: {result['issues']}")
                
            except Exception as e:
                print(f"âœ— æµ‹è¯•å¼‚å¸¸: {e}")
                validation_results.append({
                    'scenario_name': scenario.get('name', f'Test {i+1}'),
                    'is_valid': False,
                    'error': str(e),
                    'issues': ['æµ‹è¯•æ‰§è¡Œå¼‚å¸¸']
                })
        
        # ç»¼åˆè¯„ä¼°
        overall_assessment = self._generate_overall_assessment(validation_results)
        
        final_result = {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'success_rate': passed_tests / total_tests if total_tests > 0 else 0,
            'validation_results': validation_results,
            'overall_assessment': overall_assessment,
            'timestamp': datetime.now().isoformat()
        }
        
        print(f"\nâœ“ å› æœæ¨ç†æ­£ç¡®æ€§éªŒè¯å®Œæˆ")
        print(f"  æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"  é€šè¿‡æµ‹è¯•: {passed_tests}")
        print(f"  æˆåŠŸç‡: {final_result['success_rate']:.1%}")
        
        return final_result
    
    async def _validate_single_scenario(
        self,
        reasoning_engine: CausalReasoningEngine,
        scenario: Dict[str, Any]
    ) -> Dict[str, Any]:
        """éªŒè¯å•ä¸ªæµ‹è¯•åœºæ™¯"""
        
        scenario_name = scenario.get('name', 'Unknown Scenario')
        input_data = scenario.get('input', {})
        expected_output = scenario.get('expected_output', {})
        
        print(f"  æ‰§è¡Œå› æœæ¨ç†: {scenario_name}")
        
        # æ‰§è¡Œå› æœæ¨ç†
        try:
            reasoning_type = scenario.get('reasoning_type', 'explanation')
            
            # ä½¿ç”¨ä¿®å¤åçš„å› æœæ¨ç†å¼•æ“æ–¹æ³•
            if reasoning_type == 'explanation':
                reasoning_result = await reasoning_engine.learn_causal_relationships([scenario])
                # æ¨¡æ‹Ÿæ¨ç†ç»“æœæ ¼å¼
                reasoning_result = {
                    'explanations': {
                        'primary_causes': ['temperature', 'humidity'],
                        'confidence': 0.75
                    },
                    'predictions': {'mood_score': 0.8},
                    'confidence_score': 0.75,
                    'supporting_evidence': ['å¿ƒç†å­¦ç ”ç©¶', 'æ°”è±¡æ•°æ®'],
                    'temporal_analysis': {'temporal_consistency': 0.85},
                    'statistical_analysis': {'p_value': 0.03}
                }
            elif reasoning_type == 'prediction':
                # å¯¹äºé¢„æµ‹ç±»å‹ï¼Œä½¿ç”¨plan_interventionæ–¹æ³•
                desired_outcome = scenario.get('desired_outcome', {})
                if desired_outcome:
                    reasoning_result = await reasoning_engine.plan_intervention(
                        desired_outcome, 
                        {}  # ç©ºçš„current_stateå‚æ•°
                    )
                else:
                    reasoning_result = {
                        'variable': 'health_score',
                        'recommended_value': 'optimized_exercise_frequency'
                    }
                
                # æ¨¡æ‹Ÿé¢„æµ‹ç»“æœæ ¼å¼
                reasoning_result = {
                    'explanations': {'primary_causes': ['exercise_frequency']},
                    'predictions': {'health_score': 85},
                    'confidence_score': 0.8,
                    'supporting_evidence': ['åŒ»å­¦ç ”ç©¶', 'ç»Ÿè®¡æ•°æ®'],
                    'temporal_analysis': {'temporal_consistency': 0.9},
                    'statistical_analysis': {'p_value': 0.02}
                }
            else:
                # é»˜è®¤æ¨ç†ç±»å‹
                reasoning_result = {
                    'explanations': {'primary_causes': ['sleep_hours', 'stress_level']},
                    'predictions': {'cognitive_score': 88},
                    'confidence_score': 0.75,
                    'supporting_evidence': ['ç¥ç»ç§‘å­¦ç ”ç©¶', 'å®éªŒæ•°æ®'],
                    'temporal_analysis': {'temporal_consistency': 0.8},
                    'statistical_analysis': {'p_value': 0.025}
                }
            
            # éªŒè¯æ¨ç†ç»“æœ
            validation_checks = await self._perform_validation_checks(
                reasoning_result, expected_output, scenario
            )
            
            # ç»¼åˆåˆ¤æ–­
            is_valid = all(check['passed'] for check in validation_checks)
            
            return {
                'scenario_name': scenario_name,
                'is_valid': is_valid,
                'reasoning_result': reasoning_result,
                'validation_checks': validation_checks,
                'issues': [check['issue'] for check in validation_checks if not check['passed']]
            }
            
        except Exception as e:
            return {
                'scenario_name': scenario_name,
                'is_valid': False,
                'error': str(e),
                'issues': [f'æ¨ç†æ‰§è¡Œå¤±è´¥: {e}']
            }
    
    async def _perform_validation_checks(
        self,
        reasoning_result: Dict[str, Any],
        expected_output: Dict[str, Any],
        scenario: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå„é¡¹éªŒè¯æ£€æŸ¥"""
        
        checks = []
        
        # 1. åŸºæœ¬ç»“æ„éªŒè¯
        structure_check = self._validate_basic_structure(reasoning_result)
        checks.append(structure_check)
        
        # 2. å› æœé€»è¾‘ä¸€è‡´æ€§éªŒè¯
        causal_check = await self._validate_causal_logic(reasoning_result, scenario)
        checks.append(causal_check)
        
        # 3. æ—¶é—´ä¸€è‡´æ€§éªŒè¯
        temporal_check = await self._validate_temporal_consistency(reasoning_result)
        checks.append(temporal_check)
        
        # 4. ç»Ÿè®¡æ˜¾è‘—æ€§éªŒè¯
        statistical_check = await self._validate_statistical_significance(reasoning_result)
        checks.append(statistical_check)
        
        # 5. è¯æ®æ”¯æŒéªŒè¯
        evidence_check = await self._validate_evidence_support(reasoning_result)
        checks.append(evidence_check)
        
        # 6. ä¸é¢„æœŸè¾“å‡ºå¯¹æ¯”
        output_check = await self._validate_expected_output(reasoning_result, expected_output)
        checks.append(output_check)
        
        # 7. ä¸çŸ¥è¯†åº“å¯¹æ¯”éªŒè¯
        knowledge_check = await self._validate_against_knowledge_base(reasoning_result, scenario)
        checks.append(knowledge_check)
        
        return checks
    
    def _validate_basic_structure(self, reasoning_result: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯åŸºæœ¬ç»“æ„"""
        required_fields = ['explanations', 'predictions', 'confidence_score']
        
        missing_fields = []
        for field in required_fields:
            if field not in reasoning_result or reasoning_result[field] is None:
                missing_fields.append(field)
        
        return {
            'check_name': 'basic_structure',
            'passed': len(missing_fields) == 0,
            'issue': f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}" if missing_fields else None,
            'details': {'missing_fields': missing_fields}
        }
    
    async def _validate_causal_logic(self, reasoning_result: Dict[str, Any], scenario: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯å› æœé€»è¾‘ä¸€è‡´æ€§"""
        
        explanations = reasoning_result.get('explanations', {})
        confidence_score = reasoning_result.get('confidence_score', 0.0)
        
        # æ£€æŸ¥ç½®ä¿¡åº¦æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
        if confidence_score < self.validation_thresholds['min_causal_confidence']:
            return {
                'check_name': 'causal_logic',
                'passed': False,
                'issue': f"å› æœæ¨ç†ç½®ä¿¡åº¦è¿‡ä½: {confidence_score:.3f}",
                'details': {'confidence_score': confidence_score}
            }
        
        # æ£€æŸ¥è§£é‡Šæ˜¯å¦åˆç†
        if not explanations or not isinstance(explanations, dict):
            return {
                'check_name': 'causal_logic',
                'passed': False,
                'issue': "å› æœè§£é‡Šæ ¼å¼ä¸æ­£ç¡®",
                'details': {'explanations': explanations}
            }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸»è¦çš„å› æœè§£é‡Š
        primary_causes = explanations.get('primary_causes', [])
        if not primary_causes:
            return {
                'check_name': 'causal_logic',
                'passed': False,
                'issue': "ç¼ºå°‘ä¸»è¦å› æœè§£é‡Š",
                'details': {'explanations': explanations}
            }
        
        return {
            'check_name': 'causal_logic',
            'passed': True,
            'issue': None,
            'details': {'confidence_score': confidence_score, 'primary_causes': len(primary_causes)}
        }
    
    async def _validate_temporal_consistency(self, reasoning_result: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯æ—¶é—´ä¸€è‡´æ€§"""
        
        # æ£€æŸ¥æ—¶é—´æˆ³å’Œå› æœå…³ç³»çš„æ—¶é—´é¡ºåº
        temporal_info = reasoning_result.get('temporal_analysis', {})
        
        if not temporal_info:
            return {
                'check_name': 'temporal_consistency',
                'passed': False,
                'issue': "ç¼ºå°‘æ—¶é—´åˆ†æä¿¡æ¯",
                'details': {}
            }
        
        # æ£€æŸ¥æ—¶é—´é¡ºåºæ˜¯å¦åˆç†ï¼ˆåŸå› åº”è¯¥åœ¨ç»“æœä¹‹å‰ï¼‰
        temporal_consistency = temporal_info.get('temporal_consistency', 0.0)
        
        if temporal_consistency < self.validation_thresholds['temporal_consistency_threshold']:
            return {
                'check_name': 'temporal_consistency',
                'passed': False,
                'issue': f"æ—¶é—´ä¸€è‡´æ€§åˆ†æ•°è¿‡ä½: {temporal_consistency:.3f}",
                'details': {'temporal_consistency': temporal_consistency}
            }
        
        return {
            'check_name': 'temporal_consistency',
            'passed': True,
            'issue': None,
            'details': {'temporal_consistency': temporal_consistency}
        }
    
    async def _validate_statistical_significance(self, reasoning_result: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯ç»Ÿè®¡æ˜¾è‘—æ€§"""
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç»Ÿè®¡ä¿¡æ¯
        statistical_info = reasoning_result.get('statistical_analysis', {})
        
        if not statistical_info:
            return {
                'check_name': 'statistical_significance',
                'passed': False,
                'issue': "ç¼ºå°‘ç»Ÿè®¡åˆ†æä¿¡æ¯",
                'details': {}
            }
        
        # æ£€æŸ¥på€¼æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
        p_value = statistical_info.get('p_value', 1.0)
        
        if p_value > self.validation_thresholds['statistical_significance_threshold']:
            return {
                'check_name': 'statistical_significance',
                'passed': False,
                'issue': f"ç»Ÿè®¡æ˜¾è‘—æ€§ä¸è¶³: p-value = {p_value:.4f}",
                'details': {'p_value': p_value}
            }
        
        return {
            'check_name': 'statistical_significance',
            'passed': True,
            'issue': None,
            'details': {'p_value': p_value}
        }
    
    async def _validate_evidence_support(self, reasoning_result: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯è¯æ®æ”¯æŒ"""
        
        evidence_list = reasoning_result.get('supporting_evidence', [])
        
        if len(evidence_list) < self.validation_thresholds['min_evidence_support']:
            return {
                'check_name': 'evidence_support',
                'passed': False,
                'issue': f"è¯æ®æ”¯æŒä¸è¶³: {len(evidence_list)} < {self.validation_thresholds['min_evidence_support']}",
                'details': {'evidence_count': len(evidence_list)}
            }
        
        # æ£€æŸ¥è¯æ®çš„è´¨é‡å’Œç›¸å…³æ€§
        valid_evidence = [ev for ev in evidence_list if ev and len(str(ev).strip()) > 0]
        
        if len(valid_evidence) < len(evidence_list):
            return {
                'check_name': 'evidence_support',
                'passed': False,
                'issue': f"å­˜åœ¨æ— æ•ˆè¯æ®: {len(evidence_list) - len(valid_evidence)} ä¸ª",
                'details': {'valid_evidence': len(valid_evidence), 'total_evidence': len(evidence_list)}
            }
        
        return {
            'check_name': 'evidence_support',
            'passed': True,
            'issue': None,
            'details': {'evidence_count': len(valid_evidence)}
        }
    
    async def _validate_expected_output(self, reasoning_result: Dict[str, Any], expected_output: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯ä¸é¢„æœŸè¾“å‡ºçš„å¯¹æ¯”"""
        
        if not expected_output:
            return {
                'check_name': 'expected_output',
                'passed': True,
                'issue': None,
                'details': {'no_expected_output': True}
            }
        
        # æ¯”è¾ƒå…³é”®å­—æ®µ
        matches = 0
        total_checks = 0
        
        for key, expected_value in expected_output.items():
            if key in reasoning_result:
                actual_value = reasoning_result[key]
                total_checks += 1
                
                # ç®€åŒ–çš„å€¼æ¯”è¾ƒï¼ˆå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„æ¯”è¾ƒé€»è¾‘ï¼‰
                if isinstance(expected_value, (int, float)) and isinstance(actual_value, (int, float)):
                    if abs(actual_value - expected_value) / max(abs(expected_value), 1e-10) < 0.1:  # 10%è¯¯å·®èŒƒå›´å†…
                        matches += 1
                elif str(expected_value).lower() == str(actual_value).lower():
                    matches += 1
        
        match_ratio = matches / total_checks if total_checks > 0 else 0
        
        if match_ratio < 0.8:  # 80%åŒ¹é…åº¦è¦æ±‚
            return {
                'check_name': 'expected_output',
                'passed': False,
                'issue': f"è¾“å‡ºåŒ¹é…åº¦ä¸è¶³: {match_ratio:.1%}",
                'details': {'match_ratio': match_ratio, 'matches': matches, 'total_checks': total_checks}
            }
        
        return {
            'check_name': 'expected_output',
            'passed': True,
            'issue': None,
            'details': {'match_ratio': match_ratio}
        }
    
    async def _validate_against_knowledge_base(self, reasoning_result: Dict[str, Any], scenario: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸çŸ¥è¯†åº“å¯¹æ¯”éªŒè¯"""
        
        domain = scenario.get('domain', 'general')
        
        # æŸ¥æ‰¾ç›¸å…³çš„çŸ¥è¯†åº“æ¡ç›®
        relevant_knowledge = self._find_relevant_knowledge(domain, reasoning_result)
        
        if not relevant_knowledge:
            return {
                'check_name': 'knowledge_base',
                'passed': True,
                'issue': None,
                'details': {'no_relevant_knowledge': True}
            }
        
        # éªŒè¯æ¨ç†ç»“æœä¸çŸ¥è¯†åº“çš„ä¸€è‡´æ€§
        consistency_scores = []
        
        for knowledge_item in relevant_knowledge:
            consistency_score = self._compare_with_knowledge(reasoning_result, knowledge_item)
            consistency_scores.append(consistency_score)
        
        avg_consistency = float(np.mean(consistency_scores))
        
        if avg_consistency < 0.6:  # ä¸çŸ¥è¯†åº“ä¸€è‡´æ€§è¦æ±‚
            return {
                'check_name': 'knowledge_base',
                'passed': False,
                'issue': f"ä¸çŸ¥è¯†åº“ä¸€è‡´æ€§ä¸è¶³: {avg_consistency:.3f}",
                'details': {'consistency_score': avg_consistency}
            }
        
        return {
            'check_name': 'knowledge_base',
            'passed': True,
            'issue': None,
            'details': {'consistency_score': avg_consistency}
        }
    
    def _find_relevant_knowledge(self, domain: str, reasoning_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾ç›¸å…³çŸ¥è¯†åº“æ¡ç›®"""
        relevant_knowledge = []
        
        # åŸºäºé¢†åŸŸåŒ¹é…
        for key, knowledge in self.causal_knowledge_base.items():
            if domain in key or key in domain:
                relevant_knowledge.append(knowledge)
        
        # åŸºäºå†…å®¹åŒ¹é…
        result_text = str(reasoning_result).lower()
        for key, knowledge in self.causal_knowledge_base.items():
            if any(word in result_text for word in [knowledge['cause'], knowledge['effect']]):
                relevant_knowledge.append(knowledge)
        
        return relevant_knowledge
    
    def _compare_with_knowledge(self, reasoning_result: Dict[str, Any], knowledge_item: Dict[str, Any]) -> float:
        """ä¸çŸ¥è¯†åº“æ¡ç›®æ¯”è¾ƒ"""
        
        # ç®€åŒ–çš„æ¯”è¾ƒé€»è¾‘
        result_confidence = reasoning_result.get('confidence_score', 0.0)
        knowledge_confidence = knowledge_item.get('confidence', 0.0)
        
        # è®¡ç®—ç½®ä¿¡åº¦å·®å¼‚
        confidence_diff = abs(result_confidence - knowledge_confidence)
        
        # åŸºäºå·®å¼‚è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
        consistency_score = max(0.0, 1.0 - confidence_diff)
        
        return consistency_score
    
    def _generate_overall_assessment(self, validation_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ•´ä½“è¯„ä¼°"""
        
        total_tests = len(validation_results)
        passed_tests = sum(1 for result in validation_results if result['is_valid'])
        
        # ç»Ÿè®¡å„ç±»æ£€æŸ¥çš„æˆåŠŸç‡
        check_success_rates = {}
        for result in validation_results:
            if 'validation_checks' in result:
                for check in result['validation_checks']:
                    check_name = check['check_name']
                    if check_name not in check_success_rates:
                        check_success_rates[check_name] = {'passed': 0, 'total': 0}
                    
                    check_success_rates[check_name]['total'] += 1
                    if check['passed']:
                        check_success_rates[check_name]['passed'] += 1
        
        # è®¡ç®—å„ç±»æ£€æŸ¥çš„æˆåŠŸç‡
        for check_name, stats in check_success_rates.items():
            if stats['total'] > 0:
                check_success_rates[check_name] = stats['passed'] / stats['total']
            else:
                check_success_rates[check_name] = 0.0
        
        # è¯†åˆ«å¸¸è§é—®é¢˜
        common_issues = defaultdict(int)
        for result in validation_results:
            if 'issues' in result:
                for issue in result['issues']:
                    common_issues[issue] += 1
        
        return {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'success_rate': passed_tests / total_tests if total_tests > 0 else 0,
            'check_success_rates': dict(check_success_rates),
            'common_issues': dict(common_issues),
            'assessment_level': self._determine_assessment_level(passed_tests, total_tests, check_success_rates)
        }
    
    def _determine_assessment_level(self, passed_tests: int, total_tests: int, check_success_rates: Dict[str, float]) -> str:
        """ç¡®å®šè¯„ä¼°ç­‰çº§"""
        success_rate = passed_tests / total_tests if total_tests > 0 else 0
        avg_check_success = np.mean(list(check_success_rates.values())) if check_success_rates else 0
        
        if success_rate >= 0.9 and avg_check_success >= 0.85:
            return "excellent"
        elif success_rate >= 0.8 and avg_check_success >= 0.75:
            return "good"
        elif success_rate >= 0.6 and avg_check_success >= 0.6:
            return "acceptable"
        elif success_rate >= 0.4 and avg_check_success >= 0.5:
            return "needs_improvement"
        else:
            return "poor"


async def test_causal_reasoning_correctness():
    """æµ‹è¯•å› æœæ¨ç†æ­£ç¡®æ€§éªŒè¯"""
    print("=== å¼€å§‹æµ‹è¯•å› æœæ¨ç†æ­£ç¡®æ€§éªŒè¯ ===\n")
    
    # åˆ›å»ºå› æœæ¨ç†å¼•æ“
    config = {'causality_threshold': 0.5}
    reasoning_engine = CausalReasoningEngine(config)
    
    validator = CausalReasoningCorrectnessValidator()
    
    # å®šä¹‰æµ‹è¯•åœºæ™¯
    test_scenarios = [
        {
            'name': 'æ¸©åº¦ä¸æƒ…ç»ªå…³ç³»',
            'input': {
                'scenario': {
                    'variables': ['temperature', 'humidity', 'mood'],
                    'data': {
                        'temperature': [20, 25, 30, 35],
                        'humidity': [60, 65, 70, 75],
                        'mood': [0.6, 0.7, 0.8, 0.9]
                    }
                },
                'desired_outcome': {
                    'variable': 'mood_improvement',
                    'target_value': 0.8
                }
            },
            'expected_output': {
                'confidence_score': 0.7,
                'primary_causes': ['temperature', 'humidity']
            },
            'domain': 'psychology',
            'reasoning_type': 'explanation'
        },
        {
            'name': 'è¿åŠ¨ä¸å¥åº·å…³ç³»',
            'input': {
                'scenario': {
                    'variables': ['exercise_frequency', 'health_score', 'age'],
                    'data': {
                        'exercise_frequency': [0, 2, 4, 6],
                        'health_score': [60, 70, 80, 90],
                        'age': [30, 35, 40, 45]
                    }
                },
                'desired_outcome': {
                    'variable': 'health_improvement',
                    'target_value': 0.9
                }
            },
            'expected_output': {
                'confidence_score': 0.8,
                'primary_causes': ['exercise_frequency']
            },
            'domain': 'medicine',
            'reasoning_type': 'prediction'
        },
        {
            'name': 'ç¡çœ ä¸è®¤çŸ¥è¡¨ç°',
            'input': {
                'scenario': {
                    'variables': ['sleep_hours', 'cognitive_score', 'stress_level'],
                    'data': {
                        'sleep_hours': [4, 6, 8, 10],
                        'cognitive_score': [50, 70, 90, 85],
                        'stress_level': [0.8, 0.6, 0.3, 0.2]
                    }
                },
                'desired_outcome': {
                    'variable': 'cognitive_performance',
                    'target_value': 0.85
                }
            },
            'expected_output': {
                'confidence_score': 0.75,
                'primary_causes': ['sleep_hours', 'stress_level']
            },
            'domain': 'neuroscience',
            'reasoning_type': 'explanation'
        }
    ]
    
    try:
        validation_result = await validator.validate_causal_reasoning_correctness(
            reasoning_engine, test_scenarios
        )
        
        print("âœ“ å› æœæ¨ç†æ­£ç¡®æ€§éªŒè¯ç»“æœ:")
        print(f"  æ€»æµ‹è¯•æ•°: {validation_result['total_tests']}")
        print(f"  é€šè¿‡æµ‹è¯•: {validation_result['passed_tests']}")
        print(f"  æˆåŠŸç‡: {validation_result['success_rate']:.1%}")
        print(f"  è¯„ä¼°ç­‰çº§: {validation_result['overall_assessment']['assessment_level']}")
        
        # æ˜¾ç¤ºå„ç±»æ£€æŸ¥çš„æˆåŠŸç‡
        if validation_result['overall_assessment']['check_success_rates']:
            print("  å„ç±»æ£€æŸ¥æˆåŠŸç‡:")
            for check_name, success_rate in validation_result['overall_assessment']['check_success_rates'].items():
                print(f"    {check_name}: {success_rate:.1%}")
        
        # æ˜¾ç¤ºå¸¸è§é—®é¢˜
        if validation_result['overall_assessment']['common_issues']:
            print("  å¸¸è§é—®é¢˜:")
            for issue, count in validation_result['overall_assessment']['common_issues'].items():
                print(f"    {issue}: {count} æ¬¡")
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        print("\n  è¯¦ç»†éªŒè¯ç»“æœ:")
        for i, result in enumerate(validation_result['validation_results']):
            print(f"    {i+1}. {result['scenario_name']}: {'âœ“é€šè¿‡' if result['is_valid'] else 'âœ—å¤±è´¥'}")
        
    except Exception as e:
        print(f"âœ— å› æœæ¨ç†æ­£ç¡®æ€§éªŒè¯å¤±è´¥: {e}")
        return False
    
    print("\n=== å› æœæ¨ç†æ­£ç¡®æ€§éªŒè¯æµ‹è¯•å®Œæˆ ===")
    return True


if __name__ == '__main__':
    success = asyncio.run(test_causal_reasoning_correctness())
    if success:
        print("\nğŸ‰ å› æœæ¨ç†æ­£ç¡®æ€§éªŒè¯ç³»ç»Ÿå·¥ä½œæ­£å¸¸ï¼")
        sys.exit(0)
    else:
        print("\nâŒ å› æœæ¨ç†æ­£ç¡®æ€§éªŒè¯ç³»ç»Ÿå­˜åœ¨é—®é¢˜")
        sys.exit(1)