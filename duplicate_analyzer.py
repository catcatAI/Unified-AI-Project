#!/usr/bin/env python3
"""
é‡å¤åŠŸèƒ½åˆ†æå™¨
æ‰«æUnified AI Projectä¸­çš„é‡å¤åŠŸèƒ½æ¨¡å—å’Œç›¸ä¼¼ä»£ç å®ç°
"""

import os
import re
import ast
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import defaultdict
import difflib

class DuplicateAnalyzer:
    def __init__(self):
        self.function_patterns = defaultdict(list)
        self.class_definitions = defaultdict(list)
        self.import_patterns = defaultdict(list)
        self.file_contents = {}
        self.similarity_threshold = 0.8
        
    def scan_project(self, root_path: str) -> Dict:
        """æ‰«ææ•´ä¸ªé¡¹ç›®"""
        root = Path(root_path)
        python_files = list(root.rglob("*.py"))
        
        print(f"æ‰«æ {len(python_files)} ä¸ªPythonæ–‡ä»¶...")
        
        # é™åˆ¶åˆ†æçš„æ–‡ä»¶æ•°é‡ï¼Œä¼˜å…ˆåˆ†æå…³é”®ç›®å½•
        key_dirs = ['apps', 'tools', 'training', 'scripts']
        priority_files = []
        other_files = []
        
        for py_file in python_files:
            if self._should_skip_file(py_file):
                continue
                
            # æ£€æŸ¥æ˜¯å¦åœ¨å…³é”®ç›®å½•ä¸­
            in_key_dir = any(key_dir in str(py_file) for key_dir in key_dirs)
            if in_key_dir or py_file.name.startswith(('check_', 'test_', 'fix_', 'repair', 'enhanced')):
                priority_files.append(py_file)
            else:
                other_files.append(py_file)
        
        # åªåˆ†æä¼˜å…ˆçº§æ–‡ä»¶å’Œå‰1000ä¸ªå…¶ä»–æ–‡ä»¶
        files_to_analyze = priority_files + other_files[:1000]
        print(f"å®é™…åˆ†æ {len(files_to_analyze)} ä¸ªæ–‡ä»¶ï¼ˆä¼˜å…ˆçº§ + å‰1000ä¸ªï¼‰...")
        
        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰å‡½æ•°å’Œç±»å®šä¹‰
        for i, py_file in enumerate(files_to_analyze):
            if i % 100 == 0:
                print(f"è¿›åº¦: {i}/{len(files_to_analyze)} æ–‡ä»¶å·²åˆ†æ")
                
            try:
                content = py_file.read_text(encoding='utf-8')
                self.file_contents[str(py_file)] = content
                self._analyze_file(str(py_file), content)
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•è¯»å–æ–‡ä»¶ {py_file}: {e}")
        
        # ç¬¬äºŒæ­¥ï¼šåˆ†æé‡å¤æ¨¡å¼
        return self._analyze_duplicates()
    
    def _should_skip_file(self, file_path: Path) -> bool:
        """è·³è¿‡ä¸åº”è¯¥åˆ†æçš„æ–‡ä»¶"""
        skip_patterns = [
            '__pycache__', '.git', 'venv/', 'env/', 'node_modules/',
            '.pytest_cache', 'dist/', 'build/', '*.egg-info/'
        ]
        
        path_str = str(file_path)
        return any(pattern in path_str for pattern in skip_patterns)
    
    def _analyze_file(self, file_path: str, content: str):
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    self._analyze_function(file_path, node)
                elif isinstance(node, ast.ClassDef):
                    self._analyze_class(file_path, node)
                elif isinstance(node, ast.Import) or isinstance(node, ast.ImportFrom):
                    self._analyze_import(file_path, node)
                    
        except SyntaxError:
            pass  # è·³è¿‡è¯­æ³•é”™è¯¯çš„æ–‡ä»¶
    
    def _analyze_function(self, file_path: str, func_node: ast.FunctionDef):
        """åˆ†æå‡½æ•°å®šä¹‰"""
        func_name = func_node.name
        
        # æå–å‡½æ•°ç­¾å
        args = [arg.arg for arg in func_node.args.args]
        signature = f"{func_name}({', '.join(args)})"
        
        # æå–å‡½æ•°ä½“çš„å‰å‡ è¡Œä½œä¸ºæ¨¡å¼
        if func_node.body:
            body_start = self._get_node_source(func_node.body[:3]) if len(func_node.body) > 3 else self._get_node_source(func_node.body)
        else:
            body_start = ""
            
        pattern = {
            'file': file_path,
            'name': func_name,
            'signature': signature,
            'args': args,
            'body_start': body_start,
            'line_num': func_node.lineno
        }
        
        self.function_patterns[func_name].append(pattern)
    
    def _analyze_class(self, file_path: str, class_node: ast.ClassDef):
        """åˆ†æç±»å®šä¹‰"""
        class_name = class_node.name
        
        # æå–æ–¹æ³•
        methods = []
        for item in class_node.body:
            if isinstance(item, ast.FunctionDef):
                methods.append(item.name)
        
        pattern = {
            'file': file_path,
            'name': class_name,
            'methods': methods,
            'line_num': class_node.lineno,
            'bases': [self._get_name(base) for base in class_node.bases]
        }
        
        self.class_definitions[class_name].append(pattern)
    
    def _analyze_import(self, file_path: str, import_node):
        """åˆ†æå¯¼å…¥è¯­å¥"""
        if isinstance(import_node, ast.Import):
            for alias in import_node.names:
                self.import_patterns[alias.name].append(file_path)
        elif isinstance(import_node, ast.ImportFrom):
            module = import_node.module or ''
            for alias in import_node.names:
                full_name = f"{module}.{alias.name}" if module else alias.name
                self.import_patterns[full_name].append(file_path)
    
    def _get_name(self, node):
        """è·å–èŠ‚ç‚¹åç§°"""
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            return f"{self._get_name(node.value)}.{node.attr}"
        return str(node)
    
    def _get_node_source(self, nodes):
        """è·å–èŠ‚ç‚¹æºç ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä½¿ç”¨ast.get_source_segment
        return str(nodes)
    
    def _analyze_duplicates(self) -> Dict:
        """åˆ†æé‡å¤æ¨¡å¼"""
        results = {
            'duplicate_functions': [],
            'duplicate_classes': [],
            'similar_files': [],
            'import_duplicates': [],
            'repair_systems': [],
            'check_scripts': [],
            'agent_managers': []
        }
        
        print("åˆ†æé‡å¤æ¨¡å¼...")
        
        # 1. åˆ†æé‡å¤å‡½æ•°ï¼ˆåªåˆ†æå‡ºç°æ¬¡æ•°>1çš„å‡½æ•°ï¼‰
        duplicate_funcs = {k: v for k, v in self.function_patterns.items() if len(v) > 1}
        print(f"å‘ç° {len(duplicate_funcs)} ä¸ªé‡å¤å‡½æ•°æ¨¡å¼")
        
        for func_name, instances in duplicate_funcs.items():
            similarity = self._calculate_group_similarity(instances)
            results['duplicate_functions'].append({
                'name': func_name,
                'count': len(instances),
                'similarity': similarity,
                'instances': instances[:3]  # åªä¿ç•™å‰3ä¸ªå®ä¾‹
            })
        
        # 2. åˆ†æé‡å¤ç±»
        duplicate_classes = {k: v for k, v in self.class_definitions.items() if len(v) > 1}
        print(f"å‘ç° {len(duplicate_classes)} ä¸ªé‡å¤ç±»æ¨¡å¼")
        
        for class_name, instances in duplicate_classes.items():
            similarity = self._calculate_class_similarity(instances)
            results['duplicate_classes'].append({
                'name': class_name,
                'count': len(instances),
                'similarity': similarity,
                'instances': instances[:3]  # åªä¿ç•™å‰3ä¸ªå®ä¾‹
            })
        
        # 3. åˆ†æç›¸ä¼¼æ–‡ä»¶ï¼ˆé™åˆ¶åˆ†ææ•°é‡ï¼‰
        print("åˆ†æç›¸ä¼¼æ–‡ä»¶...")
        results['similar_files'] = self._find_similar_files()
        
        # 4. ç‰¹å®šæ¨¡å¼åˆ†æ
        print("åˆ†æç‰¹å®šæ¨¡å¼...")
        results['repair_systems'] = self._analyze_repair_systems()
        results['check_scripts'] = self._analyze_check_scripts()
        results['agent_managers'] = self._analyze_agent_managers()
        
        return results
    
    def _calculate_group_similarity(self, instances: List[Dict]) -> float:
        """è®¡ç®—å‡½æ•°ç»„ç›¸ä¼¼åº¦"""
        if len(instances) < 2:
            return 0.0
            
        similarities = []
        for i in range(len(instances)):
            for j in range(i + 1, len(instances)):
                sim = self._calculate_function_similarity(instances[i], instances[j])
                similarities.append(sim)
        
        return sum(similarities) / len(similarities) if similarities else 0.0
    
    def _calculate_function_similarity(self, func1: Dict, func2: Dict) -> float:
        """è®¡ç®—ä¸¤ä¸ªå‡½æ•°çš„ç›¸ä¼¼åº¦"""
        # åŸºäºå‡½æ•°åã€å‚æ•°å’Œå‡½æ•°ä½“å¼€å§‹éƒ¨åˆ†è®¡ç®—ç›¸ä¼¼åº¦
        score = 0.0
        
        # å‡½æ•°åç›¸åŒ
        if func1['name'] == func2['name']:
            score += 0.3
            
        # å‚æ•°ç›¸ä¼¼åº¦
        args_sim = difflib.SequenceMatcher(None, str(func1['args']), str(func2['args'])).ratio()
        score += args_sim * 0.3
        
        # å‡½æ•°ä½“ç›¸ä¼¼åº¦
        body_sim = difflib.SequenceMatcher(None, func1['body_start'], func2['body_start']).ratio()
        score += body_sim * 0.4
        
        return score
    
    def _calculate_class_similarity(self, instances: List[Dict]) -> float:
        """è®¡ç®—ç±»ç›¸ä¼¼åº¦"""
        if len(instances) < 2:
            return 0.0
            
        similarities = []
        for i in range(len(instances)):
            for j in range(i + 1, len(instances)):
                sim = self._calculate_single_class_similarity(instances[i], instances[j])
                similarities.append(sim)
        
        return sum(similarities) / len(similarities) if similarities else 0.0
    
    def _calculate_single_class_similarity(self, class1: Dict, class2: Dict) -> float:
        """è®¡ç®—ä¸¤ä¸ªç±»çš„ç›¸ä¼¼åº¦"""
        score = 0.0
        
        # ç±»åç›¸åŒ
        if class1['name'] == class2['name']:
            score += 0.4
            
        # æ–¹æ³•ç›¸ä¼¼åº¦
        methods1 = set(class1['methods'])
        methods2 = set(class2['methods'])
        
        if methods1 or methods2:
            intersection = len(methods1.intersection(methods2))
            union = len(methods1.union(methods2))
            method_sim = intersection / union if union > 0 else 0
            score += method_sim * 0.4
        
        # ç»§æ‰¿å…³ç³»
        if class1['bases'] == class2['bases']:
            score += 0.2
            
        return score
    
    def _find_similar_files(self) -> List[Dict]:
        """æŸ¥æ‰¾ç›¸ä¼¼æ–‡ä»¶"""
        similar_files = []
        file_paths = list(self.file_contents.keys())
        
        # é™åˆ¶æ¯”è¾ƒçš„æ–‡ä»¶æ•°é‡
        max_comparisons = 1000
        comparison_count = 0
        
        for i in range(len(file_paths)):
            if comparison_count >= max_comparisons:
                break
                
            for j in range(i + 1, min(i + 50, len(file_paths))):  # é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„æ¯”è¾ƒæ•°é‡
                if comparison_count >= max_comparisons:
                    break
                    
                content1 = self.file_contents[file_paths[i]]
                content2 = self.file_contents[file_paths[j]]
                
                # å¿«é€Ÿé¢„æ£€æŸ¥ï¼šå¦‚æœæ–‡ä»¶å¤§å°å·®å¼‚å¾ˆå¤§ï¼Œè·³è¿‡è¯¦ç»†æ¯”è¾ƒ
                if abs(len(content1) - len(content2)) > max(len(content1), len(content2)) * 0.5:
                    continue
                
                similarity = difflib.SequenceMatcher(None, content1, content2).ratio()
                
                if similarity > self.similarity_threshold:
                    similar_files.append({
                        'file1': file_paths[i],
                        'file2': file_paths[j],
                        'similarity': similarity
                    })
                
                comparison_count += 1
        
        return similar_files
    
    def _analyze_repair_systems(self) -> List[Dict]:
        """åˆ†æä¿®å¤ç³»ç»Ÿ"""
        repair_keywords = ['repair', 'fix', 'correct', 'heal', 'restore']
        repair_files = []
        
        for file_path, content in self.file_contents.items():
            file_name = os.path.basename(file_path).lower()
            
            # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«ä¿®å¤å…³é”®è¯
            if any(keyword in file_name for keyword in repair_keywords):
                # åˆ†ææ–‡ä»¶å†…å®¹
                lines = content.split('\n')
                class_count = 0
                function_count = 0
                
                for line in lines:
                    if line.strip().startswith('class '):
                        class_count += 1
                    elif line.strip().startswith('def '):
                        function_count += 1
                
                repair_files.append({
                    'file': file_path,
                    'classes': class_count,
                    'functions': function_count,
                    'size': len(content)
                })
        
        return repair_files
    
    def _analyze_check_scripts(self) -> List[Dict]:
        """åˆ†ææ£€æŸ¥è„šæœ¬"""
        check_files = []
        
        for file_path, content in self.file_contents.items():
            file_name = os.path.basename(file_path)
            
            if file_name.startswith('check_') and file_name.endswith('.py'):
                # åˆ†ææ£€æŸ¥è„šæœ¬çš„æ¨¡å¼
                lines = content.split('\n')
                has_ast = 'ast' in content
                has_file_open = 'open(' in content or 'with open' in content
                has_readlines = 'readlines' in content
                
                check_files.append({
                    'file': file_path,
                    'name': file_name,
                    'has_ast': has_ast,
                    'has_file_operations': has_file_open,
                    'has_readlines': has_readlines,
                    'lines': len(lines)
                })
        
        return check_files
    
    def _analyze_agent_managers(self) -> List[Dict]:
        """åˆ†æä»£ç†ç®¡ç†å™¨"""
        manager_files = []
        
        for file_path, content in self.file_contents.items():
            file_name = os.path.basename(file_path).lower()
            
            if 'agent' in file_name and 'manager' in file_name:
                # åˆ†æä»£ç†ç®¡ç†å™¨
                has_subprocess = 'subprocess' in content
                has_asyncio = 'asyncio' in content
                has_threading = 'threading' in content
                
                manager_files.append({
                    'file': file_path,
                    'has_subprocess': has_subprocess,
                    'has_asyncio': has_asyncio,
                    'has_threading': has_threading
                })
        
        return manager_files
    
    def generate_report(self, results: Dict) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("Unified AI Project - é‡å¤åŠŸèƒ½åˆ†ææŠ¥å‘Š")
        report.append("=" * 80)
        report.append("")
        
        # 1. é‡å¤å‡½æ•°
        report.append("ğŸ”§ é‡å¤å‡½æ•°åˆ†æ")
        report.append("-" * 40)
        if results['duplicate_functions']:
            for dup in sorted(results['duplicate_functions'], key=lambda x: x['count'], reverse=True)[:10]:
                report.append(f"å‡½æ•°: {dup['name']} | å‡ºç°æ¬¡æ•°: {dup['count']} | ç›¸ä¼¼åº¦: {dup['similarity']:.2f}")
                for inst in dup['instances'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªå®ä¾‹
                    report.append(f"  ğŸ“ {inst['file']}:{inst['line_num']}")
                report.append("")
        else:
            report.append("æœªå‘ç°æ˜æ˜¾çš„å‡½æ•°é‡å¤")
        report.append("")
        
        # 2. é‡å¤ç±»
        report.append("ğŸ—ï¸ é‡å¤ç±»åˆ†æ")
        report.append("-" * 40)
        if results['duplicate_classes']:
            for dup in sorted(results['duplicate_classes'], key=lambda x: x['count'], reverse=True)[:10]:
                report.append(f"ç±»: {dup['name']} | å‡ºç°æ¬¡æ•°: {dup['count']} | ç›¸ä¼¼åº¦: {dup['similarity']:.2f}")
                for inst in dup['instances'][:3]:
                    report.append(f"  ğŸ“ {inst['file']}:{inst['line_num']}")
                report.append("")
        else:
            report.append("æœªå‘ç°æ˜æ˜¾çš„ç±»é‡å¤")
        report.append("")
        
        # 3. ç›¸ä¼¼æ–‡ä»¶
        report.append("ğŸ“„ ç›¸ä¼¼æ–‡ä»¶åˆ†æ")
        report.append("-" * 40)
        if results['similar_files']:
            for sim in sorted(results['similar_files'], key=lambda x: x['similarity'], reverse=True)[:10]:
                report.append(f"ç›¸ä¼¼åº¦: {sim['similarity']:.2f}")
                report.append(f"  ğŸ“„ {sim['file1']}")
                report.append(f"  ğŸ“„ {sim['file2']}")
                report.append("")
        else:
            report.append("æœªå‘ç°é«˜åº¦ç›¸ä¼¼çš„æ–‡ä»¶")
        report.append("")
        
        # 4. ä¿®å¤ç³»ç»Ÿ
        report.append("ğŸ”¨ ä¿®å¤ç³»ç»Ÿåˆ†æ")
        report.append("-" * 40)
        if results['repair_systems']:
            report.append(f"å‘ç° {len(results['repair_systems'])} ä¸ªä¿®å¤ç›¸å…³æ–‡ä»¶:")
            for repair in sorted(results['repair_systems'], key=lambda x: x['size'], reverse=True)[:10]:
                file_name = os.path.basename(repair['file'])
                report.append(f"  ğŸ”§ {file_name}: {repair['classes']} ç±», {repair['functions']} å‡½æ•°, {repair['size']} å­—èŠ‚")
        report.append("")
        
        # 5. æ£€æŸ¥è„šæœ¬
        report.append("ğŸ” æ£€æŸ¥è„šæœ¬åˆ†æ")
        report.append("-" * 40)
        if results['check_scripts']:
            report.append(f"å‘ç° {len(results['check_scripts'])} ä¸ªæ£€æŸ¥è„šæœ¬:")
            patterns = defaultdict(int)
            for check in results['check_scripts']:
                pattern = f"AST:{check['has_ast']}_File:{check['has_file_operations']}_Lines:{check['lines']}"
                patterns[pattern] += 1
            
            for pattern, count in sorted(patterns.items(), key=lambda x: x[1], reverse=True):
                report.append(f"  ğŸ“‹ æ¨¡å¼ {pattern}: {count} ä¸ªæ–‡ä»¶")
        report.append("")
        
        # 6. ä»£ç†ç®¡ç†å™¨
        report.append("ğŸ¤– ä»£ç†ç®¡ç†å™¨åˆ†æ")
        report.append("-" * 40)
        if results['agent_managers']:
            report.append(f"å‘ç° {len(results['agent_managers'])} ä¸ªä»£ç†ç®¡ç†å™¨:")
            for manager in results['agent_managers']:
                file_name = os.path.basename(manager['file'])
                features = []
                if manager['has_subprocess']: features.append("subprocess")
                if manager['has_asyncio']: features.append("asyncio")
                if manager['has_threading']: features.append("threading")
                report.append(f"  ğŸ¤– {file_name}: {', '.join(features)}")
        report.append("")
        
        # 7. æ•´åˆå»ºè®®
        report.append("ğŸ’¡ æ•´åˆå»ºè®®")
        report.append("-" * 40)
        report.append(self._generate_recommendations(results))
        
        return "\n".join(report)
    
    def _generate_recommendations(self, results: Dict) -> str:
        """ç”Ÿæˆæ•´åˆå»ºè®®"""
        recommendations = []
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®
        if len(results['check_scripts']) > 10:
            recommendations.append("1. ğŸ“‹ æ£€æŸ¥è„šæœ¬æ•´åˆ:")
            recommendations.append("   - åˆå¹¶ç›¸ä¼¼çš„æ£€æŸ¥è„šæœ¬ï¼Œåˆ›å»ºç»Ÿä¸€çš„æ£€æŸ¥æ¡†æ¶")
            recommendations.append("   - æ ‡å‡†åŒ–æ£€æŸ¥è„šæœ¬çš„å‚æ•°å’Œè¾“å‡ºæ ¼å¼")
            recommendations.append("")
        
        if len(results['repair_systems']) > 5:
            recommendations.append("2. ğŸ”¨ ä¿®å¤ç³»ç»Ÿæ•´åˆ:")
            recommendations.append("   - ç»Ÿä¸€ä¿®å¤ç³»ç»Ÿçš„æ¥å£å’Œé…ç½®")
            recommendations.append("   - åˆå¹¶åŠŸèƒ½ç›¸ä¼¼çš„ä¿®å¤ç±»")
            recommendations.append("   - å»ºç«‹ç»Ÿä¸€çš„ä¿®å¤ç­–ç•¥ç®¡ç†å™¨")
            recommendations.append("")
        
        if len(results['agent_managers']) > 3:
            recommendations.append("3. ğŸ¤– ä»£ç†ç®¡ç†å™¨æ•´åˆ:")
            recommendations.append("   - ç»Ÿä¸€ä»£ç†ç”Ÿå‘½å‘¨æœŸç®¡ç†")
            recommendations.append("   - æ ‡å‡†åŒ–ä»£ç†é€šä¿¡åè®®")
            recommendations.append("   - åˆå¹¶é‡å¤çš„ä»£ç†ç®¡ç†åŠŸèƒ½")
            recommendations.append("")
        
        if results['duplicate_functions']:
            recommendations.append("4. ğŸ”§ å‡½æ•°é‡å¤å¤„ç†:")
            recommendations.append("   - æå–å…¬å…±å‡½æ•°åˆ°å·¥å…·æ¨¡å—")
            recommendations.append("   - å»ºç«‹å…±äº«å‡½æ•°åº“")
            recommendations.append("   - ä½¿ç”¨ç»§æ‰¿æˆ–ç»„åˆå‡å°‘é‡å¤")
            recommendations.append("")
        
        if results['similar_files']:
            recommendations.append("5. ğŸ“„ æ–‡ä»¶ç›¸ä¼¼å¤„ç†:")
            recommendations.append("   - åˆå¹¶é«˜åº¦ç›¸ä¼¼çš„æ–‡ä»¶")
            recommendations.append("   - æå–å…¬å…±éƒ¨åˆ†ä¸ºåŸºç±»æˆ–å·¥å…·æ¨¡å—")
            recommendations.append("")
        
        if not recommendations:
            recommendations.append("åŸºäºå½“å‰åˆ†æï¼Œé¡¹ç›®ç»“æ„ç›¸å¯¹è‰¯å¥½ï¼Œé‡å¤åŠŸèƒ½è¾ƒå°‘ã€‚")
        
        return "\n".join(recommendations)

def main():
    """ä¸»å‡½æ•°"""
    analyzer = DuplicateAnalyzer()
    
    print("å¼€å§‹æ‰«æUnified AI Project...")
    results = analyzer.scan_project("D:\\Projects\\Unified-AI-Project")
    
    print("\nç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    report = analyzer.generate_report(results)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = "duplicate_analysis_report.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nåˆ†æå®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    print(report)

if __name__ == "__main__":
    main()