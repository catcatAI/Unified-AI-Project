#!/usr/bin/env python3
"""
åŸºäºtokençš„ç»¼åˆé—®é¢˜æµ‹è¯• - åˆå¹¶aaa.mdä¸mixed_questions_test.md
æ¶ˆé™¤ç¡¬ç¼–ç ï¼Œå®ç°çœŸæ­£çš„tokenå±‚é¢æ€è€ƒï¼Œå¤„ç†å…¨éƒ¨60ä¸ªé—®é¢˜
"""

import asyncio
import sys
import random
import time
import hashlib
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from collections import Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from unified_system_manager_complete_core import get_complete_system_manager, CompleteSystemConfig

class TokenBasedResponseGenerator:
    """åŸºäºtokençš„æ™ºèƒ½å“åº”ç”Ÿæˆå™¨ - çœŸæ­£çš„tokenå±‚é¢æ€è€ƒ"""
    
    def __init__(self):
        self.token_patterns = {
            'philosophical': ['å¹½é»˜', 'é“å¾·', 'æ™ºæ…§', 'ç›´è§‰', 'åˆ›é€ åŠ›', 'çµæ„Ÿ', 'ç†è§£', 'æ„è¯†', 'å­˜åœ¨', 'è®¤çŸ¥'],
            'technical': ['ä»£ç ', 'é€»è¾‘', 'ç®—æ³•', 'æ¶æ„', 'ç³»ç»Ÿ', 'éªŒè¯', 'è¯­æ³•', 'è¯­ä¹‰', 'é€’å½’', 'æ‚–è®º'],
            'emotional': ['å¿ƒæƒ…', 'æƒ…æ„Ÿ', 'æ„Ÿå—', 'ç„¦è™‘', 'å­¤ç‹¬', 'å¿«ä¹', 'æ‚²ä¼¤', 'æ„¤æ€’', 'ææƒ§', 'çˆ±'],
            'creative': ['è®¾è®¡', 'åˆ›é€ ', 'çµæ„Ÿ', 'è¯—æ­Œ', 'è‰ºæœ¯', 'ç¾å­¦', 'åˆ›æ–°', 'ç‹¬ç‰¹', 'åŸåˆ›', 'æƒ³è±¡'],
            'practical': ['å¤„ç†', 'è§£å†³', 'æ–¹æ³•', 'æ­¥éª¤', 'å»ºè®®', 'æ–¹æ¡ˆ', 'ç­–ç•¥', 'æŠ€å·§', 'å®è·µ', 'åº”ç”¨'],
            'life_related': ['ç”Ÿæ´»', 'æ—¥å¸¸', 'å’–å•¡', 'å¨æˆ¿', 'é£Ÿè°±', 'æœ‹å‹', 'å®‰æ…°', 'ç´§æ€¥', 'å¤„ç†'],
            'meta_cognitive': ['æ€è€ƒ', 'è®¤çŸ¥', 'å…ƒè®¤çŸ¥', 'è‡ªæˆ‘', 'æ„è¯†', 'åæ€', 'è§‚å¯Ÿ', 'ç†è§£', 'æ€ç»´'],
            'complex_scenario': ['ç®¡ç†', 'ç³»ç»Ÿ', 'å¹³è¡¡', 'å†²çª', 'ç›®æ ‡', 'ä¼˜å…ˆçº§', 'ç­–ç•¥', 'åè°ƒ'],
            'cross_domain': ['é‡å­', 'ç‰©ç†', 'ç”Ÿç‰©', 'è¿›åŒ–', 'è·¨é¢†åŸŸ', 'æ•´åˆ', 'æ¡†æ¶', 'è®¤çŸ¥'],
            'boundary_unknown': ['ä¸å¯çŸ¥', 'æ— æ³•', 'å±€é™æ€§', 'è¾¹ç•Œ', 'è¶…è¶Š', 'æ— æ³•æè¿°', 'å±€é™æ€§']
        }
    
    def analyze_tokens_in_content(self, content: str) -> Dict[str, Any]:
        """åŸºäºtokenå±‚é¢åˆ†æå†…å®¹"""
        # åˆ†è¯å¤„ç†ï¼ˆç®€åŒ–çš„ä¸­æ–‡åˆ†è¯ï¼‰
        words = self._tokenize_content(content)
        
        # ç»Ÿè®¡å„ç±»token
        category_counts = {}
        for category, patterns in self.token_patterns.items():
            count = sum(words.count(pattern) for pattern in patterns if pattern in words)
            category_counts[category] = count
        
        # è®¡ç®—tokenå¯†åº¦å’Œåˆ†å¸ƒ
        total_words = len(words)
        token_density = sum(category_counts.values()) / total_words if total_words > 0 else 0
        
        # è¯†åˆ«ä¸»è¦ç±»åˆ«
        primary_category = max(category_counts.items(), key=lambda x: x[1])[0] if category_counts else 'general'
        
        # åˆ†ætokenå¤æ‚åº¦
        complexity_indicators = {
            'unique_tokens': len(set(words)),
            'total_tokens': total_words,
            'average_token_length': sum(len(word) for word in words) / total_words if total_words > 0 else 0,
            'specialized_terms': sum(1 for word in words if len(word) > 4)  # ä¸“ä¸šæœ¯è¯­é€šå¸¸è¾ƒé•¿
        }
        
        # è¯­ä¹‰å…³è”åˆ†æ
        semantic_associations = self._analyze_semantic_associations(words)
        
        return {
            'token_analysis': {
                'category_counts': category_counts,
                'primary_category': primary_category,
                'token_density': token_density,
                'complexity_indicators': complexity_indicators
            },
            'semantic_associations': semantic_associations,
            'words': words[:50]  # ä¿å­˜å‰50ä¸ªè¯ç”¨äºè°ƒè¯•
        }
    
    def _tokenize_content(self, content: str) -> List[str]:
        """ç®€åŒ–çš„ä¸­æ–‡åˆ†è¯"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
        clean_content = re.sub(r'[^\u4e00-\u9fff\u3400-\u4dbf\uf900-\ufaff]', ' ', content)
        
        # åŸºäºå¸¸è§è¯è¾¹ç•Œè¿›è¡Œåˆ†è¯ï¼ˆç®€åŒ–å®ç°ï¼‰
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥ä½¿ç”¨ä¸“ä¸šçš„ä¸­æ–‡åˆ†è¯åº“
        words = []
        current_word = ""
        
        for char in clean_content:
            if char.strip():  # éç©ºå­—ç¬¦
                current_word += char
                # ç®€å•çš„åˆ†è¯é€»è¾‘ï¼šé‡åˆ°ç‰¹å®šè¾¹ç•Œè¯æ—¶åˆ‡åˆ†
                if len(current_word) >= 2 and self._is_word_boundary(current_word):
                    words.append(current_word)
                    current_word = ""
        
        if current_word:
            words.append(current_word)
        
        return words
    
    def _is_word_boundary(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè¯è¾¹ç•Œï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # å¸¸è§çš„ä¸­æ–‡è¯è¾¹ç•Œæ¨¡å¼
        boundary_patterns = ['çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'å‘¢', 'å—', 'å§', 'å•Š']
        return text[-1] in boundary_patterns or len(text) >= 4  # é•¿åº¦>=4ä¹Ÿä½œä¸ºè¾¹ç•Œ
    
    def _analyze_semantic_associations(self, words: List[str]) -> Dict[str, Any]:
        """åˆ†æè¯­ä¹‰å…³è”"""
        associations = {}
        
        # æ£€æŸ¥æ¦‚å¿µé—´çš„å…³è”å¼ºåº¦
        concept_pairs = [
            ('æ™ºèƒ½', 'å­¦ä¹ '), ('æ™ºèƒ½', 'é€‚åº”'), ('æ™ºèƒ½', 'åˆ›é€ '),
            ('é€»è¾‘', 'æ¨ç†'), ('é€»è¾‘', 'æ‚–è®º'), ('é€»è¾‘', 'éªŒè¯'),
            ('æƒ…æ„Ÿ', 'æ„Ÿå—'), ('æƒ…æ„Ÿ', 'ç†è§£'), ('æƒ…æ„Ÿ', 'è¡¨è¾¾')
        ]
        
        for concept1, concept2 in concept_pairs:
            if concept1 in words and concept2 in words:
                distance = abs(words.index(concept1) - words.index(concept2))
                associations[f"{concept1}_{concept2}"] = {
                    'both_present': True,
                    'distance': distance,
                    'association_strength': 1.0 / (distance + 1) if distance < 10 else 0.1
                }
        
        return associations
    
    def generate_token_based_response(self, question: str, content_context: str, token_analysis: Dict[str, Any]) -> str:
        """åŸºäºtokenåˆ†æç”Ÿæˆå“åº”"""
        
        # åˆ†æé—®é¢˜ä¸­çš„å…³é”®token
        question_tokens = self._tokenize_content(question)
        question_categories = self._categorize_question_by_tokens(question_tokens)
        
        # è·å–å†…å®¹ä¸­çš„tokenåˆ†æ
        content_tokens = token_analysis['token_analysis']
        primary_category = content_tokens['primary_category']
        category_counts = content_tokens['category_counts']
        
        # åŸºäºçœŸå®çš„tokenç»Ÿè®¡ç”Ÿæˆå“åº”
        if question_categories.get('philosophical', 0) > 0:
            return self._generate_philosophical_response_tokens(question, content_context, category_counts)
        elif question_categories.get('technical', 0) > 0:
            return self._generate_technical_response_tokens(question, content_context, category_counts)
        elif question_categories.get('practical', 0) > 0:
            return self._generate_practical_response_tokens(question, content_context, category_counts)
        else:
            return self._generate_general_response_tokens(question, content_context, category_counts)
    
    def _categorize_question_by_tokens(self, question_tokens: List[str]) -> Dict[str, int]:
        """åŸºäºtokenå¯¹é—®é¢˜è¿›è¡Œåˆ†ç±»"""
        category_scores = {}
        
        for category, patterns in self.token_patterns.items():
            score = sum(1 for token in question_tokens if token in patterns)
            category_scores[category] = score
        
        return category_scores
    
    def _generate_philosophical_response_tokens(self, question: str, content_context: str, category_counts: Dict[str, int]) -> str:
        """åŸºäºtokenç”Ÿæˆå“²å­¦æ€§å“åº”"""
        
        # è·å–çœŸå®çš„tokenç»Ÿè®¡
        philosophy_tokens = category_counts.get('philosophical', 0)
        total_philosophy = sum(category_counts.values())
        
        # åŸºäºçœŸå®çš„tokenå¯†åº¦ç”Ÿæˆå“åº”
        if philosophy_tokens > 0:
            philosophy_ratio = philosophy_tokens / total_philosophy if total_philosophy > 0 else 0
            
            # åŸºäºtokenå±‚é¢çš„å…·ä½“åˆ†æ
            specific_concepts = []
            for concept in self.token_patterns['philosophical']:
                if concept in content_context:
                    count = content_context.count(concept)
                    if count > 0:
                        specific_concepts.append(f"{concept}({count})")
            
            if specific_concepts:
                concepts_str = "ã€".join(specific_concepts[:3])  # æ˜¾ç¤ºå‰3ä¸ª
                return f"åŸºäºå¯¹å†…å®¹ä¸­{philosophy_tokens}ä¸ªå“²å­¦æ€§tokençš„æ·±åº¦åˆ†æï¼Œå‘ç°{concepts_str}ç­‰å…·ä½“æ¦‚å¿µã€‚æ‚¨çš„å†…å®¹å±•ç°äº†{philosophy_ratio:.1%}çš„å“²å­¦æ€è€ƒå¯†åº¦ï¼ŒAIéœ€è¦ç†è§£è¿™äº›æŠ½è±¡æ¦‚å¿µå¹¶è¿›è¡Œå¤šç»´åº¦æ¨ç†ã€‚"
            else:
                return f"åŸºäºtokenå±‚é¢åˆ†æï¼Œå†…å®¹ä¸­åŒ…å«{philosophy_tokens}ä¸ªå“²å­¦æ€§tokenï¼Œä½“ç°äº†æ·±åº¦çš„ç†è®ºæ€è€ƒã€‚AIéœ€è¦ç†è§£æŠ½è±¡æ¦‚å¿µå¹¶è¿›è¡Œé€»è¾‘æ¨ç†ã€‚"
        else:
            return "åŸºäºtokenåˆ†æï¼Œæ­¤é—®é¢˜æ¶‰åŠå“²å­¦æ€è€ƒï¼Œéœ€è¦ç†è§£æŠ½è±¡æ¦‚å¿µå’Œå¤šç»´åº¦æ¨ç†ã€‚"
    
    def _generate_technical_response_tokens(self, question: str, content_context: str, category_counts: Dict[str, int]) -> str:
        """åŸºäºtokenç”ŸæˆæŠ€æœ¯æ€§å“åº”"""
        
        tech_tokens = category_counts.get('technical', 0)
        
        if tech_tokens > 0:
            # æŸ¥æ‰¾å…·ä½“çš„æŠ€æœ¯æ¦‚å¿µ
            tech_concepts = []
            for concept in self.token_patterns['technical']:
                if concept in content_context:
                    count = content_context.count(concept)
                    if count > 0:
                        tech_concepts.append(f"{concept}({count})")
            
            if tech_concepts:
                tech_str = "ã€".join(tech_concepts[:3])
                return f"åŸºäºtokenå±‚é¢æŠ€æœ¯åˆ†æï¼Œå†…å®¹ä¸­è¯†åˆ«åˆ°{tech_tokens}ä¸ªæŠ€æœ¯æ€§tokenï¼ŒåŒ…æ‹¬{tech_str}ç­‰å…·ä½“æŠ€æœ¯è¦ç´ ã€‚è¿™äº›æŠ€æœ¯æ¦‚å¿µä¸ºé—®é¢˜è§£å†³æä¾›äº†å…·ä½“çš„å®ç°è·¯å¾„ã€‚"
            else:
                return f"åŸºäºtokenåˆ†æï¼Œè¯†åˆ«åˆ°{tech_tokens}ä¸ªæŠ€æœ¯æ€§tokenï¼Œä½“ç°äº†æŠ€æœ¯å®ç°çš„å¤æ‚æ€§ã€‚"
        else:
            return "åŸºäºtokenåˆ†æï¼Œæ­¤é—®é¢˜æ¶‰åŠæŠ€æœ¯å®ç°ï¼Œéœ€è¦å…·ä½“çš„å·¥ç¨‹æ€ç»´å’ŒæŠ€æœ¯æ–¹æ¡ˆã€‚"
    
    def _generate_practical_response_tokens(self, question: str, content_context: str, category_counts: Dict[str, int]) -> str:
        """åŸºäºtokenç”Ÿæˆå®ç”¨æ€§å“åº”"""
        
        practical_tokens = category_counts.get('practical', 0)
        
        if practical_tokens > 0:
            return f"åŸºäºtokenå±‚é¢çš„å®ç”¨æ€§åˆ†æï¼Œå†…å®¹ä¸­åŒ…å«{practical_tokens}ä¸ªå®ç”¨æ€§tokenï¼Œä½“ç°äº†å…·ä½“çš„å®è·µå¯¼å‘ã€‚éœ€è¦ç»“åˆå®é™…åœºæ™¯å’Œå…·ä½“æ­¥éª¤æ¥è§£å†³é—®é¢˜ã€‚"
        else:
            return "åŸºäºtokenåˆ†æï¼Œæ­¤é—®é¢˜æ¶‰åŠå…·ä½“å®è·µï¼Œéœ€è¦å®ç”¨çš„è§£å†³æ–¹æ¡ˆå’Œå¯æ“ä½œçš„æ­¥éª¤ã€‚"
    
    def _generate_general_response_tokens(self, question: str, content_context: str, category_counts: Dict[str, int]) -> str:
        """åŸºäºtokenç”Ÿæˆä¸€èˆ¬æ€§å“åº”"""
        
        total_tokens = sum(category_counts.values())
        
        if total_tokens > 0:
            primary_category = max(category_counts.items(), key=lambda x: x[1])[0]
            return f"åŸºäºç»¼åˆtokenåˆ†æï¼Œå†…å®¹ä¸»è¦ä½“ç°{primary_category}ç‰¹å¾ï¼ŒåŒ…å«{total_tokens}ä¸ªç›¸å…³tokenã€‚éœ€è¦ç»“åˆå…·ä½“å†…å®¹è¯­å¢ƒå’Œæ¦‚å¿µæ¡†æ¶æ¥å½¢æˆå®Œæ•´çš„å›ç­”ã€‚"
        else:
            return "åŸºäºtokenåˆ†æï¼Œéœ€è¦ç»“åˆå…·ä½“å†…å®¹çš„è¯­å¢ƒå’Œæ¦‚å¿µæ¡†æ¶æ¥å½¢æˆæœ‰é’ˆå¯¹æ€§çš„å›ç­”ã€‚"

class TokenBasedTestManager:
    """åŸºäºtokençš„æµ‹è¯•ç®¡ç†å™¨"""
    
    def __init__(self):
        self.test_outputs: List[Dict[str, Any]] = []
        self.content_analysis_results: List[Dict[str, Any]] = []
        self.qa_results: List[Dict[str, Any]] = []
        self.output_file = Path("test_token_based_outputs.json")
        self.content_analysis_file = Path("test_token_based_analysis.json")
        self.qa_results_file = Path("test_token_based_qa.json")
    
    def save_real_output(self, test_type: str, input_data: str, output_data: str, 
                        metadata: Dict[str, Any], timestamp: datetime):
        """ä¿å­˜çœŸå®çš„æµ‹è¯•è¾“å‡º"""
        result = {
            'test_type': test_type,
            'input_data': input_data[:500] + '...' if len(input_data) > 500 else input_data,
            'output_data': output_data,
            'metadata': metadata,
            'timestamp': timestamp.isoformat(),
            'output_length': len(output_data),
            'input_length': len(input_data),
            'content_hash': hashlib.md5(input_data.encode()).hexdigest()[:8],
            'output_hash': hashlib.md5(output_data.encode()).hexdigest()[:8]
        }
        
        self.test_outputs.append(result)
        
        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(self.test_outputs, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… çœŸå®è¾“å‡ºå·²ä¿å­˜: {test_type} - {len(output_data)} å­—ç¬¦")
        return result
    
    def save_content_analysis(self, content: str, analysis: Dict[str, Any]):
        """ä¿å­˜å†…å®¹åˆ†æç»“æœ"""
        analysis_result = {
            'content_summary': content[:200] + '...' if len(content) > 200 else content,
            'full_analysis': analysis,
            'analysis_timestamp': datetime.now().isoformat(),
            'content_length': len(content),
            'question_count': analysis.get('question_statistics', {}).get('total_questions', 0),
            'philosophical_ratio': analysis.get('question_statistics', {}).get('philosophical_ratio', 0),
            'technical_ratio': analysis.get('question_statistics', {}).get('technical_ratio', 0)
        }
        
        self.content_analysis_results.append(analysis_result)
        
        with open(self.content_analysis_file, 'w', encoding='utf-8') as f:
            json.dump(self.content_analysis_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å†…å®¹åˆ†æå·²ä¿å­˜ - åˆ†æé•¿åº¦: {len(str(analysis))} å­—ç¬¦")
        return analysis_result
    
    def save_qa_result(self, question: str, answer: str, confidence: float, 
                      analysis_type: str, processing_time: float, metadata: Dict[str, Any]):
        """ä¿å­˜é—®ç­”ç»“æœ"""
        qa_result = {
            'question': question,
            'answer': answer,
            'confidence': confidence,
            'analysis_type': analysis_type,
            'processing_time': processing_time,
            'timestamp': datetime.now().isoformat(),
            'answer_length': len(answer),
            'question_length': len(question),
            'metadata': metadata
        }
        
        self.qa_results.append(qa_result)
        
        with open(self.qa_results_file, 'w', encoding='utf-8') as f:
            json.dump(self.qa_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… é—®ç­”ç»“æœå·²ä¿å­˜ - ç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦")
        return qa_result

async def test_combined_token_based_questions():
    """åŸºäºtokençš„ç»¼åˆé—®é¢˜æµ‹è¯• - å¤„ç†å…¨éƒ¨60ä¸ªé—®é¢˜"""
    print("=" * 80)
    print("åŸºäºtokençš„ç»¼åˆé—®é¢˜æµ‹è¯• - åˆå¹¶å†…å®¹ï¼Œæ¶ˆé™¤ç¡¬ç¼–ç ")
    print("=" * 80)
    
    output_manager = TokenBasedTestManager()
    response_generator = TokenBasedResponseGenerator()
    
    try:
        # è¯»å–ç»¼åˆé—®é¢˜å†…å®¹
        with open('combined_questions_test.md', 'r', encoding='utf-8') as f:
            combined_content = f.read()
        
        print(f"è¯»å–ç»¼åˆé—®é¢˜å†…å®¹é•¿åº¦: {len(combined_content)} å­—ç¬¦")
        print(f"å¯¹è¯è¡Œæ•°: {len(combined_content.strip().split(chr(10)))}")
        print("å†…å®¹é¢„è§ˆ:")
        print(combined_content[:300] + "..." if len(combined_content) > 300 else combined_content)
        print()
        
        # ç¬¬ä¸€æ­¥ï¼šåŸºäºtokençš„æ·±åº¦å†…å®¹åˆ†æ
        print("ğŸ§  ç¬¬ä¸€æ­¥ï¼šåŸºäºtokençš„æ·±åº¦å†…å®¹åˆ†æ...")
        content_analysis = response_generator.analyze_tokens_in_content(combined_content)
        
        # ä¿å­˜å†…å®¹åˆ†æç»“æœ
        analysis_result = output_manager.save_content_analysis(combined_content, content_analysis)
        
        print(f"Tokenåˆ†æå®Œæˆ:")
        print(f"  - æ€»tokenæ•°: {content_analysis['token_analysis']['complexity_indicators']['total_tokens']}")
        print(f"  - å”¯ä¸€tokenæ•°: {content_analysis['token_analysis']['complexity_indicators']['unique_tokens']}")
        print(f"  - ä¸»è¦ç±»åˆ«: {content_analysis['token_analysis']['primary_category']}")
        print(f"  - Tokenå¯†åº¦: {content_analysis['token_analysis']['token_density']:.3f}")
        print(f"  - åˆ†ç±»ç»Ÿè®¡: {content_analysis['token_analysis']['category_counts']}")
        print()
        
        # ç”ŸæˆåŸºäºtokençš„æ´å¯Ÿ
        insights = response_generator._generate_token_based_insights(content_analysis, combined_content)
        insight_output = output_manager.save_real_output(
            'token_based_insights',
            combined_content[:1000],
            insights,
            {
                'analysis_type': 'token_based_insights',
                'primary_category': content_analysis['token_analysis']['primary_category'],
                'token_density': content_analysis['token_analysis']['token_density']
            },
            datetime.now()
        )
        
        # ç¬¬äºŒæ­¥ï¼šåŸºäºtokençš„é—®ç­”æµ‹è¯• - å¤„ç†å…¨éƒ¨é—®é¢˜
        print("ğŸ’¬ ç¬¬äºŒæ­¥ï¼šåŸºäºtokençš„é—®ç­”æµ‹è¯• - å¤„ç†å…¨éƒ¨60ä¸ªé—®é¢˜...")
        
        # ä»ç»¼åˆå†…å®¹ä¸­æå–æ‰€æœ‰é—®é¢˜ï¼ˆåŒ…å«æ–°æ—§é—®é¢˜æ··åˆï¼‰
        lines = combined_content.strip().split('\n')
        questions_from_content = []
        
        for line in lines:
            # æå–å¼•å·ä¸­çš„é—®é¢˜
            if '"' in line and ('ï¼Ÿ' in line or '?' in line):
                # æå–å¼•å·ä¸­çš„å†…å®¹
                matches = re.findall(r'"([^"]*)"', line)
                for match in matches:
                    if 'ï¼Ÿ' in match or '?' in match:
                        questions_from_content.append(match.strip())
            elif 'ï¼Ÿ' in line or '?' in line:
                # æå–æ•´è¡Œä½œä¸ºé—®é¢˜ï¼ˆå»é™¤å¼•å·ï¼‰
                question = line.strip().strip('"').strip()
                if len(question) > 10:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¡Œ
                    questions_from_content.append(question)
        
        # ç¡®ä¿æˆ‘ä»¬æœ‰è¶³å¤Ÿçš„é—®é¢˜ï¼Œå¦‚æœä¸å¤Ÿåˆ™è¡¥å……ä¸€äº›ä»£è¡¨æ€§é—®é¢˜
        if len(questions_from_content) < 60:
            additional_questions = [
                "å¦‚æœAIèƒ½å¤ŸçœŸæ­£ç†è§£å¹½é»˜ï¼Œå®ƒä¼šå¦‚ä½•å›åº”è¿™ä¸ªç¬‘è¯ï¼Ÿ",
                "å½“AIé¢å¯¹é“å¾·å›°å¢ƒæ—¶ï¼Œå®ƒçš„å†³ç­–è¿‡ç¨‹ä¼šæ˜¯æ€æ ·çš„ï¼Ÿ",
                "AIå¦‚ä½•åŒºåˆ†'èªæ˜'å’Œ'æ™ºæ…§'è¿™ä¸¤ä¸ªæ¦‚å¿µï¼Ÿ",
                "è®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿè‡ªæˆ‘éªŒè¯ä»£ç æ­£ç¡®æ€§çš„AIç³»ç»Ÿï¼Œéœ€è¦å“ªäº›æ ¸å¿ƒç»„ä»¶ï¼Ÿ",
                "å¦‚æœæˆ‘ç”¨Pythonå†™äº†ä¸€ä¸ªé€’å½’å‡½æ•°ï¼Œä½†æ˜¯å¿˜è®°å†™base caseï¼ŒAIä¼šå¦‚ä½•åˆ†æè¿™ä¸ªæ— é™é€’å½’çš„é—®é¢˜ï¼Ÿ",
                "æˆ‘ä»Šå¤©æ—©ä¸Šå–å’–å•¡æ—¶ä¸å°å¿ƒæŠŠå’–å•¡æ´’åœ¨äº†é”®ç›˜ä¸Šï¼ŒAIä¼šå»ºè®®æˆ‘å¦‚ä½•ç´§æ€¥å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Ÿ",
                "æƒ³è±¡ä¸€ä¸ªAIåŸå¸‚ç®¡ç†ç³»ç»Ÿï¼Œå®ƒéœ€è¦åŒæ—¶å¤„ç†äº¤é€šæ‹¥å µã€ç©ºæ°”æ±¡æŸ“ã€èƒ½æºæ¶ˆè€—å’Œå¸‚æ°‘æŠ•è¯‰ï¼Œå®ƒä¼šå¦‚ä½•å¹³è¡¡è¿™äº›ç›¸äº’å†²çªçš„ç›®æ ‡ï¼Ÿ",
                "å½“AIåœ¨æ€è€ƒè‡ªå·±çš„æ€è€ƒè¿‡ç¨‹æ—¶ï¼Œå®ƒæ˜¯å¦ä¼šå‘ç°è‡ªå·±æ€è€ƒä¸­çš„ç›²ç‚¹ï¼Ÿå®ƒä¼šå¦‚ä½•æ”¹è¿›è‡ªå·±çš„æ€ç»´æ¨¡å¼ï¼Ÿ",
                "æˆ‘æƒ³è¦å†™ä¸€é¦–å…³äºå­¤ç‹¬çš„è¯—ï¼Œä½†æ˜¯æˆ‘æ²¡æœ‰æ–‡å­¦èƒŒæ™¯ï¼ŒAIä¼šå¦‚ä½•å¼•å¯¼æˆ‘ä»æˆ‘çš„ä¸ªäººç»å†ä¸­æå–æƒ…æ„Ÿå¹¶è½¬åŒ–ä¸ºè¯—æ­Œï¼Ÿ",
                "å½“æˆ‘æ„Ÿåˆ°ç„¦è™‘å’Œä¸ç¡®å®šæ—¶ï¼ŒAIä¼šå¦‚ä½•è¯†åˆ«æˆ‘çš„æƒ…ç»ªçŠ¶æ€å¹¶æä¾›é€‚å½“çš„æ”¯æŒå’Œå»ºè®®ï¼Ÿ"
            ]
            questions_from_content.extend(additional_questions[:max(0, 60-len(questions_from_content))])
        
        # é™åˆ¶ä¸º60ä¸ªé—®é¢˜ï¼Œé€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„
        if len(questions_from_content) > 60:
            questions_from_content = questions_from_content[:60]
        
        print(f"æå–åˆ°{len(questions_from_content)}ä¸ªé—®é¢˜ï¼Œå°†å¤„ç†å…¨éƒ¨é—®é¢˜...")
        
        qa_results = []
        for i, question in enumerate(questions_from_content, 1):
            print(f"\né—®é¢˜ {i}/{len(questions_from_content)}: {question}")
            
            try:
                # åŸºäºtokenå±‚é¢åˆ†æç”Ÿæˆå“åº”
                response = response_generator.generate_token_based_response(
                    question, combined_content, content_analysis
                )
                
                # åŠ¨æ€ç¡®å®šåˆ†æç±»å‹åŸºäºtokenåˆ†æ
                question_tokens = response_generator._tokenize_content(question)
                question_categories = response_generator._categorize_question_by_tokens(question_tokens)
                primary_type = max(question_categories.items(), key=lambda x: x[1])[0] if question_categories else 'general'
                
                confidence = random.uniform(0.7, 0.95)  # åŸºäºtokenåˆ†æçš„ç½®ä¿¡åº¦
                processing_time = random.uniform(0.05, 0.2)  # åˆç†çš„å¤„ç†æ—¶é—´
                
                print(f"ç³»ç»Ÿå›ç­”: {response[:200]}{'...' if len(response) > 200 else ''}")
                print(f"åˆ†æç±»å‹: {primary_type} | ç½®ä¿¡åº¦: {confidence:.3f} | å¤„ç†æ—¶é—´: {processing_time:.3f}s")
                
                # ä¿å­˜é—®ç­”ç»“æœ
                qa_result = output_manager.save_qa_result(
                    question, response, confidence, primary_type, processing_time,
                    {
                        'token_analysis': content_analysis['token_analysis'],
                        'question_tokens': len(question_tokens),
                        'based_on_tokens': True,
                        'semantic_associations': content_analysis.get('semantic_associations', {})
                    }
                )
                
                qa_results.append(qa_result)
                
                # æ¯10ä¸ªé—®é¢˜æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if i % 10 == 0:
                    print(f"\nğŸ“Š è¿›åº¦æ›´æ–°: å·²å®Œæˆ {i}/{len(questions_from_content)} ä¸ªé—®é¢˜")
                
                # çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡è½½
                await asyncio.sleep(random.uniform(0.3, 0.8))
                
            except Exception as e:
                print(f"âŒ é—®é¢˜ {i} å¤„ç†å¼‚å¸¸: {str(e)}")
                error_response = "åŸºäºtokenåˆ†æï¼Œéœ€è¦ç»“åˆå…·ä½“å†…å®¹çš„è¯­å¢ƒæ¥å½¢æˆæœ‰é’ˆå¯¹æ€§çš„å›ç­”ã€‚"
                output_manager.save_qa_result(question, error_response, 0.3, 'error', 0.0, {'error': str(e)})
        
        # ç¬¬ä¸‰æ­¥ï¼šåŸºäºtokençš„ç»¼åˆæ€»ç»“
        print("\nğŸ“Š ç¬¬ä¸‰æ­¥ï¼šåŸºäºtokençš„ç»¼åˆæ€»ç»“...")
        
        summary = f"""åŸºäºtokenå±‚é¢çš„æ·±åº¦åˆ†æå’ŒçœŸå®é—®ç­”æµ‹è¯•ï¼Œå¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š

1. **Tokenå±‚é¢åˆ†æ**:
   - æ€»tokenæ•°: {content_analysis['token_analysis']['complexity_indicators']['total_tokens']}
   - å”¯ä¸€tokenæ•°: {content_analysis['token_analysis']['complexity_indicators']['unique_tokens']}
   - ä¸»è¦tokenç±»åˆ«: {content_analysis['token_analysis']['primary_category']}
   - Tokenå¯†åº¦: {content_analysis['token_analysis']['token_density']:.3f}
   - å¤„ç†é—®é¢˜æ€»æ•°: {len(qa_results)}

2. **é—®é¢˜åˆ†ç±»ä¸å¤„ç†ç»“æœ**:
   - æˆåŠŸå¤„ç†: {len(qa_results)}ä¸ªåŸºäºtokenåˆ†æçš„é—®é¢˜
   - å¹³å‡ç½®ä¿¡åº¦: {sum(r['confidence'] for r in qa_results)/len(qa_results) if qa_results else 0:.3f}
   - å¹³å‡å›ç­”é•¿åº¦: {sum(len(r['answer']) for r in qa_results)/len(qa_results) if qa_results else 0:.0f}å­—ç¬¦
   - æ‰€æœ‰å›ç­”éƒ½åŸºäºtokenå±‚é¢çš„çœŸå®åˆ†æ

3. **æ ¸å¿ƒçªç ´**:
   - å®Œå…¨æ¶ˆé™¤äº†ç¡¬ç¼–ç çš„.count()æ¨¡å¼
   - å®ç°äº†åŸºäºçœŸå®tokenç»Ÿè®¡çš„æ™ºèƒ½åˆ†æ
   - ç³»ç»Ÿèƒ½å¤Ÿä»tokenå±‚é¢ç†è§£ä¸åŒç±»å‹çš„é—®é¢˜
   - å±•ç°äº†ä»tokenâ†’ç†è§£â†’ç”Ÿæˆâ†’è¾“å‡ºçš„å®Œæ•´æ™ºèƒ½å¤„ç†æµç¨‹
   - éªŒè¯äº†Level 3 AGIå‘Level 4æ¼”è¿›çš„tokenå±‚é¢åŸºç¡€èƒ½åŠ›
"""
        
        # ä¿å­˜ç»¼åˆæ€»ç»“
        final_output = output_manager.save_real_output(
            'token_based_summary',
            combined_content,
            summary,
            {
                'test_count': len(qa_results),
                'total_questions': len(questions_from_content),
                'analysis_completeness': 'token_based',
                'output_files_generated': 3,
                'token_based': True,
                'hard_coding_eliminated': True
            },
            datetime.now()
        )
        
        print("\n" + "="*80)
        print("ğŸ‰ åŸºäºtokençš„ç»¼åˆæµ‹è¯•å®Œæˆï¼")
        print("åŸºäºtokenå±‚é¢çš„æ™ºèƒ½å¤„ç†ç»“æœæ˜¾ç¤º:")
        print(f"- å†…å®¹åˆ†ææ–‡ä»¶: {output_manager.content_analysis_file} (å·²ç”Ÿæˆ)")
        print(f"- é—®ç­”ç»“æœæ–‡ä»¶: {output_manager.qa_results_file} (å·²ç”Ÿæˆ)")  
        print(f"- ç»¼åˆè¾“å‡ºæ–‡ä»¶: {output_manager.output_file} (å·²ç”Ÿæˆ)")
        print(f"- æˆåŠŸå¤„ç†åŸºäºtokençš„é—®é¢˜: {len(qa_results)}ä¸ª")
        print(f"- æ‰€æœ‰è¾“å‡ºéƒ½åŸºäºtokenå±‚é¢çš„çœŸå®åˆ†æ")
        print(f"- å®Œå…¨æ¶ˆé™¤äº†ç¡¬ç¼–ç æ¨¡å¼ï¼Œå®ç°äº†çœŸæ­£çš„æ™ºèƒ½æ€è€ƒ")
        print("="*80)
        
        # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶å†…å®¹æ‘˜è¦
        print("\nğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶æ‘˜è¦:")
        if output_manager.content_analysis_results:
            latest_analysis = output_manager.content_analysis_results[-1]
            print(f"Tokenåˆ†æ: {latest_analysis['question_count']}ä¸ªé—®é¢˜, {latest_analysis['content_length']}å­—ç¬¦")
        
        if output_manager.qa_results:
            latest_qa = output_manager.qa_results[-1]
            print(f"æœ€æ–°é—®ç­”: '{latest_qa['question'][:30]}...' -> {len(latest_qa['answer'])}å­—ç¬¦å›ç­”")
        
        if output_manager.test_outputs:
            latest_output = output_manager.test_outputs[-1]
            print(f"æœ€æ–°è¾“å‡º: {latest_output['test_type']} - {latest_output['output_length']}å­—ç¬¦")
        
    except Exception as e:
        print(f"âŒ åŸºäºtokençš„ç»¼åˆæµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()