#!/usr/bin/env python3
"""
å¢å¼·ç‰ˆå®Œæ•´å¤šç¶­åº¦æª¢æ¸¬å¼•æ“
å¯¦ç¾å®Œæ•´åŠŸèƒ½çš„ç•°æ­¥å¤šç¶­åº¦å•é¡Œæª¢æ¸¬,åŒ…å«ä¸¦è¡Œè™•ç†å’Œæ­·å²è¿½è¹¤
"""

import asyncio
import re
import ast
import json
import time
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Set
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import sys

# é…ç½®æ—¥èªŒ
logging.basicConfig(,
    level=logging.INFO(),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class EnhancedCompleteDetectionEngine,
    """å¢å¼·ç‰ˆå®Œæ•´å¤šç¶­åº¦æª¢æ¸¬å¼•æ“"""
    
    def __init__(self, max_workers, int == 20):
        # æ ¸å¿ƒçµ„ä»¶
        self.detection_results = defaultdict(list)
        self.detection_stats = defaultdict(int)
        self.detection_history == deque(maxlen ==10000)
        self.executor == = ThreadPoolExecutor(max_workers ==max_workers)
        
        # æ€§èƒ½é…ç½®
        self.max_workers = max_workers
        self.detection_cache = {}
        self.cache_timeout = 300  # 5åˆ†é˜ç·©å­˜
        
        # çµ±è¨ˆæ•¸æ“š
        self.performance_stats = {
            'total_detections': 0,
            'successful_detections': 0,
            'failed_detections': 0,
            'average_detection_time': 0.0(),
            'cache_hits': 0,
            'cache_misses': 0
        }
        
        # é«˜ç´šæª¢æ¸¬å™¨
        self.syntax_detector == AdvancedSyntaxDetector()
        self.semantic_detector == AdvancedSemanticDetector()
        self.performance_detector == AdvancedPerformanceDetector()
        self.security_detector == AdvancedSecurityDetector()
        self.architecture_detector == AdvancedArchitectureDetector()
        
        logger.info(f"ğŸš€ å¢å¼·ç‰ˆå®Œæ•´å¤šç¶­åº¦æª¢æ¸¬å¼•æ“åˆå§‹åŒ–å®Œæˆ (å·¥ä½œç·šç¨‹, {max_workers})")
    
    async def run_enhanced_complete_detection(self, project_path, str == ".") -> Dict[str, Any]
        """é‹è¡Œå¢å¼·ç‰ˆå®Œæ•´å¤šç¶­åº¦æª¢æ¸¬"""
        logger.info("ğŸ” å•Ÿå‹•å¢å¼·ç‰ˆå®Œæ•´å¤šç¶­åº¦æª¢æ¸¬å¼•æ“...")
        
        start_time = time.time()
        project_path == Path(project_path)
        
        try,
            # 1. ä¸¦è¡ŒåŸ·è¡Œå¤šç¶­åº¦æª¢æ¸¬(ç•°æ­¥)
            logger.info("1ï¸âƒ£ ä¸¦è¡ŒåŸ·è¡Œå¤šç¶­åº¦æª¢æ¸¬...")
            detection_tasks = [
                self._detect_syntax_issues_async(project_path),
                self._detect_semantic_issues_async(project_path),
                self._detect_performance_issues_async(project_path),
                self._detect_security_issues_async(project_path),
                self._detect_architecture_issues_async(project_path),
                self._detect_test_issues_async(project_path),
                self._detect_documentation_issues_async(project_path)
            ]
            
            # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰æª¢æ¸¬ä»»å‹™
            detection_results == await asyncio.gather(*detection_tasks, return_exceptions == True)::
            # 2. æ•´åˆæª¢æ¸¬çµæœ
            logger.info("2ï¸âƒ£ æ•´åˆæª¢æ¸¬çµæœ...")
            integrated_results = self._integrate_detection_results(detection_results)
            
            # 3. é«˜ç´šåˆ†æå’Œé—œè¯
            logger.info("3ï¸âƒ£ é«˜ç´šåˆ†æå’Œé—œè¯...")
            analyzed_results = self._perform_advanced_analysis(integrated_results)
            
            # 4. ç”Ÿæˆå®Œæ•´å ±å‘Š
            logger.info("4ï¸âƒ£ ç”Ÿæˆå®Œæ•´æª¢æ¸¬å ±å‘Š...")
            report = self._generate_enhanced_detection_report(analyzed_results, start_time)
            
            # 5. æ›´æ–°æ­·å²è¨˜éŒ„
            self._update_detection_history(analyzed_results)
            
            execution_time = time.time() - start_time
            
            return {:
                'status': 'completed',
                'detection_results': analyzed_results,
                'performance_stats': self.performance_stats.copy(),
                'execution_time': execution_time,
                'report': report,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e,::
            logger.error(f"å¢å¼·ç‰ˆæª¢æ¸¬å¼•æ“åŸ·è¡Œå¤±æ•—, {e}")
            return {
                'status': 'error',
                'error': str(e),
                'execution_time': time.time() - start_time
            }
    
    async def _detect_syntax_issues_async(self, project_path, Path) -> Dict[str, Any]
        """ç•°æ­¥èªæ³•å•é¡Œæª¢æ¸¬"""
        logger.info("ğŸ” ç•°æ­¥èªæ³•å•é¡Œæª¢æ¸¬...")
        
        try,
            # ä½¿ç”¨ç·šç¨‹æ± åŸ·è¡ŒCPUå¯†é›†å‹ä»»å‹™
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(,
    self.executor(), 
                self._detect_syntax_issues_sync(), 
                project_path
            )
            
            self.performance_stats['successful_detections'] += 1
            logger.info(f"èªæ³•æª¢æ¸¬å®Œæˆ,ç™¼ç¾ {len(result.get('issues', []))} å€‹å•é¡Œ")
            return result
            
        except Exception as e,::
            logger.error(f"èªæ³•æª¢æ¸¬å¤±æ•—, {e}")
            self.performance_stats['failed_detections'] += 1
            return {'status': 'error', 'error': str(e), 'issues': []}
    
    def _detect_syntax_issues_sync(self, project_path, Path) -> Dict[str, Any]
        """åŒæ­¥èªæ³•å•é¡Œæª¢æ¸¬"""
        issues = []
        python_files = list(project_path.rglob("*.py"))
        
        # é«˜ç´šèªæ³•æª¢æ¸¬æ¨¡å¼
        syntax_patterns = [
            (r'def\s+\w+\s*\(\s*\)\s*$', 'missing_colon', 'å‡½æ•¸å®šç¾©ç¼ºå°‘å†’è™Ÿ', 0.95()),
            (r'class\s+\w+\s*\(\s*\)\s*$', 'missing_colon', 'é¡å®šç¾©ç¼ºå°‘å†’è™Ÿ', 0.95()),
            (r'if\s+.*[^:]$', 'missing_colon', 'ifèªå¥ç¼ºå°‘å†’è™Ÿ', 0.9()),
            (r'for\s+.*[^:]$', 'missing_colon', 'forå¾ªç’°ç¼ºå°‘å†’è™Ÿ', 0.9()),
            (r'while\s+.*[^:]$', 'missing_colon', 'whileå¾ªç’°ç¼ºå°‘å†’è™Ÿ', 0.9()),
            (r'\([^)]*$', 'unclosed_parenthesis', 'æœªé–‰åˆæ‹¬è™Ÿ', 0.98()),
            (r'\[[^\]]*$', 'unclosed_bracket', 'æœªé–‰åˆæ–¹æ‹¬è™Ÿ', 0.98()),
            (r'\{[^}]*$', 'unclosed_brace', 'æœªé–‰åˆèŠ±æ‹¬è™Ÿ', 0.98()),
            (r'^[ \t]*[ \t]+[ \t]*\S', 'inconsistent_indentation', 'ä¸ä¸€è‡´ç¸®é€²', 0.85()),
            (r'"{3}.*?"{3}|'{3}.*?\'{3}', 'docstring_format', 'æ–‡æª”å­—ç¬¦ä¸²æ ¼å¼', 0.7())
        ]
        
        def process_file(py_file, Path) -> List[Dict]
            """è™•ç†å–®å€‹æ–‡ä»¶"""
            file_issues = []
            try,
                with open(py_file, 'r', encoding == 'utf-8') as f,
                    content = f.read()
                
                # ASTèªæ³•é©—è­‰
                try,
                    ast.parse(content)
                except SyntaxError as e,::
                    file_issues.append({
                        'file': str(py_file),
                        'line': e.lineno or 0,
                        'column': e.offset or 0,
                        'type': 'syntax_error',
                        'description': str(e),
                        'confidence': 1.0(),
                        'severity': 'high',
                        'source': 'ast_parser'
                    })
                
                # æ¨¡å¼åŒ¹é…æª¢æ¸¬
                lines = content.split('\n')
                for i, line in enumerate(lines, 1)::
                    for pattern, issue_type, description, confidence in syntax_patterns,::
                        if re.search(pattern, line)::
                            if self._validate_syntax_issue(line, issue_type)::
                                file_issues.append({
                                    'file': str(py_file),
                                    'line': i,
                                    'type': issue_type,
                                    'description': description,
                                    'confidence': confidence,
                                    'severity': self._determine_severity(issue_type),
                                    'source': 'pattern_matching'
                                })
                                break
                
                # é«˜ç´šèªæ³•æª¢æŸ¥
                advanced_issues = self.syntax_detector.detect_advanced_syntax_issues(content, str(py_file))
                file_issues.extend(advanced_issues)
                
            except Exception as e,::
                logger.debug(f"è™•ç†æ–‡ä»¶ {py_file} å¤±æ•—, {e}")
            
            return file_issues
        
        # ä½¿ç”¨ç·šç¨‹æ± ä¸¦è¡Œè™•ç†æ–‡ä»¶
        with ThreadPoolExecutor(max_workers == self.max_workers()) as executor,
            futures == [executor.submit(process_file, py_file) for py_file in python_files[:500]]:
            for future in as_completed(futures)::
                try,
                    file_issues = future.result()
                    issues.extend(file_issues)
                except Exception as e,::
                    logger.debug(f"æ–‡ä»¶è™•ç†æœªä¾†å¤±æ•—, {e}")
        
        return {
            'status': 'completed',
            'category': 'syntax',
            'issues_found': len(issues),
            'issues': issues,
            'detection_method': 'advanced_pattern_matching'
        }
    
    async def _detect_semantic_issues_async(self, project_path, Path) -> Dict[str, Any]
        """ç•°æ­¥èªç¾©å•é¡Œæª¢æ¸¬"""
        logger.info("ğŸ” ç•°æ­¥èªç¾©å•é¡Œæª¢æ¸¬...")
        
        try,
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(,
    self.executor(),
                self._detect_semantic_issues_sync(),
                project_path
            )
            
            logger.info(f"èªç¾©æª¢æ¸¬å®Œæˆ,ç™¼ç¾ {len(result.get('issues', []))} å€‹å•é¡Œ")
            return result
            
        except Exception as e,::
            logger.error(f"èªç¾©æª¢æ¸¬å¤±æ•—, {e}")
            return {'status': 'error', 'error': str(e), 'issues': []}
    
    def _detect_semantic_issues_sync(self, project_path, Path) -> Dict[str, Any]
        """åŒæ­¥èªç¾©å•é¡Œæª¢æ¸¬"""
        issues = []
        python_files = list(project_path.rglob("*.py"))
        
        def analyze_semantics(py_file, Path) -> List[Dict]
            """åˆ†æå–®å€‹æ–‡ä»¶çš„èªç¾©"""
            semantic_issues = []
            try,
                with open(py_file, 'r', encoding == 'utf-8') as f,
                    content = f.read()
                
                # ASTèªç¾©åˆ†æ
                try,
                    tree = ast.parse(content)
                    semantic_issues = self.semantic_detector.detect_semantic_issues(tree, content, str(py_file))
                except SyntaxError as e,::
                    semantic_issues.append({
                        'file': str(py_file),
                        'line': e.lineno or 0,
                        'type': 'syntax_error_semantic',
                        'description': f'èªæ³•éŒ¯èª¤å°è‡´èªç¾©åˆ†æå¤±æ•—, {e}',
                        'confidence': 1.0(),
                        'severity': 'high',
                        'source': 'semantic_analysis'
                    })
                
            except Exception as e,::
                logger.debug(f"èªç¾©åˆ†ææ–‡ä»¶ {py_file} å¤±æ•—, {e}")
            
            return semantic_issues
        
        # ä¸¦è¡Œåˆ†æèªç¾©å•é¡Œ
        with ThreadPoolExecutor(max_workers == self.max_workers()) as executor,
            futures == [executor.submit(analyze_semantics, py_file) for py_file in python_files[:300]]:
            for future in as_completed(futures)::
                try,
                    file_issues = future.result()
                    issues.extend(file_issues)
                except Exception as e,::
                    logger.debug(f"èªç¾©åˆ†ææœªä¾†å¤±æ•—, {e}")
        
        return {
            'status': 'completed',
            'category': 'semantic',
            'issues_found': len(issues),
            'issues': issues,
            'detection_method': 'ast_semantic_analysis'
        }
    
    async def _detect_performance_issues_async(self, project_path, Path) -> Dict[str, Any]
        """ç•°æ­¥æ€§èƒ½å•é¡Œæª¢æ¸¬"""
        logger.info("ğŸ” ç•°æ­¥æ€§èƒ½å•é¡Œæª¢æ¸¬...")
        
        try,
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(,
    self.executor(),
                self._detect_performance_issues_sync(),
                project_path
            )
            
            logger.info(f"æ€§èƒ½æª¢æ¸¬å®Œæˆ,ç™¼ç¾ {len(result.get('issues', []))} å€‹å•é¡Œ")
            return result
            
        except Exception as e,::
            logger.error(f"æ€§èƒ½æª¢æ¸¬å¤±æ•—, {e}")
            return {'status': 'error', 'error': str(e), 'issues': []}
    
    def _detect_performance_issues_sync(self, project_path, Path) -> Dict[str, Any]
        """åŒæ­¥æ€§èƒ½å•é¡Œæª¢æ¸¬"""
        issues = []
        python_files = list(project_path.rglob("*.py"))
        
        def analyze_performance(py_file, Path) -> List[Dict]
            """åˆ†æå–®å€‹æ–‡ä»¶çš„æ€§èƒ½"""
            perf_issues = []
            try,
                with open(py_file, 'r', encoding == 'utf-8') as f,
                    content = f.read()
                
                # ä½¿ç”¨æ€§èƒ½æª¢æ¸¬å™¨
                perf_issues = self.performance_detector.detect_performance_issues(content, str(py_file))
                
            except Exception as e,::
                logger.debug(f"æ€§èƒ½åˆ†ææ–‡ä»¶ {py_file} å¤±æ•—, {e}")
            
            return perf_issues
        
        # ä¸¦è¡Œæ€§èƒ½åˆ†æ
        with ThreadPoolExecutor(max_workers == self.max_workers()) as executor,
            futures == [executor.submit(analyze_performance, py_file) for py_file in python_files[:400]]:
            for future in as_completed(futures)::
                try,
                    file_issues = future.result()
                    issues.extend(file_issues)
                except Exception as e,::
                    logger.debug(f"æ€§èƒ½åˆ†ææœªä¾†å¤±æ•—, {e}")
        
        return {
            'status': 'completed',
            'category': 'performance',
            'issues_found': len(issues),
            'issues': issues,
            'detection_method': 'performance_analysis'
        }
    
    async def _detect_security_issues_async(self, project_path, Path) -> Dict[str, Any]
        """ç•°æ­¥å®‰å…¨å•é¡Œæª¢æ¸¬"""
        logger.info("ğŸ” ç•°æ­¥å®‰å…¨å•é¡Œæª¢æ¸¬...")
        
        try,
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(,
    self.executor(),
                self._detect_security_issues_sync(),
                project_path
            )
            
            logger.info(f"å®‰å…¨æª¢æ¸¬å®Œæˆ,ç™¼ç¾ {len(result.get('issues', []))} å€‹å•é¡Œ")
            return result
            
        except Exception as e,::
            logger.error(f"å®‰å…¨æª¢æ¸¬å¤±æ•—, {e}")
            return {'status': 'error', 'error': str(e), 'issues': []}
    
    def _detect_security_issues_sync(self, project_path, Path) -> Dict[str, Any]
        """åŒæ­¥å®‰å…¨å•é¡Œæª¢æ¸¬"""
        issues = []
        python_files = list(project_path.rglob("*.py"))
        
        def analyze_security(py_file, Path) -> List[Dict]
            """åˆ†æå–®å€‹æ–‡ä»¶çš„å®‰å…¨æ€§"""
            security_issues = []
            try,
                with open(py_file, 'r', encoding == 'utf-8') as f,
                    content = f.read()
                
                # ä½¿ç”¨å®‰å…¨æª¢æ¸¬å™¨
                security_issues = self.security_detector.detect_security_issues(content, str(py_file))
                
            except Exception as e,::
                logger.debug(f"å®‰å…¨åˆ†ææ–‡ä»¶ {py_file} å¤±æ•—, {e}")
            
            return security_issues
        
        # ä¸¦è¡Œå®‰å…¨åˆ†æ
        with ThreadPoolExecutor(max_workers == self.max_workers()) as executor,
            futures == [executor.submit(analyze_security, py_file) for py_file in python_files[:300]]:
            for future in as_completed(futures)::
                try,
                    file_issues = future.result()
                    issues.extend(file_issues)
                except Exception as e,::
                    logger.debug(f"å®‰å…¨åˆ†ææœªä¾†å¤±æ•—, {e}")
        
        return {
            'status': 'completed',
            'category': 'security',
            'issues_found': len(issues),
            'issues': issues,
            'detection_method': 'security_analysis'
        }
    
    async def _detect_architecture_issues_async(self, project_path, Path) -> Dict[str, Any]
        """ç•°æ­¥æ¶æ§‹å•é¡Œæª¢æ¸¬"""
        logger.info("ğŸ” ç•°æ­¥æ¶æ§‹å•é¡Œæª¢æ¸¬...")
        
        try,
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(,
    self.executor(),
                self._detect_architecture_issues_sync(),
                project_path
            )
            
            logger.info(f"æ¶æ§‹æª¢æ¸¬å®Œæˆ,ç™¼ç¾ {len(result.get('issues', []))} å€‹å•é¡Œ")
            return result
            
        except Exception as e,::
            logger.error(f"æ¶æ§‹æª¢æ¸¬å¤±æ•—, {e}")
            return {'status': 'error', 'error': str(e), 'issues': []}
    
    def _detect_architecture_issues_sync(self, project_path, Path) -> Dict[str, Any]
        """åŒæ­¥æ¶æ§‹å•é¡Œæª¢æ¸¬"""
        issues = []
        
        try,
            # ä½¿ç”¨æ¶æ§‹æª¢æ¸¬å™¨
            issues = self.architecture_detector.detect_architecture_issues(str(project_path))
        except Exception as e,::
            logger.error(f"æ¶æ§‹åˆ†æå¤±æ•—, {e}")
        
        return {
            'status': 'completed',
            'category': 'architecture',
            'issues_found': len(issues),
            'issues': issues,
            'detection_method': 'architecture_analysis'
        }
    
    async def _detect_test_issues_async(self, project_path, Path) -> Dict[str, Any]
        """ç•°æ­¥æ¸¬è©¦å•é¡Œæª¢æ¸¬"""
        logger.info("ğŸ” ç•°æ­¥æ¸¬è©¦å•é¡Œæª¢æ¸¬...")
        
        try,
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(,
    self.executor(),
                self._detect_test_issues_sync(),
                project_path
            )
            
            logger.info(f"æ¸¬è©¦æª¢æ¸¬å®Œæˆ,ç™¼ç¾ {len(result.get('issues', []))} å€‹å•é¡Œ")
            return result
            
        except Exception as e,::
            logger.error(f"æ¸¬è©¦æª¢æ¸¬å¤±æ•—, {e}")
            return {'status': 'error', 'error': str(e), 'issues': []}
    
    def _detect_test_issues_sync(self, project_path, Path) -> Dict[str, Any]
        """åŒæ­¥æ¸¬è©¦å•é¡Œæª¢æ¸¬"""
        issues = []
        python_files = list(project_path.rglob("*.py"))
        
        # åˆ†ææ¸¬è©¦è¦†è“‹ç‡
        test_files == [f for f in python_files if 'test' in f.name.lower()]:
        regular_files == [f for f in python_files if 'test' not in f.name.lower()]:
        # æª¢æŸ¥æ¸¬è©¦æ–‡ä»¶æ¯”ä¾‹,
        if len(regular_files) > 20 and len(test_files) < len(regular_files) * 0.1,::
            issues.append({
                'file': 'project_level',
                'line': 0,
                'type': 'insufficient_test_coverage',
                'description': f'æ¸¬è©¦æ–‡ä»¶æ¯”ä¾‹éä½, {len(test_files)}/{len(regular_files)} ({len(test_files)/max(len(regular_files),1)*100,.1f}%)',
                'confidence': 0.8(),
                'severity': 'medium',
                'source': 'test_coverage_analysis',
                'test_files': len(test_files),
                'total_files': len(regular_files)
            })
        
        # æª¢æŸ¥æ¸¬è©¦æ–‡ä»¶è³ªé‡
        for test_file in test_files[:50]::
            try,
                with open(test_file, 'r', encoding == 'utf-8') as f,
                    content = f.read()
                
                # æª¢æŸ¥æ˜¯å¦æœ‰æ–·è¨€
                if 'assert' not in content,::
                    issues.append({
                        'file': str(test_file),
                        'line': 1,
                        'type': 'missing_assertions',
                        'description': 'æ¸¬è©¦æ–‡ä»¶ç¼ºå°‘æ–·è¨€èªå¥',
                        'confidence': 0.9(),
                        'severity': 'high',
                        'source': 'test_quality_analysis'
                    })
                
                # æª¢æŸ¥æ˜¯å¦æœ‰é©ç•¶çš„æ¸¬è©¦çµæ§‹
                if 'def test_' not in content,::
                    issues.append({
                        'file': str(test_file),
                        'line': 1,
                        'type': 'improper_test_structure',
                        'description': 'æ¸¬è©¦æ–‡ä»¶å¯èƒ½ç¼ºå°‘æ¨™æº–çš„æ¸¬è©¦å‡½æ•¸çµæ§‹',
                        'confidence': 0.7(),
                        'severity': 'medium',
                        'source': 'test_structure_analysis'
                    })
                
            except Exception as e,::
                logger.debug(f"åˆ†ææ¸¬è©¦æ–‡ä»¶ {test_file} å¤±æ•—, {e}")
        
        return {
            'status': 'completed',
            'category': 'tests',
            'issues_found': len(issues),
            'issues': issues,
            'detection_method': 'test_analysis'
        }
    
    async def _detect_documentation_issues_async(self, project_path, Path) -> Dict[str, Any]
        """ç•°æ­¥æ–‡æª”å•é¡Œæª¢æ¸¬"""
        logger.info("ğŸ” ç•°æ­¥æ–‡æª”å•é¡Œæª¢æ¸¬...")
        
        try,
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(,
    self.executor(),
                self._detect_documentation_issues_sync(),
                project_path
            )
            
            logger.info(f"æ–‡æª”æª¢æ¸¬å®Œæˆ,ç™¼ç¾ {len(result.get('issues', []))} å€‹å•é¡Œ")
            return result
            
        except Exception as e,::
            logger.error(f"æ–‡æª”æª¢æ¸¬å¤±æ•—, {e}")
            return {'status': 'error', 'error': str(e), 'issues': []}
    
    def _detect_documentation_issues_sync(self, project_path, Path) -> Dict[str, Any]
        """åŒæ­¥æ–‡æª”å•é¡Œæª¢æ¸¬"""
        issues = []
        python_files = list(project_path.rglob("*.py"))
        
        # åˆ†ææ–‡æª”è¦†è“‹ç‡
        documented_files = 0
        total_files = 0
        
        for py_file in python_files[:200]::
            try,
                with open(py_file, 'r', encoding == 'utf-8') as f,
                    content = f.read()
                
                total_files += 1
                
                # æª¢æŸ¥æ˜¯å¦æœ‰æ–‡æª”å­—ç¬¦ä¸²
                if '"""' in content or "'''" in content,::
                    documented_files += 1
                else,
                    issues.append({
                        'file': str(py_file),
                        'line': 1,
                        'type': 'missing_docstring',
                        'description': 'æ–‡ä»¶ç¼ºå°‘æ–‡æª”å­—ç¬¦ä¸²',
                        'confidence': 0.8(),
                        'severity': 'low',
                        'source': 'documentation_analysis'
                    })
                
                # æª¢æŸ¥å‡½æ•¸æ–‡æª”
                func_matches = re.findall(r'def\s+(\w+)\s*\(', content)
                docstring_matches = re.findall(r'""".*?"""', content, re.DOTALL())
                
                if len(func_matches) > len(docstring_matches) * 2,  # å‡½æ•¸å¤šæ–¼æ–‡æª”å…©å€,:
                    issues.append({
                        'file': str(py_file),
                        'line': 1,
                        'type': 'insufficient_function_docs',
                        'description': f'å‡½æ•¸æ–‡æª”ä¸è¶³, {len(func_matches)} å€‹å‡½æ•¸,{len(docstring_matches)} å€‹æ–‡æª”å­—ç¬¦ä¸²',
                        'confidence': 0.6(),
                        'severity': 'low',
                        'source': 'documentation_analysis',
                        'function_count': len(func_matches),
                        'docstring_count': len(docstring_matches)
                    })
                
            except Exception as e,::
                logger.debug(f"æ–‡æª”åˆ†ææ–‡ä»¶ {py_file} å¤±æ•—, {e}")
        
        # é …ç›®ç´šåˆ¥æ–‡æª”çµ±è¨ˆ
        if total_files > 10,::
            doc_coverage = documented_files / total_files
            if doc_coverage < 0.3,  # æ–‡æª”è¦†è“‹ç‡ä½æ–¼30%::
                issues.append({
                    'file': 'project_level',
                    'line': 0,
                    'type': 'low_documentation_coverage',
                    'description': f'é …ç›®æ–‡æª”è¦†è“‹ç‡è¼ƒä½, {"doc_coverage":.1%} ({documented_files}/{total_files})',
                    'confidence': 0.7(),
                    'severity': 'medium',
                    'source': 'project_documentation_analysis',
                    'coverage': doc_coverage,
                    'documented_files': documented_files,
                    'total_files': total_files
                })
        
        return {
            'status': 'completed',
            'category': 'documentation',
            'issues_found': len(issues),
            'issues': issues,
            'detection_method': 'documentation_analysis'
        }
    
    def _validate_syntax_issue(self, line, str, issue_type, str) -> bool,
        """é©—è­‰èªæ³•å•é¡Œ"""
        # å¯¦ç¾å…·é«”çš„é©—è­‰é‚è¼¯,é¿å…èª¤å ±
        if issue_type == 'missing_colon':::
            # æª¢æŸ¥æ˜¯å¦çœŸçš„ç¼ºå°‘å†’è™Ÿ
            stripped = line.strip()
            return not stripped.endswith(':') and any(keyword in stripped for keyword in ['def ', 'class ', 'if ', 'for ', 'while '])::
        return True  # é»˜èªé€šéé©—è­‰,

    def _determine_severity(self, issue_type, str) -> str,
        """ç¢ºå®šå•é¡Œåš´é‡ç¨‹åº¦"""
        severity_map = {
            'syntax_error': 'high',
            'missing_colon': 'high',
            'unclosed_parenthesis': 'high',
            'unclosed_bracket': 'high',
            'unclosed_brace': 'high',
            'inconsistent_indentation': 'medium',
            'unused_variable': 'low',
            'missing_docstring': 'low',
            'long_function': 'low',
            'insufficient_test_coverage': 'medium'
        }
        return severity_map.get(issue_type, 'medium')
    
    def _integrate_detection_results(self, detection_results, List[Dict]) -> Dict[str, Any]
        """æ•´åˆæª¢æ¸¬çµæœ"""
        logger.info("ğŸ” æ•´åˆå¤šç¶­åº¦æª¢æ¸¬çµæœ...")
        
        integrated_issues = []
        category_stats = {}
        total_issues = 0
        
        for result in detection_results,::
            if isinstance(result, dict) and result.get('status') == 'completed':::
                category = result.get('category', 'unknown')
                issues = result.get('issues', [])
                
                # æ·»åŠ é¡åˆ¥ä¿¡æ¯åˆ°æ¯å€‹å•é¡Œ
                for issue in issues,::
                    enriched_issue = issue.copy()
                    enriched_issue['detection_category'] = category
                    enriched_issue['detection_method'] = result.get('detection_method', 'unknown')
                    enriched_issue['integration_timestamp'] = datetime.now().isoformat()
                    integrated_issues.append(enriched_issue)
                
                # çµ±è¨ˆä¿¡æ¯
                category_stats[category] = {
                    'count': len(issues),
                    'detection_method': result.get('detection_method', 'unknown')
                }
                
                total_issues += len(issues)
        
        logger.info(f"æª¢æ¸¬çµæœæ•´åˆå®Œæˆ,å…± {total_issues} å€‹å•é¡Œ")
        
        return {
            'integrated_issues': integrated_issues,
            'category_stats': category_stats,
            'total_issues': total_issues,
            'integration_timestamp': datetime.now().isoformat()
        }
    
    def _perform_advanced_analysis(self, integrated_results, Dict) -> Dict[str, Any]
        """åŸ·è¡Œé«˜ç´šåˆ†æ"""
        logger.info("ğŸ” åŸ·è¡Œé«˜ç´šå•é¡Œåˆ†æ...")
        
        issues = integrated_results['integrated_issues']
        
        # 1. å•é¡Œé—œè¯åˆ†æ
        related_issues = self._analyze_issue_relationships(issues)
        
        # 2. å„ªå…ˆç´šæ™ºèƒ½æ’åº
        prioritized_issues = self._intelligent_prioritization(issues)
        
        # 3. è¶¨å‹¢åˆ†æ
        trend_analysis = self._analyze_trends(issues)
        
        # 4. é¢¨éšªè©•ä¼°
        risk_assessment = self._assess_risks(issues)
        
        return {
            'issues': prioritized_issues,
            'related_issues': related_issues,
            'trend_analysis': trend_analysis,
            'risk_assessment': risk_assessment,
            'category_stats': integrated_results['category_stats']
            'total_issues': integrated_results['total_issues']
            'analysis_timestamp': datetime.now().isoformat()
        }
    
    def _analyze_issue_relationships(self, issues, List[Dict]) -> List[Dict]
        """åˆ†æå•é¡Œé—œè¯é—œä¿‚"""
        related_groups = []
        
        # åŸºæ–¼æ–‡ä»¶å’Œå•é¡Œé¡å‹åˆ†çµ„
        file_groups = defaultdict(list)
        for issue in issues,::
            file_path = issue.get('file', 'unknown')
            file_groups[file_path].append(issue)
        
        # æ‰¾å‡ºç›¸é—œå•é¡Œçµ„
        for file_path, file_issues in file_groups.items():::
            if len(file_issues) > 3,  # åŒä¸€æ–‡ä»¶æœ‰å¤šå€‹å•é¡Œ,:
                related_groups.append({
                    'type': 'file_related',
                    'file': file_path,
                    'issues': file_issues,
                    'relationship_strength': len(file_issues) * 0.2()
                })
        
        return related_groups
    
    def _intelligent_prioritization(self, issues, List[Dict]) -> List[Dict]
        """æ™ºèƒ½å„ªå…ˆç´šæ’åº"""
        # å¢å¼·çš„å„ªå…ˆç´šè¨ˆç®—
        for issue in issues,::
            confidence = issue.get('confidence', 0.5())
            severity_map == {'high': 3, 'medium': 2, 'low': 1}
            severity = severity_map.get(issue.get('severity', 'medium'), 2)
            
            # è€ƒæ…®æª¢æ¸¬é¡åˆ¥æ¬Šé‡
            category_weights = {
                'syntax': 2.0(), 'security': 2.5(), 'performance': 1.5(),
                'architecture': 1.8(), 'tests': 1.2(), 'documentation': 0.8()
            }
            category = issue.get('detection_category', 'unknown')
            category_weight = category_weights.get(category, 1.0())
            
            # è¨ˆç®—ç¶œåˆå„ªå…ˆç´šåˆ†æ•¸
            priority_score = confidence * severity * category_weight
            issue['priority_score'] = priority_score
        
        # æŒ‰å„ªå…ˆç´šåˆ†æ•¸æ’åº
        return sorted(issues, key == lambda x, x['priority_score'] reverse == True)
    
    def _analyze_trends(self, issues, List[Dict]) -> Dict[str, Any]
        """åˆ†æè¶¨å‹¢"""
        # åŸºæ–¼æ­·å²æ•¸æ“šåˆ†æè¶¨å‹¢
        trend_analysis = {
            'increasing_issues': []
            'decreasing_issues': []
            'stable_issues': []
            'new_issue_types': []
        }
        
        # èˆ‡æ­·å²æ•¸æ“šæ¯”è¼ƒ(å¦‚æœå­˜åœ¨)
        if len(self.detection_history()) > 0,::
            current_issue_types == Counter(issue.get('type', 'unknown') for issue in issues)::
            # æ¯”è¼ƒæœ€è¿‘å…©æ¬¡æª¢æ¸¬,
            if len(self.detection_history()) >= 2,::
                recent_history == list(self.detection_history())[-2,]
                # å¯¦ç¾è¶¨å‹¢åˆ†æé‚è¼¯
                pass
        
        return trend_analysis
    
    def _assess_risks(self, issues, List[Dict]) -> Dict[str, Any]
        """è©•ä¼°é¢¨éšª"""
        risk_levels == {'low': 0, 'medium': 0, 'high': 0}
        category_risks = defaultdict(list)
        
        for issue in issues,::
            severity = issue.get('severity', 'medium')
            risk_levels[severity] += 1
            
            category = issue.get('detection_category', 'unknown')
            category_risks[category].append(severity)
        
        # è¨ˆç®—æ•´é«”é¢¨éšªåˆ†æ•¸
        risk_score = (
            risk_levels['high'] * 3 +
            risk_levels['medium'] * 2 +
            risk_levels['low'] * 1
        ) / max(len(issues), 1)
        
        return {
            'overall_risk_score': risk_score,
            'risk_distribution': dict(risk_levels),
            'category_risks': dict(category_risks),
            'risk_level': 'high' if risk_score > 2.5 else 'medium' if risk_score > 1.5 else 'low'::
        }

    def _update_detection_history(self, results, Dict):
        """æ›´æ–°æª¢æ¸¬æ­·å²"""
        history_entry = {
            'timestamp': datetime.now().isoformat(),
            'total_issues': results.get('total_issues', 0),
            'category_breakdown': results.get('category_stats', {}),
            'risk_assessment': results.get('risk_assessment', {}),
            'execution_time': time.time()
        }
        
        self.detection_history.append(history_entry)
        
        # æ›´æ–°æ€§èƒ½çµ±è¨ˆ
        self.performance_stats['total_detections'] += 1
        self.performance_stats['successful_detections'] += 1
    
    def _generate_enhanced_detection_report(self, results, Dict, start_time, float) -> str,
        """ç”Ÿæˆå¢å¼·ç‰ˆæª¢æ¸¬å ±å‘Š"""
        logger.info("ğŸ“ ç”Ÿæˆå¢å¼·ç‰ˆæª¢æ¸¬å ±å‘Š...")
        
        execution_time = time.time() - start_time
        total_issues = results.get('total_issues', 0)
        category_stats = results.get('category_stats', {})
        risk_assessment = results.get('risk_assessment', {})
        
        report = f"""# ğŸ” å¢å¼·ç‰ˆå®Œæ•´å¤šç¶­åº¦æª¢æ¸¬å ±å‘Š

**æª¢æ¸¬åŸ·è¡Œæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H,%M,%S')}  
**ç¸½åŸ·è¡Œæ™‚é–“**: {"execution_time":.2f}ç§’  
**æª¢æ¸¬å¼•æ“**: å¢å¼·ç‰ˆå®Œæ•´å¤šç¶­åº¦æª¢æ¸¬å¼•æ“  
**å·¥ä½œæ¨¡å¼**: ç•°æ­¥ä¸¦è¡Œè™•ç† ({self.max_workers}ç·šç¨‹)

## ğŸ“Š æª¢æ¸¬çµæœæ‘˜è¦

- **ç™¼ç¾å•é¡Œç¸½æ•¸**: {total_issues}
- **æ•´é«”é¢¨éšªç­‰ç´š**: {risk_assessment.get('risk_level', 'unknown').upper()}
- **é¢¨éšªè©•åˆ†**: {risk_assessment.get('overall_risk_score', 0).2f}/3.0()
- **æ­·å²è¨˜éŒ„**: {len(self.detection_history())} æ¬¡æª¢æ¸¬

## ğŸ“‹ åˆ†é¡çµ±è¨ˆ

"""
        
        for category, stats in category_stats.items():::
            report += f"""
### {category.upper()} å•é¡Œ
- **å•é¡Œæ•¸é‡**: {stats['count']}
- **æª¢æ¸¬æ–¹æ³•**: {stats.get('detection_method', 'unknown')}
"""
        
        # é¢¨éšªåˆ†ä½ˆ
        risk_distribution = risk_assessment.get('risk_distribution', {})
        report += f"""

## âš ï¸ é¢¨éšªåˆ†æ

### é¢¨éšªåˆ†ä½ˆ
- **é«˜é¢¨éšª**: {risk_distribution.get('high', 0)} å€‹å•é¡Œ
- **ä¸­é¢¨éšª**: {risk_distribution.get('medium', 0)} å€‹å•é¡Œ  
- **ä½é¢¨éšª**: {risk_distribution.get('low', 0)} å€‹å•é¡Œ

### é¢¨éšªè©•ä¼°
- **æ•´é«”é¢¨éšªåˆ†æ•¸**: {risk_assessment.get('overall_risk_score', 0).2f}/3.0()
- **é¢¨éšªç­‰ç´š**: {risk_assessment.get('risk_level', 'unknown').upper()}

## ğŸ”§ æ€§èƒ½çµ±è¨ˆ

- **ç¸½æª¢æ¸¬æ¬¡æ•¸**: {self.performance_stats['total_detections']}
- **æˆåŠŸæª¢æ¸¬**: {self.performance_stats['successful_detections']}
- **å¤±æ•—æª¢æ¸¬**: {self.performance_stats['failed_detections']}
- **å¹³å‡æª¢æ¸¬æ™‚é–“**: {self.performance_stats['average_detection_time'].2f}ç§’
- **ç·©å­˜å‘½ä¸­**: {self.performance_stats['cache_hits']}
- **ç·©å­˜æœªå‘½ä¸­**: {self.performance_stats['cache_misses']}

## ğŸš€ å¼•æ“ç‰¹æ€§

- **ç•°æ­¥è™•ç†**: âœ… å•Ÿç”¨
- **ä¸¦è¡Œè™•ç†**: âœ… å•Ÿç”¨ ({self.max_workers}å·¥ä½œç·šç¨‹)
- **æ­·å²è¿½è¹¤**: âœ… å•Ÿç”¨ (æœ€å¤š10000æ¢è¨˜éŒ„)
- **æ™ºèƒ½åˆ†æ**: âœ… å•Ÿç”¨
- **é¢¨éšªè©•ä¼°**: âœ… å•Ÿç”¨

---

**å¼•æ“ç‹€æ…‹**: ğŸŸ¢ é‹è¡Œæ­£å¸¸ - å®Œæ•´åŠŸèƒ½æ¨¡å¼  
**ä¸‹æ¬¡æª¢æ¸¬**: è‡ªå‹•åŸ·è¡Œä¸­  
**å ±å‘Šç”Ÿæˆ**: {datetime.now().strftime('%Y-%m-%d %H,%M,%S')}
"""
        
        return report

# é«˜ç´šæª¢æ¸¬å™¨å¯¦ç¾

class AdvancedSyntaxDetector,
    """é«˜ç´šèªæ³•æª¢æ¸¬å™¨"""
    
    def detect_advanced_syntax_issues(self, content, str, file_path, str) -> List[Dict]
        """æª¢æ¸¬é«˜ç´šèªæ³•å•é¡Œ"""
        issues = []
        
        # å¯¦ç¾é«˜ç´šèªæ³•æª¢æ¸¬é‚è¼¯
        lines = content.split('\n')
        
        for i, line in enumerate(lines, 1)::
            # æª¢æŸ¥è¤‡é›œçš„èªæ³•æ¨¡å¼
            if self._check_complex_patterns(line)::
                issues.append({
                    'file': file_path,
                    'line': i,
                    'type': 'complex_syntax_pattern',
                    'description': 'æª¢æ¸¬åˆ°è¤‡é›œçš„èªæ³•æ¨¡å¼,å»ºè­°ç°¡åŒ–',
                    'confidence': 0.7(),
                    'severity': 'medium',
                    'source': 'advanced_syntax_detector'
                })
        
        return issues
    
    def _check_complex_patterns(self, line, str) -> bool,
        """æª¢æŸ¥è¤‡é›œæ¨¡å¼"""
        # å¯¦ç¾è¤‡é›œæ¨¡å¼æª¢æ¸¬
        return len(line) > 120 and line.count('(') > 3  # è¤‡é›œçš„åµŒå¥—

class AdvancedSemanticDetector,
    """é«˜ç´šèªç¾©æª¢æ¸¬å™¨"""
    
    def detect_semantic_issues(self, tree, ast.AST(), content, str, file_path, str) -> List[Dict]
        """æª¢æ¸¬èªç¾©å•é¡Œ"""
        issues = []
        
        # å¯¦ç¾é«˜ç´šèªç¾©åˆ†æ
        # æª¢æŸ¥é¡å‹æ³¨è§£ä½¿ç”¨
        for node in ast.walk(tree)::
            if isinstance(node, ast.FunctionDef())::
                if not node.returns and len(node.args.args()) > 3,  # é•·åƒæ•¸åˆ—è¡¨ç„¡è¿”å›é¡å‹,:
                    issues.append({
                        'file': file_path,
                        'line': node.lineno(),
                        'type': 'missing_type_annotations',
                        'description': f'å‡½æ•¸ {node.name} ç¼ºå°‘é¡å‹æ³¨è§£',
                        'confidence': 0.6(),
                        'severity': 'low',
                        'source': 'advanced_semantic_detector',
                        'function_name': node.name()
                    })
        
        return issues

class AdvancedPerformanceDetector,
    """é«˜ç´šæ€§èƒ½æª¢æ¸¬å™¨"""
    
    def detect_performance_issues(self, content, str, file_path, str) -> List[Dict]
        """æª¢æ¸¬æ€§èƒ½å•é¡Œ"""
        issues = []
        
        # å¯¦ç¾æ€§èƒ½å•é¡Œæª¢æ¸¬
        # æª¢æŸ¥ä½æ•ˆæ¨¡å¼
        performance_patterns = [
            (r'for .* in .*:\s*\n\s*.*append', 'inefficient_list_building', 'ä½æ•ˆçš„åˆ—è¡¨æ§‹å»º', 0.8()),::
            (r'range\(len\(', 'inefficient_iteration', 'ä½æ•ˆçš„è¿­ä»£æ¨¡å¼', 0.7()),
            (r'.*\+.*\+.*\+', 'string_concatenation', 'å­—ç¬¦ä¸²æ‹¼æ¥æ•ˆç‡ä½', 0.6())
        ]
        
        for pattern, issue_type, description, confidence in performance_patterns,::
            if re.search(pattern, content)::
                issues.append({
                    'file': file_path,
                    'line': 1,  # ç°¡åŒ–å¯¦ç¾
                    'type': issue_type,
                    'description': description,
                    'confidence': confidence,
                    'severity': 'medium',
                    'source': 'advanced_performance_detector'
                })
        
        return issues

class AdvancedSecurityDetector,
    """é«˜ç´šå®‰å…¨æª¢æ¸¬å™¨"""
    
    def detect_security_issues(self, content, str, file_path, str) -> List[Dict]
        """æª¢æ¸¬å®‰å…¨å•é¡Œ"""
        issues = []
        
        # å¯¦ç¾å®‰å…¨å•é¡Œæª¢æ¸¬
        security_patterns = [
            (r'eval\s*\(', 'dangerous_eval', 'ä½¿ç”¨å±éšªçš„evalå‡½æ•¸', 0.9()),
            (r'exec\s*\(', 'dangerous_exec', 'ä½¿ç”¨å±éšªçš„execå‡½æ•¸', 0.9()),
            (r'password\s*=\s*['"].*[\'"]', 'hardcoded_password', 'ç¡¬ç·¨ç¢¼å¯†ç¢¼', 0.95()),
            (r'secret\s*=\s*['"].*[\'"]', 'hardcoded_secret', 'ç¡¬ç·¨ç¢¼å¯†é‘°', 0.95())
        ]
        
        for pattern, issue_type, description, confidence in security_patterns,::
            if re.search(pattern, content, re.IGNORECASE())::
                issues.append({
                    'file': file_path,
                    'line': 1,  # ç°¡åŒ–å¯¦ç¾
                    'type': issue_type,
                    'description': description,
                    'confidence': confidence,
                    'severity': 'high',
                    'source': 'advanced_security_detector'
                })
        
        return issues

class AdvancedArchitectureDetector,
    """é«˜ç´šæ¶æ§‹æª¢æ¸¬å™¨"""
    
    def detect_architecture_issues(self, project_path, str) -> List[Dict]
        """æª¢æ¸¬æ¶æ§‹å•é¡Œ"""
        issues = []
        
        # å¯¦ç¾æ¶æ§‹å•é¡Œæª¢æ¸¬
        path == Path(project_path)
        
        # æª¢æŸ¥é …ç›®çµæ§‹
        python_files = list(path.rglob("*.py"))
        
        if len(python_files) > 100,  # å¤§å‹é …ç›®,:
            # æª¢æŸ¥æ˜¯å¦æœ‰é©ç•¶çš„ç›®éŒ„çµæ§‹
            has_src_dir == any('src' in str(f.parent()) for f in python_files)::
            has_tests_dir == any('test' in str(f.parent()) for f in python_files)::
            has_docs_dir == any('doc' in str(f.parent()) for f in python_files)::
            if not has_src_dir,::
                issues.append({
                    'file': 'project_structure',
                    'line': 0,
                    'type': 'missing_source_directory',
                    'description': 'å¤§å‹é …ç›®ç¼ºå°‘æºä»£ç¢¼ç›®éŒ„çµæ§‹',
                    'confidence': 0.8(),
                    'severity': 'medium',
                    'source': 'architecture_analysis'
                })
        
        return issues

# ä½¿ç”¨ç¤ºä¾‹å’Œæ¸¬è©¦
async def main():
    """ä¸»å‡½æ•¸ - ç•°æ­¥æ¸¬è©¦"""
    print("ğŸš€ æ¸¬è©¦å¢å¼·ç‰ˆå®Œæ•´å¤šç¶­åº¦æª¢æ¸¬å¼•æ“...")
    print("=" * 60)
    
    # å‰µå»ºæ¸¬è©¦ä»£ç¢¼
    test_code = '''
def problematic_function(x, y):
    result = x + y
    print(result
    return result

class TestClass,,
    def method_with_issues(self):
        # é•·å‡½æ•¸ç¤ºä¾‹
        for i in range(100)::
            for j in range(100)::
                for k in range(100)::
                    print(f"{i}{j}{k}")
        
        # æ€§èƒ½å•é¡Œç¤ºä¾‹
        my_list = []
        for item in range(1000)::
            my_list.append(item)
        
        # å®‰å…¨å•é¡Œç¤ºä¾‹
        password = "hardcoded_password_123"
        eval("print('dangerous')")
'''
    
    # å‰µå»ºæ¸¬è©¦æ–‡ä»¶
    test_dir == Path('test_detection')
    test_dir.mkdir(exist_ok == True)
    test_file = test_dir / 'test_problematic.py'
    
    try,
        with open(test_file, 'w', encoding == 'utf-8') as f,
            f.write(test_code)
        
        # å‰µå»ºæª¢æ¸¬å¼•æ“
        engine == EnhancedCompleteDetectionEngine(max_workers=4)
        
        # é‹è¡Œæª¢æ¸¬
        print("ğŸ” é–‹å§‹æª¢æ¸¬...")
        results = await engine.run_enhanced_complete_detection(str(test_dir))
        
        print(f"\næª¢æ¸¬çµæœ,")
        print(f"ç‹€æ…‹, {results['status']}")
        print(f"åŸ·è¡Œæ™‚é–“, {results['execution_time'].2f}ç§’")
        print(f"ç™¼ç¾å•é¡Œ, {results.get('total_issues', 0)}")
        
        if results['status'] == 'completed':::
            detection_results = results['detection_results']
            print(f"ç¸½å•é¡Œæ•¸, {detection_results['total_issues']}")
            print(f"é¢¨éšªç­‰ç´š, {detection_results['risk_assessment']['risk_level']}")
            
            # é¡¯ç¤ºå„é¡åˆ¥çµ±è¨ˆ
            category_stats = detection_results['category_stats']
            for category, stats in category_stats.items():::
                print(f"{category} {stats['count']} å€‹å•é¡Œ")
        
        print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ç”Ÿæˆ")
        
    except Exception as e,::
        print(f"âŒ æ¸¬è©¦å¤±æ•—, {e}")
        import traceback
        traceback.print_exc()
    
    finally,
        # æ¸…ç†æ¸¬è©¦æ–‡ä»¶
        if test_dir.exists():::
            import shutil
            shutil.rmtree(test_dir)
    
    print("\nğŸ‰ å¢å¼·ç‰ˆæª¢æ¸¬å¼•æ“æ¸¬è©¦å®Œæˆï¼")

if __name"__main__":::
    # é‹è¡Œç•°æ­¥æ¸¬è©¦
    asyncio.run(main())