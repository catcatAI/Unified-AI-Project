#!/usr/bin/env python3
"""
Test Real Intelligence Backend
æµ‹è¯•çœŸæ­£æ™ºèƒ½çš„ç³»ç»Ÿ
"""
import json
import time
from datetime import datetime

def test_real_intelligence():
    """æµ‹è¯•çœŸæ­£çš„æ™ºèƒ½ç³»ç»Ÿ"""
    print("ğŸ§  æµ‹è¯•çœŸæ­£æ™ºèƒ½çš„åç«¯ç³»ç»Ÿ")
    print("=" * 60)
    
    try:
        # å¯¼å…¥å’Œæµ‹è¯•ç»„ä»¶
        import sys
        import os
        sys.path.insert(0, '.')
        
        # æµ‹è¯•Conversation Engine
        print("\nğŸ“‹ æµ‹è¯• 1: Conversation Engine")
        try:
            from apps.backend.src.services.conversation_engine import ConversationEngine
            engine = ConversationEngine()
            
            test_input = "è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½"
            result = engine.process(test_input)
            
            print(f"è¾“å…¥: {test_input}")
            print(f"å“åº”: {result.get('response', '')[:100]}...")
            print(f"ç±»å‹: {result.get('type', 'unknown')}")
            print(f"ç½®ä¿¡åº¦: {result.get('confidence', 0):.3f}")
            print(f"å“åº”é•¿åº¦: {len(result.get('response', ''))}")
            
            if len(result.get('response', '')) > 20:
                print("âœ… Conversation Engineæ­£å¸¸å·¥ä½œ")
                conversation_engine_working = True
            else:
                print("âŒ Conversation Engineå“åº”è¿‡çŸ­")
                conversation_engine_working = False
                
        except Exception as e:
            print(f"âŒ Conversation Engineæµ‹è¯•å¤±è´¥: {e}")
            conversation_engine_working = False
        
        # æµ‹è¯•ç®€å•LLM
        print("\nğŸ“‹ æµ‹è¯• 2: Simple LLM")
        try:
            from apps.backend.src.services.simple_llm import generate_sync
            
            test_input = "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ"
            response = generate_sync(test_input, max_tokens=100)
            
            print(f"è¾“å…¥: {test_input}")
            print(f"å“åº”: {response[:100]}...")
            print(f"å“åº”é•¿åº¦: {len(response)}")
            
            if len(response) > 20 and not response.startswith("[Error"):
                print("âœ… Simple LLMæ­£å¸¸å·¥ä½œ")
                simple_llm_working = True
            else:
                print("âŒ Simple LLMå·¥ä½œå¼‚å¸¸")
                simple_llm_working = False
                
        except Exception as e:
            print(f"âŒ Simple LLMæµ‹è¯•å¤±è´¥: {e}")
            simple_llm_working = False
        
        # æµ‹è¯•Ollamaé›†æˆ
        print("\nğŸ“‹ æµ‹è¯• 3: Ollama Integration")
        try:
            # ç›´æ¥æµ‹è¯•Ollamaè¿æ¥
            import subprocess
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0 and 'phi3:3.8b' in result.stdout:
                print("âœ… Ollamaå¯ç”¨ï¼Œphi3:3.8bæ¨¡å‹å°±ç»ª")
                ollama_available = True
                
                # æµ‹è¯•Ollamaç”Ÿæˆ
                try:
                    test_result = subprocess.run([
                        'ollama', 'run', 'phi3:3.8b', 
                        'ä»€ä¹ˆæ˜¯AIï¼Ÿ', '--verbose'
                    ], capture_output=True, text=True, timeout=30)
                    
                    if test_result.returncode == 0:
                        response = test_result.stdout.strip()
                        print(f"Ollamaå“åº”: {response[:200]}...")
                        if len(response) > 50:
                            print("âœ… Ollamaæ¨¡å‹ç”Ÿæˆæ­£å¸¸")
                            ollama_generating = True
                        else:
                            print("âš ï¸ Ollamaå“åº”è¾ƒçŸ­")
                            ollama_generating = False
                    else:
                        print("âŒ Ollamaæ¨¡å‹ç”Ÿæˆå¤±è´¥")
                        ollama_generating = False
                        
                except Exception as e:
                    print(f"âŒ Ollamaç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
                    ollama_generating = False
            else:
                print("âŒ Ollamaä¸å¯ç”¨æˆ–æ¨¡å‹æœªå®‰è£…")
                ollama_available = False
                ollama_generating = False
                
        except Exception as e:
            print(f"âŒ Ollamaæµ‹è¯•å¤±è´¥: {e}")
            ollama_available = False
            ollama_generating = False
        
        # æµ‹è¯•HSM+CDMé›†æˆ
        print("\nğŸ“‹ æµ‹è¯• 4: HSM+CDM Integration")
        try:
            from phase2_hsm_cdm_engine import HSMCDMEngine
            
            engine = HSMCDMEngine()
            test_input = "è¯·è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"
            result = engine.process_input(test_input)
            
            print(f"è¾“å…¥: {test_input}")
            print(f"å“åº”: {result.get('response', '')[:100]}...")
            print(f"å­¦ä¹ è§¦å‘: {result.get('metadata', {}).get('learning_triggered', False)}")
            print(f"è®¤çŸ¥ç¼ºå£: {result.get('metadata', {}).get('gap_magnitude', 0):.3f}")
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æ˜¯æ¨¡æ¿å“åº”
            response = result.get('response', '')
            if test_input not in response and len(response) > 30:
                print("âœ… HSM+CDMç”Ÿæˆéæ¨¡æ¿å“åº”")
                hsm_cdm_improved = True
            else:
                print("âŒ HSM+CDMä»æ˜¯æ¨¡æ¿å“åº”")
                hsm_cdm_improved = False
                
            hsm_cdm_available = True
            
        except Exception as e:
            print(f"âŒ HSM+CDMæµ‹è¯•å¤±è´¥: {e}")
            hsm_cdm_available = False
            hsm_cdm_improved = False
        
        # è®¡ç®—æ€»ä½“æ™ºèƒ½æ°´å¹³
        print("\n" + "=" * 60)
        print("ğŸ“Š ç³»ç»Ÿæ™ºèƒ½è¯„ä¼°ç»“æœ")
        print("=" * 60)
        
        intelligent_components = 0
        total_components = 4
        
        if conversation_engine_working:
            intelligent_components += 1
            print("âœ… Conversation Engine: æ™ºèƒ½")
        else:
            print("âŒ Conversation Engine: ä¸å¯ç”¨")
            
        if simple_llm_working:
            intelligent_components += 1
            print("âœ… Simple LLM: æ™ºèƒ½")
        else:
            print("âŒ Simple LLM: ä¸å¯ç”¨")
            
        if ollama_available:
            intelligent_components += 1
            print("âœ… Ollama: å¯ç”¨")
        else:
            print("âŒ Ollama: ä¸å¯ç”¨")
            
        if hsm_cdm_improved:
            intelligent_components += 1
            print("âœ… HSM+CDM: æ”¹è¿›")
        else:
            print("âŒ HSM+CDM: éœ€è¦æ”¹è¿›")
        
        intelligence_level = (intelligent_components / total_components) * 100
        
        print(f"\nğŸ¯ æ™ºèƒ½åŒ–ç¨‹åº¦: {intelligent_components}/{total_components} ({intelligence_level:.1f}%)")
        
        # è¯„ä¼°
        if intelligence_level >= 75:
            level_desc = "é«˜ç­‰AI (æ¥è¿‘AGI)"
            action = "å¯ä»¥å¼€å§‹Phase 3"
        elif intelligence_level >= 50:
            level_desc = "ä¸­ç­‰AI (åŠŸèƒ½æ­£å¸¸)"
            action = "å»ºè®®å®Œå–„ç»„ä»¶"
        elif intelligence_level >= 25:
            level_desc = "åˆçº§AI (åŸºç¡€åŠŸèƒ½)"
            action = "éœ€è¦é‡è¦æ”¹è¿›"
        else:
            level_desc = "AIæ¡†æ¶ (ç©ºå£³)"
            action = "éœ€è¦å®Œå…¨é‡å»º"
        
        print(f"ğŸ§  ç³»ç»Ÿç­‰çº§: {level_desc}")
        print(f"ğŸ¯ å»ºè®®è¡ŒåŠ¨: {action}")
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        test_report = {
            "test_time": datetime.now().isoformat(),
            "intelligent_components": intelligent_components,
            "total_components": total_components,
            "intelligence_level": intelligence_level,
            "system_level": level_desc,
            "recommended_action": action,
            "components": {
                "conversation_engine": {
                    "available": conversation_engine_working,
                    "status": "functional" if conversation_engine_working else "non-functional"
                },
                "simple_llm": {
                    "available": simple_llm_working,
                    "status": "functional" if simple_llm_working else "non-functional"
                },
                "ollama": {
                    "available": ollama_available,
                    "generating": ollama_generating,
                    "status": "functional" if ollama_available else "non-functional"
                },
                "hsm_cdm": {
                    "available": hsm_cdm_available,
                    "improved": hsm_cdm_improved,
                    "status": "improved" if hsm_cdm_improved else "template_based"
                }
            },
            "overall_assessment": {
                "real_intelligence": intelligence_level > 50,
                "ready_for_production": intelligence_level > 75,
                "need_improvements": intelligence_level < 100
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open('REAL_INTELLIGENCE_TEST_REPORT.json', 'w', encoding='utf-8') as f:
            json.dump(test_report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: REAL_INTELLIGENCE_TEST_REPORT.json")
        
        return test_report
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
        return {"error": str(e), "intelligence_level": 0}

if __name__ == "__main__":
    report = test_real_intelligence()
    
    intelligence_level = report.get("intelligence_level", 0)
    
    if intelligence_level > 50:
        print(f"\nâœ… ç³»ç»Ÿå…·å¤‡çœŸæ­£çš„æ™ºèƒ½èƒ½åŠ›")
        print(f"ğŸ¯ æ™ºèƒ½åŒ–ç¨‹åº¦: {intelligence_level:.1f}%")
    else:
        print(f"\nâš ï¸ ç³»ç»Ÿæ™ºèƒ½ç¨‹åº¦è¾ƒä½ ({intelligence_level:.1f}%)")
        print(f"ğŸ”§ éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›")