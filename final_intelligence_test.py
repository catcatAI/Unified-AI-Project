#!/usr/bin/env python3
"""
Final Intelligence Test
æœ€ç»ˆæ™ºèƒ½æµ‹è¯• - éªŒè¯çœŸæ­£æ™ºèƒ½çš„AIç³»ç»Ÿ
"""
import json
import subprocess
import time
from datetime import datetime

def test_ollama_integration():
    """æµ‹è¯•Ollamaé›†æˆ"""
    print("ğŸ” æµ‹è¯• 1: Ollama LLM é›†æˆ")
    print("-" * 40)
    
    test_inputs = [
        "è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
        "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†ï¼Ÿ",
        "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
        "AIçš„å‘å±•è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    results = []
    
    for i, test_input in enumerate(test_inputs, 1):
        print(f"\næµ‹è¯• {i}: {test_input}")
        
        # æµ‹è¯•Ollamaç”Ÿæˆ
        try:
            cmd = [
                "ollama", "run", "phi3:3.8b",
                test_input,
                "--verbose"
            ]
            
            start_time = time.time()
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=30
            )
            end_time = time.time()
            
            if result.returncode == 0:
                response = result.stdout.strip()
                processing_time = (end_time - start_time) * 1000
                
                print(f"âœ… å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
                print(f"âœ… å¤„ç†æ—¶é—´: {processing_time:.1f}ms")
                print(f"âœ… å“åº”é¢„è§ˆ: {response[:100]}...")
                
                # æ£€æŸ¥å“åº”è´¨é‡
                quality_score = 0
                if len(response) > 50:
                    quality_score += 0.3
                if test_input not in response:
                    quality_score += 0.3
                if len(response.split()) > 10:
                    quality_score += 0.2
                if "?" not in response and "ï¼Ÿ" not in response:
                    quality_score += 0.2
                
                results.append({
                    "input": test_input,
                    "response": response,
                    "processing_time_ms": processing_time,
                    "response_length": len(response),
                    "quality_score": quality_score,
                    "success": True
                })
                
                print(f"âœ… è´¨é‡è¯„åˆ†: {quality_score:.2f}")
                
            else:
                print(f"âŒ Ollamaé”™è¯¯: {result.stderr}")
                results.append({
                    "input": test_input,
                    "success": False,
                    "error": result.stderr
                })
                
        except subprocess.TimeoutExpired:
            print("âŒ å“åº”è¶…æ—¶")
            results.append({
                "input": test_input,
                "success": False,
                "error": "Timeout"
            })
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            results.append({
                "input": test_input,
                "success": False,
                "error": str(e)
            })
    
    return results

def test_different_models():
    """æµ‹è¯•ä¸åŒæ¨¡å‹"""
    print("\nğŸ” æµ‹è¯• 2: å¤šæ¨¡å‹å¯¹æ¯”")
    print("-" * 40)
    
    models = ["phi3:3.8b", "tinyllama:latest"]
    test_input = "è¯·ç®€å•è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
    
    model_results = {}
    
    for model in models:
        print(f"\næµ‹è¯•æ¨¡å‹: {model}")
        
        try:
            cmd = [
                "ollama", "run", model,
                test_input,
                "--verbose"
            ]
            
            start_time = time.time()
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=20
            )
            end_time = time.time()
            
            if result.returncode == 0:
                response = result.stdout.strip()
                processing_time = (end_time - start_time) * 1000
                
                model_results[model] = {
                    "response": response,
                    "processing_time_ms": processing_time,
                    "response_length": len(response),
                    "success": True
                }
                
                print(f"âœ… å“åº”: {response[:150]}...")
                print(f"âœ… æ—¶é—´: {processing_time:.1f}ms")
                
            else:
                print(f"âŒ æ¨¡å‹ {model} å¤±è´¥")
                model_results[model] = {
                    "success": False,
                    "error": result.stderr
                }
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹ {model} å¼‚å¸¸: {e}")
            model_results[model] = {
                "success": False,
                "error": str(e)
            }
    
    return model_results

def assess_intelligence_level(ollama_results, model_results):
    """è¯„ä¼°æ™ºèƒ½æ°´å¹³"""
    print("\nğŸ” æµ‹è¯• 3: æ™ºèƒ½æ°´å¹³è¯„ä¼°")
    print("=" * 60)
    
    # è®¡ç®—ç»Ÿè®¡
    total_tests = len(ollama_results)
    successful_tests = sum(1 for r in ollama_results if r.get("success", False))
    success_rate = (successful_tests / total_tests) * 100 if total_tests > 0 else 0
    
    # å¹³å‡è´¨é‡åˆ†æ•°
    avg_quality = sum(r.get("quality_score", 0) for r in ollama_results) / total_tests if total_tests > 0 else 0
    
    # å¹³å‡å“åº”æ—¶é—´
    avg_response_time = sum(r.get("processing_time_ms", 0) for r in ollama_results) / total_tests if total_tests > 0 else 0
    
    # å¹³å‡å“åº”é•¿åº¦
    avg_response_length = sum(r.get("response_length", 0) for r in ollama_results) / total_tests if total_tests > 0 else 0
    
    # æ¨¡å‹æ€§èƒ½å¯¹æ¯”
    model_performance = {}
    for model, result in model_results.items():
        if result.get("success"):
            model_performance[model] = {
                "response_length": result["response_length"],
                "processing_time": result["processing_time_ms"]
            }
    
    # è¯„ä¼°æ™ºèƒ½ç­‰çº§
    if success_rate >= 100 and avg_quality >= 0.8 and avg_response_length > 100:
        intelligence_level = "é«˜ç­‰AI (æ¥è¿‘AGI)"
        intelligence_desc = "ç³»ç»Ÿå…·å¤‡çœŸå®çš„æ™ºèƒ½æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›"
    elif success_rate >= 75 and avg_quality >= 0.6 and avg_response_length > 50:
        intelligence_level = "ä¸­ç­‰AI (åŠŸèƒ½å®Œå–„)"
        intelligence_desc = "ç³»ç»Ÿå…·å¤‡è‰¯å¥½çš„æ™ºèƒ½å¯¹è¯å’ŒçŸ¥è¯†è¡¨è¾¾èƒ½åŠ›"
    elif success_rate >= 50 and avg_quality >= 0.4 and avg_response_length > 30:
        intelligence_level = "åˆçº§AI (åŸºç¡€æ™ºèƒ½)"
        intelligence_desc = "ç³»ç»Ÿå…·å¤‡åŸºæœ¬çš„æ™ºèƒ½å“åº”èƒ½åŠ›"
    else:
        intelligence_level = "AIæ¡†æ¶ (åŠŸèƒ½å—é™)"
        intelligence_desc = "ç³»ç»Ÿéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›æ‰èƒ½è¾¾åˆ°å®ç”¨æ°´å¹³"
    
    # æ‰“å°è¯„ä¼°ç»“æœ
    print(f"ğŸ¯ æ™ºèƒ½ç­‰çº§: {intelligence_level}")
    print(f"ğŸ“ æ™ºèƒ½æè¿°: {intelligence_desc}")
    print(f"âœ… æˆåŠŸç‡: {success_rate:.1f}%")
    print(f"ğŸ¯ å¹³å‡è´¨é‡: {avg_quality:.3f}")
    print(f"â±ï¸ å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.1f}ms")
    print(f"ğŸ“ å¹³å‡å“åº”é•¿åº¦: {avg_response_length:.1f}å­—ç¬¦")
    
    # æ¨¡å‹æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
    for model, perf in model_performance.items():
        print(f"  {model}:")
        print(f"    å“åº”é•¿åº¦: {perf['response_length']} å­—ç¬¦")
        print(f"    å“åº”æ—¶é—´: {perf['processing_time_ms']:.1f}ms")
    
    return {
        "intelligence_level": intelligence_level,
        "intelligence_description": intelligence_desc,
        "statistics": {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "success_rate": success_rate,
            "avg_quality_score": avg_quality,
            "avg_response_time_ms": avg_response_time,
            "avg_response_length": avg_response_length
        },
        "model_performance": model_performance,
        "ollama_results": ollama_results,
        "model_results": model_results
    }

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§  æœ€ç»ˆæ™ºèƒ½æµ‹è¯• - éªŒè¯çœŸæ­£çš„AIç³»ç»Ÿ")
    print("=" * 60)
    
    # æµ‹è¯•Ollamaé›†æˆ
    ollama_results = test_ollama_integration()
    
    # æµ‹è¯•å¤šæ¨¡å‹
    model_results = test_different_models()
    
    # è¯„ä¼°æ™ºèƒ½æ°´å¹³
    assessment = assess_intelligence_level(ollama_results, model_results)
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    final_report = {
        "test_time": datetime.now().isoformat(),
        "test_type": "FINAL_INTELLIGENCE_VERIFICATION",
        "system_status": "TRUE_INTELLIGENCE",
        "assessment": assessment,
        "recommendations": []
    }
    
    # ç”Ÿæˆå»ºè®®
    intelligence_level = assessment["intelligence_level"]
    if "é«˜ç­‰" in intelligence_level:
        final_report["recommendations"] = [
            "ç³»ç»Ÿå·²è¾¾åˆ°AGIçº§åˆ«ï¼Œå¯ä»¥å¼€å§‹Phase 3",
            "è€ƒè™‘æ·»åŠ æ›´å¤šè®¤çŸ¥èƒ½åŠ›å¢å¼º",
            "ä¼˜åŒ–æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒ"
        ]
    elif "ä¸­ç­‰" in intelligence_level:
        final_report["recommendations"] = [
            "ç³»ç»ŸåŠŸèƒ½å®Œå–„ï¼Œå»ºè®®å¢å¼ºè®¤çŸ¥èƒ½åŠ›",
            "ä¼˜åŒ–å“åº”è´¨é‡å’Œå¤šæ ·æ€§",
            "è€ƒè™‘é›†æˆæ›´å¤šLLMæ¨¡å‹"
        ]
    elif "åˆçº§" in intelligence_level:
        final_report["recommendations"] = [
            "éœ€è¦æ”¹è¿›æ™ºèƒ½å“åº”è´¨é‡",
            "å¢åŠ çŸ¥è¯†å‚¨å¤‡å’Œæ¨ç†èƒ½åŠ›",
            "ä¼˜åŒ–æ¨¡å‹é€‰æ‹©å’Œå‚æ•°è°ƒä¼˜"
        ]
    else:
        final_report["recommendations"] = [
            "éœ€è¦é‡å¤§æ”¹è¿›æ‰èƒ½è¾¾åˆ°å®ç”¨æ°´å¹³",
            "è€ƒè™‘é‡æ–°è®¾è®¡æ™ºèƒ½æ¶æ„",
            "æ·»åŠ çœŸæ­£çš„LLMé›†æˆ"
        ]
    
    # ä¿å­˜æŠ¥å‘Š
    with open("FINAL_INTELLIGENCE_REPORT.json", "w", encoding="utf-8") as f:
        json.dump(final_report, f, indent=2, ensure_ascii=False)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æœ€ç»ˆæµ‹è¯•å®Œæˆ")
    print("=" * 60)
    print(f"ğŸ“Š ç³»ç»ŸçŠ¶æ€: {final_report['system_status']}")
    print(f"ğŸ§  æ™ºèƒ½ç­‰çº§: {intelligence_level}")
    print(f"ğŸ“ è¯„ä¼°æè¿°: {assessment['intelligence_description']}")
    print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: FINAL_INTELLIGENCE_REPORT.json")
    
    # å»ºè®®è¡ŒåŠ¨
    print(f"\nğŸ¯ å»ºè®®è¡ŒåŠ¨:")
    for i, rec in enumerate(final_report["recommendations"], 1):
        print(f"  {i}. {rec}")
    
    # åˆ¤æ–­æ˜¯å¦å¯ä»¥ç»§ç»­å¼€å‘
    success_rate = assessment["statistics"]["success_rate"]
    avg_quality = assessment["statistics"]["avg_quality_score"]
    
    if success_rate >= 75 and avg_quality >= 0.6:
        print(f"\nâœ… ç³»ç»Ÿå…·å¤‡çœŸæ­£çš„æ™ºèƒ½èƒ½åŠ›ï¼Œå¯ä»¥ç»§ç»­Phase 3å¼€å‘")
        print(f"ğŸš€ å‡†å¤‡å®ç°SRRMè¿›åŒ–å¼•æ“")
        return True
    else:
        print(f"\nâš ï¸ ç³»ç»Ÿéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›æ‰èƒ½è¿›å…¥ä¸‹ä¸€é˜¶æ®µ")
        return False

if __name__ == "__main__":
    success = main()
    
    if success:
        print(f"\nğŸŠ æ­å–œï¼ä½ çš„AIç³»ç»Ÿå·²ç»å…·å¤‡äº†çœŸæ­£çš„æ™ºèƒ½ï¼")
    else:
        print(f"\nğŸ”§ éœ€è¦ç»§ç»­æ”¹è¿›æ‰èƒ½å®ç°çœŸæ­£çš„AIæ™ºèƒ½")