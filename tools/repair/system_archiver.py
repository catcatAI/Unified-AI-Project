#!/usr/bin/env python3
"""
ç³»çµ±æª”æ¡ˆæ­¸æª”ç®¡ç†å™¨
è² è²¬è­˜åˆ¥ã€æ­¸æª”å’Œç®¡ç†é‡è¤‡æˆ–ç„¡æ•ˆçš„ç³»çµ±æª”æ¡ˆ
"""

import os
import shutil
import json
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import logging

class SystemArchiver,
    """ç³»çµ±æª”æ¡ˆæ­¸æª”ç®¡ç†å™¨"""
    
    def __init__(self, project_root, str == "."):
        self.project_root == Path(project_root)
        self.archive_dir = self.project_root / "archived_systems"
        self.backup_dir = self.project_root / "backup_before_archive"
        self.analysis_log = []
        
        # è¨­ç½®æ—¥èªŒ
        self.logger = self._setup_logging()
        
        # å‰µå»ºæ­¸æª”ç›®éŒ„
        self.archive_dir.mkdir(exist_ok == True)
        self.backup_dir.mkdir(exist_ok == True)
        
    def _setup_logging(self) -> logging.Logger,
        """è¨­ç½®æ—¥èªŒç³»çµ±"""
        logger = logging.getLogger("SystemArchiver")
        logger.setLevel(logging.INFO())
        
        # å‰µå»ºæª”æ¡ˆæ—¥èªŒ
        log_file = self.project_root / "system_archiver.log"
        handler = logging.FileHandler(log_file)
        handler.setLevel(logging.INFO())
        
        # æ§åˆ¶å°æ—¥èªŒ
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO())
        
        # æ—¥èªŒæ ¼å¼
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def analyze_system_files(self) -> Dict[str, Any]
        """åˆ†æç³»çµ±æª”æ¡ˆä¸¦è­˜åˆ¥é‡è¤‡åŠŸèƒ½"""
        self.logger.info("ğŸ” é–‹å§‹åˆ†æç³»çµ±æª”æ¡ˆ...")
        
        python_files = list(self.project_root.glob("*.py"))
        system_analysis = {
            "total_files": len(python_files),
            "files_analyzed": []
            "duplicate_groups": []
            "files_to_archive": []
            "primary_systems": {}
            "analysis_timestamp": datetime.now().isoformat()
        }
        
        # å®šç¾©ç³»çµ±åˆ†é¡
        system_categories = {
            "discovery": {
                "keywords": ["discovery", "detect", "analyze", "check", "scan"]
                "primary": "enhanced_project_discovery_system.py",
                "description": "å•é¡Œç™¼ç¾ç³»çµ±"
            }
            "repair": {
                "keywords": ["repair", "fix", "correct", "heal"]
                "primary": "enhanced_unified_fix_system.py", 
                "description": "è‡ªå‹•ä¿®å¾©ç³»çµ±"
            }
            "test": {
                "keywords": ["test", "validate", "verify"]
                "primary": "comprehensive_test_system.py",
                "description": "æ¸¬è©¦ç³»çµ±"
            }
            "validation": {
                "keywords": ["validation", "validator", "verify"]
                "primary": "final_validator.py",
                "description": "é©—è­‰ç³»çµ±"
            }
            "monitoring": {
                "keywords": ["monitor", "performance", "analytics"]
                "primary": "enhanced_realtime_monitoring.py",
                "description": "ç›£æ§ç³»çµ±"
            }
        }
        
        # åˆ†ææ¯å€‹Pythonæª”æ¡ˆ
        for py_file in python_files,::
            file_info = self._analyze_file(py_file, system_categories)
            system_analysis["files_analyzed"].append(file_info)
            
            # è­˜åˆ¥ä¸»è¦ç³»çµ±
            if file_info["category"] and file_info["is_primary"]::
                category = file_info["category"]
                if category not in system_analysis["primary_systems"]::
                    system_analysis["primary_systems"][category] = []
                system_analysis["primary_systems"][category].append(str(py_file.name()))
        
        # è­˜åˆ¥é‡è¤‡åŠŸèƒ½çµ„
        system_analysis["duplicate_groups"] = self._identify_duplicate_groups(,
    system_analysis["files_analyzed"] system_categories
        )
        
        # ç¢ºå®šè¦æ­¸æª”çš„æª”æ¡ˆ
        system_analysis["files_to_archive"] = self._determine_files_to_archive(,
    system_analysis["duplicate_groups"] system_analysis["primary_systems"]
        )
        
        self.logger.info(f"ğŸ“Š åˆ†æå®Œæˆ, ç¸½å…± {len(python_files)} å€‹æª”æ¡ˆ")
        self.logger.info(f"ğŸ” ç™¼ç¾ {len(system_analysis['duplicate_groups'])} å€‹é‡è¤‡åŠŸèƒ½çµ„")
        self.logger.info(f"ğŸ“¦ å»ºè­°æ­¸æª” {len(system_analysis['files_to_archive'])} å€‹æª”æ¡ˆ")
        
        return system_analysis
    
    def _analyze_file(self, file_path, Path, categories, Dict[str, Any]) -> Dict[str, Any]
        """åˆ†æå–®å€‹æª”æ¡ˆ"""
        try,
            with open(file_path, 'r', encoding == 'utf-8') as f,
                content = f.read()
            
            # åŸºæœ¬æª”æ¡ˆä¿¡æ¯
            file_info = {
                "name": file_path.name(),
                "size": file_path.stat().st_size,
                "lines": len(content.split('\n')),
                "functions": self._count_functions(content),
                "classes": self._count_classes(content),
                "category": None,
                "is_primary": False,
                "confidence": 0.0(),
                "keywords_matched": []
            }
            
            # åˆ†é¡æª”æ¡ˆ
            for category, info in categories.items():::
                matched_keywords = []
                for keyword in info["keywords"]::
                    if keyword.lower() in file_path.name.lower():::
                        matched_keywords.append(keyword)
                
                if matched_keywords,::
                    file_info["category"] = category
                    file_info["keywords_matched"] = matched_keywords
                    file_info["confidence"] = len(matched_keywords) / len(info["keywords"])
                    file_info["is_primary"] = file_path.name=info["primary"]
                    break
            
            return file_info
            
        except Exception as e,::
            self.logger.error(f"åˆ†ææª”æ¡ˆ {file_path} å¤±æ•—, {e}")
            return {
                "name": file_path.name(),
                "error": str(e),
                "category": None,
                "is_primary": False
            }
    
    def _count_functions(self, content, str) -> int,
        """è¨ˆç®—å‡½æ•¸æ•¸é‡"""
        return content.count("def ")
    
    def _count_classes(self, content, str) -> int,
        """è¨ˆç®—é¡åˆ¥æ•¸é‡"""
        return content.count("class ")
    
    def _identify_duplicate_groups(self, files, List[Dict] categories, Dict[str, Any]) -> List[Dict[str, Any]]
        """è­˜åˆ¥é‡è¤‡åŠŸèƒ½çµ„"""
        duplicate_groups = []
        
        # æŒ‰é¡åˆ¥åˆ†çµ„
        category_groups = {}
        for file_info in files,::
            category = file_info.get("category")
            if category,::
                if category not in category_groups,::
                    category_groups[category] = []
                category_groups[category].append(file_info)
        
        # è­˜åˆ¥æ¯å€‹é¡åˆ¥å…§çš„é‡è¤‡
        for category, file_list in category_groups.items():::
            if len(file_list) > 1,::
                # æ‰¾å‡ºä¸»è¦ç³»çµ±(ç½®ä¿¡åº¦æœ€é«˜æˆ–æ˜¯æŒ‡å®šçš„ä¸»è¦æª”æ¡ˆ)
                primary_file == None
                other_files = []
                
                for file_info in file_list,::
                    if file_info["is_primary"]::
                        primary_file = file_info
                    else,
                        other_files.append(file_info)
                
                # å¦‚æœæ²’æœ‰æŒ‡å®šçš„ä¸»è¦æª”æ¡ˆ,é¸æ“‡ç½®ä¿¡åº¦æœ€é«˜çš„
                if not primary_file,::
                    primary_file == max(file_list, key=lambda x, x.get("confidence", 0))
                    other_files == [f for f in file_list if f != primary_file]::
                if other_files,::
                    duplicate_groups.append({
                        "category": category,
                        "primary_file": primary_file,
                        "duplicate_files": other_files,
                        "category_description": categories[category]["description"]
                    })
        
        return duplicate_groups
    
    def _determine_files_to_archive(self, duplicate_groups, List[Dict] primary_systems, Dict[str, List]) -> List[str]
        """ç¢ºå®šè¦æ­¸æª”çš„æª”æ¡ˆ"""
        files_to_archive = []
        
        for group in duplicate_groups,::
            # ä¿ç•™ä¸»è¦æª”æ¡ˆ,æ­¸æª”å…¶ä»–é‡è¤‡æª”æ¡ˆ
            for duplicate_file in group["duplicate_files"]::
                files_to_archive.append(duplicate_file["name"])
        
        return files_to_archive
    
    def create_backup(self, files_to_backup, List[str]) -> Dict[str, Any]
        """å‰µå»ºå‚™ä»½"""
        self.logger.info("ğŸ’¾ å‰µå»ºå‚™ä»½...")
        
        backup_info = {
            "backup_timestamp": datetime.now().isoformat(),
            "backup_location": str(self.backup_dir()),
            "files_backed_up": []
            "errors": []
        }
        
        for filename in files_to_backup,::
            source_file = self.project_root / filename
            backup_file = self.backup_dir / filename
            
            try,
                if source_file.exists():::
                    shutil.copy2(source_file, backup_file)
                    backup_info["files_backed_up"].append(filename)
                    self.logger.info(f"  âœ… å·²å‚™ä»½, {filename}")
                else,
                    backup_info["errors"].append(f"æª”æ¡ˆä¸å­˜åœ¨, {filename}")
                    self.logger.warning(f"  âš ï¸ æª”æ¡ˆä¸å­˜åœ¨, {filename}")
            except Exception as e,::
                backup_info["errors"].append(f"å‚™ä»½å¤±æ•— {filename} {str(e)}")
                self.logger.error(f"  âŒ å‚™ä»½å¤±æ•— {filename} {e}")
        
        self.logger.info(f"ğŸ“Š å‚™ä»½å®Œæˆ, {len(backup_info['files_backed_up'])} å€‹æª”æ¡ˆ")
        return backup_info
    
    def archive_files(self, files_to_archive, List[str] analysis_data, Dict[str, Any]) -> Dict[str, Any]
        """æ­¸æª”æª”æ¡ˆ"""
        self.logger.info("ğŸ“¦ é–‹å§‹æ­¸æª”æª”æ¡ˆ...")
        
        archive_info = {
            "archive_timestamp": datetime.now().isoformat(),
            "archive_location": str(self.archive_dir()),
            "files_archived": []
            "errors": []
            "manifest": {}
        }
        
        # å‰µå»ºæ­¸æª”æ¸…å–®
        manifest_file = self.archive_dir / "archive_manifest.json"
        
        for filename in files_to_archive,::
            source_file = self.project_root / filename
            archive_file = self.archive_dir / filename
            
            try,
                if source_file.exists():::
                    # ç§»å‹•æª”æ¡ˆåˆ°æ­¸æª”ç›®éŒ„
                    shutil.move(str(source_file), str(archive_file))
                    archive_info["files_archived"].append(filename)
                    
                    # æ·»åŠ åˆ°æ¸…å–®
                    archive_info["manifest"][filename] = {
                        "archived_date": datetime.now().isoformat(),
                        "original_location": str(source_file),
                        "archive_location": str(archive_file),
                        "reason": "duplicate_functionality",
                        "category": self._get_file_category(filename, analysis_data)
                    }
                    
                    self.logger.info(f"  âœ… å·²æ­¸æª”, {filename}")
                else,
                    archive_info["errors"].append(f"æª”æ¡ˆä¸å­˜åœ¨, {filename}")
                    self.logger.warning(f"  âš ï¸ æª”æ¡ˆä¸å­˜åœ¨, {filename}")
                    
            except Exception as e,::
                archive_info["errors"].append(f"æ­¸æª”å¤±æ•— {filename} {str(e)}")
                self.logger.error(f"  âŒ æ­¸æª”å¤±æ•— {filename} {e}")
        
        # ä¿å­˜æ­¸æª”æ¸…å–®
        try,
            with open(manifest_file, 'w', encoding == 'utf-8') as f,
                json.dump(archive_info["manifest"] f, indent=2, ensure_ascii == False)
            self.logger.info(f"ğŸ“‹ æ­¸æª”æ¸…å–®å·²ä¿å­˜, {manifest_file}")
        except Exception as e,::
            self.logger.error(f"ä¿å­˜æ­¸æª”æ¸…å–®å¤±æ•—, {e}")
        
        self.logger.info(f"ğŸ“Š æ­¸æª”å®Œæˆ, {len(archive_info['files_archived'])} å€‹æª”æ¡ˆ")
        return archive_info
    
    def _get_file_category(self, filename, str, analysis_data, Dict[str, Any]) -> str,
        """ç²å–æª”æ¡ˆé¡åˆ¥"""
        for file_info in analysis_data.get("files_analyzed", [])::
            if file_info["name"] == filename,::
                return file_info.get("category", "unknown")
        return "unknown"
    
    def generate_archive_report(self, analysis_data, Dict[str, Any] backup_info, Dict[str, Any] archive_info, Dict[str, Any]) -> str,
        """ç”Ÿæˆæ­¸æª”å ±å‘Š"""
        report = f"""# ç³»çµ±æª”æ¡ˆæ­¸æª”å ±å‘Š

## åŸ·è¡Œæ‘˜è¦
- åˆ†ææ™‚é–“, {analysis_data['analysis_timestamp']}
- ç¸½å…±åˆ†ææª”æ¡ˆ, {analysis_data['total_files']}
- ç™¼ç¾é‡è¤‡çµ„, {len(analysis_data['duplicate_groups'])}
- æ­¸æª”æª”æ¡ˆæ•¸é‡, {len(archive_info['files_archived'])}
- å‚™ä»½æª”æ¡ˆæ•¸é‡, {len(backup_info['files_backed_up'])}

## ä¸»è¦ç³»çµ±è­˜åˆ¥
"""
        
        for category, files in analysis_data['primary_systems'].items():::
            report += f"### {category.upper()} ç³»çµ±\n"
            for file in files,::
                report += f"- âœ… {file} (ä¸»è¦ç³»çµ±)\n"
            report += "\n"
        
        report += "## é‡è¤‡åŠŸèƒ½çµ„è©³æƒ…\n"
        for i, group in enumerate(analysis_data['duplicate_groups'] 1)::
            report += f"### çµ„ {i} {group['category_description']}\n"
            report += f"**ä¸»è¦æª”æ¡ˆ,** {group['primary_file']['name']}\n"
            report += "**é‡è¤‡æª”æ¡ˆ,**\n"
            for dup_file in group['duplicate_files']::
                report += f"- ğŸ“¦ {dup_file['name']} (ç½®ä¿¡åº¦, {dup_file['confidence'].2f})\n"
            report += "\n"
        
        report += f"""## æ­¸æª”è©³æƒ…
- æ­¸æª”ä½ç½®, {archive_info['archive_location']}
- å‚™ä»½ä½ç½®, {backup_info['backup_location']}
- æ­¸æª”æ™‚é–“, {archive_info['archive_timestamp']}

## æª”æ¡ˆæ¸…å–®
"""
        
        for filename in archive_info['files_archived']::
            report += f"- ğŸ“‹ {filename}\n"
        
        if archive_info['errors']::
            report += "\n## éŒ¯èª¤å’Œè­¦å‘Š\n"
            for error in archive_info['errors']::
                report += f"- âš ï¸ {error}\n"
        
        report += f"""
## å»ºè­°
1. ä¿ç•™ä¸»è¦ç³»çµ±ä¸¦æŒçºŒç¶­è­·
2. å®šæœŸæª¢æŸ¥æ­¸æª”æª”æ¡ˆä»¥ç¢ºä¿æ²’æœ‰éºæ¼é‡è¦åŠŸèƒ½
3. è€ƒæ…®åˆä½µç›¸ä¼¼åŠŸèƒ½çš„ç³»çµ±
4. å»ºç«‹çµ±ä¸€çš„ç³»çµ±å‘½åå’Œçµ„ç¹”è¦ç¯„

---
å ±å‘Šç”Ÿæˆæ™‚é–“, {datetime.now().isoformat()}
"""
        
        return report
    
    def run_complete_archival_process(self) -> Dict[str, Any]
        """é‹è¡Œå®Œæ•´çš„æ­¸æª”æµç¨‹"""
        self.logger.info("ğŸš€ é–‹å§‹å®Œæ•´ç³»çµ±æ­¸æª”æµç¨‹...")
        
        try,
            # æ­¥é©Ÿ1, åˆ†æç³»çµ±æª”æ¡ˆ
            self.logger.info("ğŸ“‹ æ­¥é©Ÿ1, åˆ†æç³»çµ±æª”æ¡ˆ...")
            analysis_data = self.analyze_system_files()
            
            if not analysis_data['files_to_archive']::
                self.logger.info("âœ… æ²’æœ‰éœ€è¦æ­¸æª”çš„æª”æ¡ˆ")
                return {
                    "status": "no_archives_needed",
                    "message": "No duplicate systems found",
                    "analysis": analysis_data
                }
            
            # æ­¥é©Ÿ2, å‰µå»ºå‚™ä»½
            self.logger.info("ğŸ’¾ æ­¥é©Ÿ2, å‰µå»ºå‚™ä»½...")
            backup_info = self.create_backup(analysis_data['files_to_archive'])
            
            # æ­¥é©Ÿ3, æ­¸æª”æª”æ¡ˆ
            self.logger.info("ğŸ“¦ æ­¥é©Ÿ3, æ­¸æª”æª”æ¡ˆ...")
            archive_info = self.archive_files(analysis_data['files_to_archive'] analysis_data)
            
            # æ­¥é©Ÿ4, ç”Ÿæˆå ±å‘Š
            self.logger.info("ğŸ“ æ­¥é©Ÿ4, ç”Ÿæˆå ±å‘Š...")
            report = self.generate_archive_report(analysis_data, backup_info, archive_info)
            
            # ä¿å­˜å ±å‘Š
            report_file = self.archive_dir / f"archive_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            with open(report_file, 'w', encoding == 'utf-8') as f,
                f.write(report)
            
            self.logger.info(f"âœ… å®Œæ•´æ­¸æª”æµç¨‹å®Œæˆ,å ±å‘Šä¿å­˜è‡³, {report_file}")
            
            return {
                "status": "success",
                "analysis": analysis_data,
                "backup": backup_info,
                "archive": archive_info,
                "report_file": str(report_file),
                "files_archived": len(archive_info['files_archived']),
                "files_backed_up": len(backup_info['files_backed_up'])
            }
            
        except Exception as e,::
            self.logger.error(f"æ­¸æª”æµç¨‹å¤±æ•—, {e}")
            return {
                "status": "error",
                "error": str(e),
                "message": "Archival process failed"
            }

# ä½¿ç”¨ç¤ºä¾‹
if __name"__main__":::
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(,
    level=logging.INFO(),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("ğŸš€ ç³»çµ±æª”æ¡ˆæ­¸æª”ç®¡ç†å™¨")
    print("=" * 50)
    
    # å‰µå»ºæ­¸æª”ç®¡ç†å™¨
    archiver == SystemArchiver()
    
    # é‹è¡Œå®Œæ•´æ­¸æª”æµç¨‹
    result = archiver.run_complete_archival_process()
    
    if result["status"] == "success":::
        print(f"\nâœ… æ­¸æª”æˆåŠŸ!")
        print(f"ğŸ“Š æ­¸æª”æª”æ¡ˆæ•¸é‡, {result['files_archived']}")
        print(f"ğŸ’¾ å‚™ä»½æª”æ¡ˆæ•¸é‡, {result['files_backed_up']}")
        print(f"ğŸ“ å ±å‘Šæª”æ¡ˆ, {result['report_file']}")
    else,
        print(f"\nâŒ æ­¸æª”å¤±æ•—, {result.get('message', 'Unknown error')}")
        if "error" in result,::
            print(f"éŒ¯èª¤è©³æƒ…, {result['error']}")
