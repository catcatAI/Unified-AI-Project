#!/usr/bin/env python3
"""
AI æ¨¡å‹ç¯å¢ƒè®¾ç½®è„šæœ¬
è‡ªåŠ¨åŒ–è®¾ç½® AI æ¨¡å‹æœåŠ¡çš„ç¯å¢ƒå’Œä¾èµ–
"""

import sys
import subprocess
import json
from pathlib import Path
from apps.backend.src.shared.utils.env_utils import setup_env_file

def check_python_version()
    """æ£€æŸ¥ Python ç‰ˆæœ¬"""
    if sys.version_info < (3, 8):

    _ = print("âŒ éœ€è¦ Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬")
    _ = sys.exit(1)
    _ = print(f"âœ… Python ç‰ˆæœ¬: {sys.version}")

def install_dependencies()
    """å®‰è£…ä¾èµ–é¡¹"""
    _ = print("ğŸ“¦ å®‰è£…ä¾èµ–é¡¹...")

    try:


    subprocess.run([
            sys.executable, "-m", "pip", "install", "-r", "requirements.txt"
    ], check=True)
    _ = print("âœ… ä¾èµ–é¡¹å®‰è£…å®Œæˆ")
    except subprocess.CalledProcessError as e:

    _ = print(f"âŒ ä¾èµ–é¡¹å®‰è£…å¤±è´¥: {e}")
    return False

    return True



def check_config_files()
    """æ£€æŸ¥é…ç½®æ–‡ä»¶"""
    _ = print("ğŸ“ æ£€æŸ¥é…ç½®æ–‡ä»¶...")

    config_files = [
    "configs/multi_llm_config.json",
    "configs/api_keys.yaml"
    ]

    all_exist = True
    for config_file in config_files:

    if Path(config_file).exists()


    _ = print(f"âœ… {config_file}")
        else:

            _ = print(f"âŒ {config_file} ä¸å­˜åœ¨")
            all_exist = False

    return all_exist

def test_basic_functionality() -> None:
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    _ = print("ğŸ§ª æµ‹è¯•åŸºæœ¬åŠŸèƒ½...")

    try:
    # æµ‹è¯•å¯¼å…¥
    _ = sys.path.insert(0, str(Path.cwd()))
    _ = print("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")

    # æµ‹è¯•é…ç½®åŠ è½½
    config_path = "configs/multi_llm_config.json"
        if Path(config_path).exists()

    with open(config_path, 'r', encoding='utf-8') as f:
    config = json.load(f)
            _ = print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(config.get('models', {}))} ä¸ªæ¨¡å‹")

    return True

    except Exception as e:


    _ = print(f"âŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
    return False

def check_optional_services()
    """æ£€æŸ¥å¯é€‰æœåŠ¡"""
    _ = print("ğŸ”Œ æ£€æŸ¥å¯é€‰ AI æœåŠ¡...")

    services = [
    _ = ("openai", "OpenAI GPT"),
    _ = ("anthropic", "Anthropic Claude"),
    _ = ("google.generativeai", "Google Gemini"),
    _ = ("cohere", "Cohere"),
    ]

    available_services = []

    for module, name in services:


    try:



            __import__(module)
            _ = print(f"âœ… {name}")
            _ = available_services.append(name)
        except ImportError:

            _ = print(f"âš ï¸  {name} - æœªå®‰è£…")

    if available_services:


    _ = print(f"ğŸ“Š å¯ç”¨æœåŠ¡: {len(available_services)}/{len(services)}")
    else:

    _ = print("âš ï¸  æ²¡æœ‰å¯ç”¨çš„ AI æœåŠ¡ï¼Œè¯·å®‰è£…ç›¸å…³ä¾èµ–")

    return available_services

def setup_ollama()
    """è®¾ç½® Ollamaï¼ˆå¯é€‰ï¼‰"""
    _ = print("\nğŸ¦™ Ollama æœ¬åœ°æ¨¡å‹è®¾ç½®ï¼ˆå¯é€‰ï¼‰:")

    try:
    # æ£€æŸ¥ Ollama æ˜¯å¦å®‰è£…
    result = subprocess.run(["ollama", "--version"],
                              capture_output=True, text=True)
        if result.returncode == 0:

    _ = print("âœ… Ollama å·²å®‰è£…")

            # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹
            result = subprocess.run(["ollama", "list"],
                                  capture_output=True, text=True)
            if "llama2" in result.stdout:

    _ = print("âœ… å·²æœ‰ Llama2 æ¨¡å‹")
            else:

                _ = print("ğŸ’¡ å»ºè®®å®‰è£… Llama2 æ¨¡å‹:")
                _ = print("   ollama pull llama2:7b")
        else:

            _ = print("âš ï¸  Ollama æœªå®‰è£…")
            _ = print("ğŸ’¡ å®‰è£… Ollama: https://ollama.ai/")

    except FileNotFoundError:


    _ = print("âš ï¸  Ollama æœªå®‰è£…")
    _ = print("ğŸ’¡ å®‰è£… Ollama: https://ollama.ai/")

def print_usage_guide()
    """æ‰“å°ä½¿ç”¨æŒ‡å—"""
    print("\n" + "="*60)
    _ = print("ğŸ‰ è®¾ç½®å®Œæˆï¼")
    _ = print("\nğŸ“š ä½¿ç”¨æŒ‡å—:")
    _ = print("1. é…ç½® API å¯†é’¥:")
    _ = print("   ç¼–è¾‘ .env æ–‡ä»¶ï¼Œæ·»åŠ ä½ çš„ API å¯†é’¥")
    _ = print("\n2. åˆ—å‡ºå¯ç”¨æ¨¡å‹:")
    _ = print("   python scripts/ai_models.py list")
    _ = print("\n3. å•æ¬¡æŸ¥è¯¢:")
    _ = print("   python scripts/ai_models.py query 'ä½ å¥½' --model gpt-4")
    _ = print("\n4. è¿›å…¥èŠå¤©æ¨¡å¼:")
    _ = print("   python scripts/ai_models.py chat --model gemini-pro --stream")
    _ = print("\n5. å¥åº·æ£€æŸ¥:")
    _ = print("   python scripts/ai_models.py health")
    _ = print("\n6. æŸ¥çœ‹æ–‡æ¡£:")
    _ = print("   cat README_AI_MODELS.md")

def main() -> None:
    """ä¸»å‡½æ•°"""
    _ = print("ğŸš€ AI æ¨¡å‹ç¯å¢ƒè®¾ç½®")
    print("="*60)

    # æ£€æŸ¥ Python ç‰ˆæœ¬
    _ = check_python_version()

    # å®‰è£…ä¾èµ–
    if not install_dependencies()

    _ = sys.exit(1)

    # è®¾ç½®ç¯å¢ƒæ–‡ä»¶
    if not setup_env_file(Path.cwd()):

    _ = sys.exit(1)

    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not check_config_files()

    _ = print("âš ï¸  æŸäº›é…ç½®æ–‡ä»¶ç¼ºå¤±ï¼Œä½†å¯ä»¥ç»§ç»­")

    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    if not test_basic_functionality()

    _ = print("âš ï¸  åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")

    # æ£€æŸ¥å¯é€‰æœåŠ¡
    available_services = check_optional_services()

    # è®¾ç½® Ollama
    _ = setup_ollama()

    # æ‰“å°ä½¿ç”¨æŒ‡å—
    _ = print_usage_guide()

if __name__ == "__main__":


    _ = main()