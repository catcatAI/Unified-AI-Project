#!/usr/bin/env python3
"""
æ–‡æ¡£é“¾æ¥éªŒè¯è„šæœ¬
æ£€æŸ¥æ‰€æœ‰ Markdown æ–‡ä»¶ä¸­çš„å†…éƒ¨é“¾æ¥æ˜¯å¦æœ‰æ•ˆ
"""

import os
import re
import argparse
import json
from pathlib import Path
from typing import List, Tuple, Dict

# é»˜è®¤å¿½ç•¥ç›®å½•ï¼ˆä¸å‚ä¸æ‰«æï¼‰
IGNORE_PARTS = {"node_modules", ".git", "backup", "archives", "__pycache__", "venv", ".venv", "dist", "build"}
VERBOSE = os.environ.get('DOC_LINK_CHECK_VERBOSE') == '1'

# å…¼å®¹æ€§é‡å®šä½æ˜ å°„ï¼šå½“æ—§æ–‡æ¡£è·¯å¾„å·²æ›´å/è¿ç§»æ—¶ï¼Œä½¿ç”¨æ­¤è¡¨å°†å…¶åˆ¤å®šä¸ºæœ‰æ•ˆ
# Key ä¸º Markdown ä¸­å‡ºç°çš„åŸå§‹ç›¸å¯¹é“¾æ¥ï¼ˆå»é™¤é”šç‚¹åï¼‰ï¼ŒValue ä¸ºæ–°çš„ç›¸å¯¹é“¾æ¥ï¼ˆå¯åŒ…å« ..ï¼‰
RELOCATED_LINKS: Dict[str, str] = {
    # 1) è®­ç»ƒé…ç½®æ–‡ä»¶è¯´æ˜ï¼ˆåŸï¼šTRAINING_CONFIG_REFERENCE.md â†’ æ–°ï¼šé¡¹ç›®æ ¹ README çš„è®­ç»ƒé…ç½®ç« èŠ‚ï¼‰
    "TRAINING_CONFIG_REFERENCE.md": "../README.md",
    # 2) å¤šæ¨¡å‹ LLM é…ç½®æŒ‡å—ï¼ˆåŸï¼š../configuration/llm-config.md â†’ æ–°ï¼šæ ¸å¿ƒæœåŠ¡ ConfigLoader è¯´æ˜ï¼‰
    "../configuration/llm-config.md": "../core-services/config-loader.md",
    # 3) å¤šæ¨¡å‹ LLM éƒ¨ç½²æŒ‡å—ï¼ˆåŸï¼š../deployment/llm-deployment.md â†’ æ–°ï¼šç»Ÿä¸€éƒ¨ç½²æŒ‡å—ï¼‰
    "../deployment/llm-deployment.md": "../../05-development/DEPLOYMENT_GUIDE.md",
    # 4) HAM è®¾è®¡è§„èŒƒï¼ˆåŸï¼š./HAM_design_spec.md â†’ æ–°ï¼š./ham-design.mdï¼‰
    "./HAM_design_spec.md": "./ham-design.md",
    "HAM_design_spec.md": "ham-design.md",
    # 5) MQTT æ›¿ä»£æ–¹æ¡ˆåˆ†æï¼ˆåŸï¼š../../../../07-research/experimental/mqtt-broker-analysis.md â†’ æ–°ï¼šæ¶ˆæ¯ä¼ è¾“æœºåˆ¶æ€»è§ˆï¼‰
    "../../../../07-research/experimental/mqtt-broker-analysis.md": "../message-transport.md",
    # 6) HSP è§„èŒƒï¼ˆåŸï¼š../technical_design/HSP_SPECIFICATION.md â†’ æ–°ï¼šHSP è§„èŒƒç›®å½• READMEï¼‰
    "../technical_design/HSP_SPECIFICATION.md": "../../03-technical-architecture/communication/hsp-specification/README.md",
    # 7) HAM è®¾è®¡ï¼ˆåŸï¼š../technical_design/architecture/HAM_design_spec.md â†’ æ–°ï¼šham-design.mdï¼‰
    "../technical_design/architecture/HAM_design_spec.md": "../../03-technical-architecture/memory-systems/ham-design.md",
    # 8) ä»£ç†åä½œï¼ˆåŸï¼š../technical_design/architecture/AGENT_COLLABORATION_FRAMEWORK.md â†’ æ–°ï¼šagent-collaboration.mdï¼‰
    "../technical_design/architecture/AGENT_COLLABORATION_FRAMEWORK.md": "../../04-advanced-concepts/agent-collaboration.md",
}


def _should_ignore(path: Path) -> bool:
    lower = str(path).lower()
    # å…¼å®¹ Windows å’Œ *nix åˆ†éš”ç¬¦
    return any((f"{os.sep}{part}{os.sep}" in lower) or (f"/{part}/" in lower) or (f"\\{part}\\" in lower) for part in IGNORE_PARTS)
    def find_markdown_files(root_dir: str) -> List[Path]:
    """æŸ¥æ‰¾æ‰€æœ‰ Markdown æ–‡ä»¶"""
    root_path = Path(root_dir)
    return [p for p in root_path.rglob("*.md") if not _should_ignore(p)]
    def extract_links(file_path: Path) -> List[Tuple[str, str]]:
    """ä» Markdown æ–‡ä»¶ä¸­æå–é“¾æ¥"""
    try:

    with open(file_path, 'r', encoding='utf-8') as f:
    content = f.read()

    # åŒ¹é… [text](link) æ ¼å¼çš„é“¾æ¥
    pattern = r'\[([^\]]+)\]\(([^)]+)\)'
    matches = re.findall(pattern, content)
    return matches
    except Exception as e:

    _ = print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {file_path} - {e}")
    return []


def _normalize_link_path(link: str) -> str:
    """å½’ä¸€åŒ–é“¾æ¥è·¯å¾„ï¼ˆå»é™¤ä¸¤ç«¯ç©ºæ ¼ã€å°–æ‹¬å·ã€é”šç‚¹ã€å‰ç½®çš„./ï¼‰"""
    link = (link or '').strip()
    # å»é™¤å°–æ‹¬å·åŒ…è£¹
    if link.startswith('<') and link.endswith('>')

    link = link[1:-1].strip()
    # å»é™¤ fragmentï¼ˆ#é”šç‚¹ï¼‰éƒ¨åˆ†ï¼Œä»…éªŒè¯è·¯å¾„å­˜åœ¨æ€§
    if '#' in link

    link = link.split('#', 1)[0].strip()
    # è§„èŒƒåŒ–ç›¸å¯¹è·¯å¾„
    if link.startswith('./')

    link = link[2:]
    return link


def validate_link(base_path: Path, link: str) -> bool:
    """éªŒè¯é“¾æ¥æ˜¯å¦æœ‰æ•ˆï¼ˆæ”¯æŒé‡å®šä½æ˜ å°„ï¼‰"""
    # è·³è¿‡å¤–éƒ¨é“¾æ¥ä¸çº¯é”šç‚¹
    if link.startswith(('http://', 'https://', 'mailto:')):

    return True
    if not link or link.strip().startswith('#')

    return True

    # å½’ä¸€åŒ–åå¾—åˆ°ç”¨äºåŒ¹é…/æ‹¼æ¥çš„è·¯å¾„
    link_path = _normalize_link_path(link)

    # é¦–å…ˆç›´æ¥éªŒè¯åŸè·¯å¾„
    target_path = (base_path.parent / link_path)
    try:

    target_resolved = target_path.resolve()
    except Exception:

    target_resolved = target_path  # è§£æå¤±è´¥æ—¶ï¼Œä¿æŒç›¸å¯¹è·¯å¾„
    if target_resolved.exists()

    return True

    # å°è¯•é‡å®šä½æ˜ å°„
    relocated = RELOCATED_LINKS.get(link_path)
    if relocated:

    relocated_path = (base_path.parent / relocated)
        try:

            relocated_resolved = relocated_path.resolve()
        except Exception:

            relocated_resolved = relocated_path
        if relocated_resolved.exists()

    if VERBOSE:
    _ = print(f"  ğŸ” è·¯å¾„é‡å®šä½: '{link_path}' â†’ '{relocated}' â†’ å­˜åœ¨ âœ…")
            return True
        else:

            if VERBOSE:


    _ = print(f"  ğŸ” è·¯å¾„é‡å®šä½å¤±è´¥: '{link_path}' â†’ '{relocated}' â†’ ä¸å­˜åœ¨ âŒ")

    # å‡æœªé€šè¿‡ â†’ æ— æ•ˆ
    return False


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="éªŒè¯ Markdown æ–‡æ¡£ä¸­çš„å†…éƒ¨é“¾æ¥")
    parser.add_argument('--root', default='.', help='æ‰«ææ ¹ç›®å½•ï¼ˆé»˜è®¤å½“å‰ç›®å½•ï¼‰')
    parser.add_argument('--ignore', default='', help='ä»¥é€—å·åˆ†éš”çš„å¿½ç•¥ç›®å½•åç§°ï¼ˆè¿½åŠ åˆ°é»˜è®¤å¿½ç•¥åˆ—è¡¨ï¼‰')
    parser.add_argument('-v', '--verbose', action='store_true', help='æ‰“å°æœ‰æ•ˆé“¾æ¥çš„è¯¦ç»†ä¿¡æ¯')
    # æ–°å¢ï¼šé”™è¯¯æŠ¥å‘Šè¾“å‡ºä½ç½®
    parser.add_argument('--report-json', default='docs_link_check_errors.json', help='è¾“å‡ºæ— æ•ˆé“¾æ¥ JSON æŠ¥å‘Šè·¯å¾„')
    parser.add_argument('--report-text', default='docs_link_check_errors.txt', help='è¾“å‡ºæ— æ•ˆé“¾æ¥çº¯æ–‡æœ¬æŠ¥å‘Šè·¯å¾„')
    return parser.parse_args()


def main() -> None:
    """ä¸»å‡½æ•°"""
    global IGNORE_PARTS, VERBOSE
    args = parse_args()
    VERBOSE = VERBOSE or args.verbose
    # åˆå¹¶å¿½ç•¥ç›®å½•
    extra_ignores = {part.strip() for part in args.ignore.split(',') if part.strip()}:
    IGNORE_PARTS = set(IGNORE_PARTS) | extra_ignores

    _ = print("ğŸ” å¼€å§‹éªŒè¯æ–‡æ¡£é“¾æ¥...")

    root_dir = args.root
    markdown_files = find_markdown_files(root_dir)

    broken_links: List[Tuple[Path, str, str]] = []
    total_links = 0

    for md_file in markdown_files:


    _ = print(f"\nğŸ“„ æ£€æŸ¥æ–‡ä»¶: {md_file}")
    links = extract_links(md_file)

        for text, link in links:


    total_links += 1
            if not validate_link(md_file, link)

    _ = broken_links.append((md_file, text, link))
                _ = print(f"  âŒ æ— æ•ˆé“¾æ¥: [{text}]({link})")
            else:

                if VERBOSE:


    _ = print(f"  âœ… æœ‰æ•ˆé“¾æ¥: [{text}]({link})")

    _ = print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
    _ = print(f"æ€»é“¾æ¥æ•°: {total_links}")
    _ = print(f"æ— æ•ˆé“¾æ¥æ•°: {len(broken_links)}")

    # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶ï¼Œä¾¿äºå¤–éƒ¨å·¥å…·è§£æ
    try:

    json_report = {
            _ = 'root': str(Path(root_dir).resolve()),
            'total_links': total_links,
            _ = 'broken_count': len(broken_links),
            'broken': [
                {
                    _ = 'file': str(p),
                    'text': t,
                    'link': l
                } for (p, t, l) in broken_links
            ]
    }
    Path(args.report_json).write_text(json.dumps(json_report, ensure_ascii=False, indent=2), encoding='utf-8')
    # æ–‡æœ¬æŠ¥å‘Šï¼ˆæ›´æ˜“æ–¼äººå·¥ç€è¦½ï¼‰
    lines = [
            f"Root: {json_report['root']}",
            f"Total: {total_links}",
            _ = f"Broken: {len(broken_links)}",
            "",
    ]
        for p, t, l in broken_links[:1000]:

            _ = lines.append(f"{p}: [{t}]({l})")
    Path(args.report_text).write_text("\n".join(lines), encoding='utf-8')
    _ = print(f"ğŸ“ æŠ¥å‘Šå·²ç”Ÿæˆ: {args.report_json}, {args.report_text}")
    except Exception as e:

    _ = print(f"âš ï¸ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")

    if broken_links:


    _ = print(f"\nâŒ å‘ç° {len(broken_links)} ä¸ªæ— æ•ˆé“¾æ¥")
    return 1
    else:

    _ = print("âœ… æ‰€æœ‰é“¾æ¥éƒ½æœ‰æ•ˆ!")
    return 0


if __name__ == "__main__":



    _ = exit(main())