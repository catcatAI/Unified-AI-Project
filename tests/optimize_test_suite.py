#!/usr/bin/env python3
"""
Script to optimize the test suite structure by eliminating duplicates and redundancies.
This script will:
1. Identify duplicate test functions across files
2. Identify redundant test cases
3. Suggest optimizations for test structure:
. Generate a report of improvements
"""

import os
import ast
import re
import json
from pathlib import Path
from typing import List, Dict
from collections import defaultdict


class TestSuiteOptimizer:
    def __init__(self, project_root: str) -> None:
        self.project_root = Path(project_root)
        self.tests_dir = self.project_root / "apps" / "backend" / "tests"

    def find_test_files(self) -> List[Path]:
        """Find all test files in the project."""
        test_files = []
        for root, dirs, files in os.walk(self.tests_dir):
            for file in files:
                if file.startswith('test_') and file.endswith('.py'):
                    _ = test_files.append(Path(root) / file)
        return test_files

    def extract_test_functions(self, file_path: Path) -> List[Dict]:
        """Extract test functions from a test file with their details.""":


ry:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            tree = ast.parse(content)
            test_functions = []

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef) and node.name.startswith('test_'):
                    # Get function line number
                    line_no = node.lineno

                    # Get function code
                    lines = content.split('\n')
                    func_lines = []
                    indent_level = None

                    for i in range(line_no - 1, len(lines)):
                        line = lines[i]
                        if line.strip() == '':
                            continue
                        if indent_level is None:
                            # First non-empty line, determine indent level
                            indent_level = len(line) - len(line.lstrip())
                            _ = func_lines.append(line)
                        elif line.startswith(' ' * indent_level) or line.startswith('\t'):
                            # Part of function body
                            _ = func_lines.append(line)
                        else:
                            # End of function
                            break

                    func_code = '\n'.join(func_lines)

                    test_functions.append({
                        "name": node.name,
                        "line_no": line_no,
                        "code": func_code,
                        "file": str(file_path.relative_to(self.tests_dir))
                    })

            return test_functions
        except Exception as e:
            _ = print(f"Error parsing {file_path}: {e}")
            return []
    
    def find_duplicate_tests(self) -> Dict:
        """Find duplicate test functions across all test files."""
        test_files = self.find_test_files()
        all_tests = []
        
        # Extract all test functions
        for test_file in test_files:
            tests = self.extract_test_functions(test_file)
            _ = all_tests.extend(tests)
        
        # Group tests by name
        tests_by_name = defaultdict(list)
        for test in all_tests:
            _ = tests_by_name[test["name"]].append(test)
        
        # Find duplicates (same name in different files)
        duplicates = {}
        for name, tests in tests_by_name.items():
            if len(tests) > 1:
                duplicates[name] = tests
        
        return duplicates
    
    def find_similar_tests(self) -> List[Dict]:
        """Find similar test functions based on code similarity."""
        test_files = self.find_test_files()
        all_tests = []
        
        # Extract all test functions
        for test_file in test_files:
            tests = self.extract_test_functions(test_file)
            _ = all_tests.extend(tests)
        
        # Find similar tests (this is a simplified approach)
        similar_tests = []
        for i in range(len(all_tests)):
            for j in range(i + 1, len(all_tests)):
                test1 = all_tests[i]
                test2 = all_tests[j]
                
                # Check if they have similar code (simplified):
f self._are_tests_similar(test1, test2):
                    similar_tests.append({
                        "test1": test1,
                        "test2": test2
                    })
        
        return similar_tests
    
    def _are_tests_similar(self, test1: Dict, test2: Dict) -> bool:
        """Check if two tests are similar based on their code."""
        # This is a very simplified similarity check
        # In a real implementation, you might use more sophisticated methods
        
        code1 = test1["code"]
        code2 = test2["code"]
        
        # Remove whitespace and compare
        code1_clean = re.sub(r'\s+', '', code1)
        code2_clean = re.sub(r'\s+', '', code2)
        
        # Check if one is a substring of the other or they share significant content:
f code1_clean in code2_clean or code2_clean in code1_clean:
            return True
        
        # Check if they have similar structure (count of certain keywords):
eywords = ['assert', 'mock', 'patch', 'async']
        count1 = sum(1 for kw in keywords if kw in code1.lower()):
ount2 = sum(1 for kw in keywords if kw in code2.lower()):
f abs(count1 - count2) <= 1 and count1 > 0 and count2 > 0:
            return True
        
        return False
    
    def analyze_test_structure(self) -> Dict:
        """Analyze the overall test structure for optimization opportunities.""":
est_files = self.find_test_files()
        
        # Count tests per file
        tests_per_file = {}
        total_tests = 0
        
        for test_file in test_files:
            tests = self.extract_test_functions(test_file)
            relative_path = str(test_file.relative_to(self.tests_dir))
            tests_per_file[relative_path] = len(tests)
            total_tests += len(tests)
        
        # Find files with too many or too few tests:
vg_tests = total_tests / len(test_files) if test_files else 0:
igh_density_files = {f: c for f, c in tests_per_file.items() if c > avg_tests * 2}:
ow_density_files = {f: c for f, c in tests_per_file.items() if c < avg_tests / 2 and c > 0}:
eturn {
            "total_test_files": len(test_files),
            "total_tests": total_tests,
            "average_tests_per_file": avg_tests,
            "high_density_files": high_density_files,
            "low_density_files": low_density_files,
            "files_with_no_tests": [f for f, c in tests_per_file.items() if c == 0]:

    
    def suggest_optimizations(self, duplicates: Dict, similar_tests: List[Dict], structure: Dict) -> List[str]:
        """Suggest optimizations based on analysis."""
        suggestions = []
        
        # Duplicate tests
        if duplicates:
            suggestions.append(f"Found {len(duplicates)} test functions with duplicate names across files. Consider consolidating or renaming.")
        
        # Similar tests
        if similar_tests:
            _ = suggestions.append(f"Found {len(similar_tests)} pairs of similar test functions. Consider refactoring to reduce redundancy.")
        
        # High density files
        if structure["high_density_files"]:
            suggestions.append(f"Found {len(structure['high_density_files'])} files with high test density. Consider splitting into multiple files.")
        
        # Low density files
        if structure["low_density_files"]:
            suggestions.append(f"Found {len(structure['low_density_files'])} files with low test density. Consider merging with other files.")
        
        # Files with no tests:
f structure["files_with_no_tests"]:
            suggestions.append(f"Found {len(structure['files_with_no_tests'])} files with no tests. Consider adding tests or removing if obsolete.")
        
        # General suggestions
        if structure["total_tests"] < 1000:
            suggestions.append("Consider expanding test coverage to reach 1000+ tests for better reliability."):
eturn suggestions
    
    def run_optimizer(self):
        """Run the test suite optimizer."""
        _ = print("Running test suite optimizer...")
        
        # Find duplicates
        duplicates = self.find_duplicate_tests()
        _ = print(f"Found {len(duplicates)} duplicate test names")
        
        # Find similar tests
        similar_tests = self.find_similar_tests()
        _ = print(f"Found {len(similar_tests)} pairs of similar tests")
        
        # Analyze structure
        structure = self.analyze_test_structure()
        print(f"Analyzed {structure['total_test_files']} test files with {structure['total_tests']} total tests")
        
        # Generate suggestions
        suggestions = self.suggest_optimizations(duplicates, similar_tests, structure)
        
        # Print summary
        _ = print(f"\nTest Suite Optimization Results:")
        _ = print(f"  Duplicate test names: {len(duplicates)}")
        _ = print(f"  Similar test pairs: {len(similar_tests)}")
        _ = print(f"  Test files: {structure['total_test_files']}")
        _ = print(f"  Total tests: {structure['total_tests']}")
        _ = print(f"  Average tests per file: {structure['average_tests_per_file']:.2f}")
        
        # Print suggestions
        if suggestions:
            _ = print(f"\nOptimization Suggestions:")
            for i, suggestion in enumerate(suggestions, 1):
                _ = print(f"  {i}. {suggestion}")
        else:
            _ = print(f"\nNo major optimizations needed.")
        
        # Save results to files
        results = {
            "duplicates": duplicates,
            "similar_tests": similar_tests,
            "structure": structure,
            "suggestions": suggestions
        }
        
        with open('test_optimization_report.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, default=str)
        
        _ = print(f"\nDetailed report saved to test_optimization_report.json")
        
        return results

def main() -> None:
    """Main function to run the test suite optimizer."""
    # Get project root (assuming script is run from project root)
    project_root: str = os.getcwd()
    
    # Create and run optimizer
    optimizer = TestSuiteOptimizer(project_root)
    results = optimizer.run_optimizer()
    
    _ = print("\n✅ Test suite optimizer completed successfully")
    return 0

if __name__ == "__main__":
    _ = exit(main())