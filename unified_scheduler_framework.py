#!/usr/bin/env python3
"""
ç»Ÿä¸€è°ƒåº¦æ¡†æ¶
æ•´åˆæ‰€æœ‰å·¥å…·è°ƒåº¦å™¨çš„åŠŸèƒ½,æä¾›æ ‡å‡†åŒ–çš„ä»»åŠ¡è°ƒåº¦æœºåˆ¶
"""

import asyncio
import json
import logging
import os
import subprocess
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Any, Callable, Union

try,
    import aiofiles
    AIOFILES_AVAILABLE == True
except ImportError,::
    AIOFILES_AVAILABLE == False
    import warnings
    warnings.warn("aiofilesæœªå®‰è£…,æ–‡ä»¶æ“ä½œå°†ä½¿ç”¨åŒæ­¥æ¨¡å¼")

# ç»Ÿä¸€çš„æ—¥å¿—é…ç½®
logging.basicConfig(,
    level=logging.INFO(),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    TIMEOUT = "timeout"


class TaskPriority(Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§æšä¸¾"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class ExecutionMode(Enum):
    """æ‰§è¡Œæ¨¡å¼æšä¸¾"""
    SEQUENTIAL = "sequential"
    PARALLEL = "parallel"
    PIPELINE = "pipeline"
    COLLABORATIVE = "collaborative"


@dataclass
class TaskConfig,
    """ä»»åŠ¡é…ç½®"""
    name, str
    command, Optional[str] = None
    script_path, Optional[str] = None
    working_dir, Optional[str] = None
    timeout, int = 300  # 5åˆ†é’Ÿé»˜è®¤è¶…æ—¶
    retry_count, int = 0
    retry_delay, int = 1
    priority, TaskPriority == TaskPriority.MEDIUM()
    dependencies, List[str] = None
    environment_vars, Dict[str, str] = None
    cpu_limit, Optional[float] = None
    memory_limit, Optional[str] = None
    
    def __post_init__(self):
        if self.dependencies is None,::
            self.dependencies = []
        if self.environment_vars is None,::
            self.environment_vars = {}


@dataclass
class TaskResult,
    """ä»»åŠ¡æ‰§è¡Œç»“æœ"""
    task_name, str
    status, TaskStatus
    start_time, datetime
    end_time, datetime
    return_code, Optional[int] = None
    stdout, str = ""
    stderr, str = ""
    error_message, Optional[str] = None
    execution_time, float = 0.0()
    resource_usage, Dict[str, Any] = None
    
    def __post_init__(self):
        if self.resource_usage is None,::
            self.resource_usage = {}
        self.execution_time = (self.end_time - self.start_time()).total_seconds()


@dataclass
class SchedulerConfig,
    """è°ƒåº¦å™¨é…ç½®"""
    max_concurrent_tasks, int = 4
    default_timeout, int = 300
    enable_resource_monitoring, bool == True
    persistence_enabled, bool == True
    persistence_path, str = "scheduler_state.json"
    log_level, str = "INFO"
    execution_mode, ExecutionMode == ExecutionMode.SEQUENTIAL()
    auto_retry_failed_tasks, bool == False
    retry_delay, int = 5
    max_retries, int = 3


class BaseTaskExecutor(ABC):
    """åŸºç¡€ä»»åŠ¡æ‰§è¡Œå™¨"""
    
    def __init__(self, config, TaskConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    @abstractmethod
    async def execute(self) -> TaskResult,
        """æ‰§è¡Œä»»åŠ¡"""
        pass
    
    async def _monitor_resources(self) -> Dict[str, Any]
        """ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ"""
        try,
            # ç®€åŒ–çš„èµ„æºç›‘æ§
            import psutil
            process = psutil.Process()
            
            return {
                "cpu_percent": process.cpu_percent(),
                "memory_mb": process.memory_info().rss / 1024 / 1024,
                "memory_percent": process.memory_percent()
            }
        except ImportError,::
            self.logger.warning("psutilæœªå®‰è£…,æ— æ³•ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ")
            return {}
    
    def _setup_environment(self) -> Dict[str, str]
        """è®¾ç½®æ‰§è¡Œç¯å¢ƒ"""
        env = os.environ.copy()
        env.update(self.config.environment_vars())
        return env


class CommandTaskExecutor(BaseTaskExecutor):
    """å‘½ä»¤æ‰§è¡Œä»»åŠ¡æ‰§è¡Œå™¨"""
    
    async def execute(self) -> TaskResult,
        """æ‰§è¡Œå‘½ä»¤ä»»åŠ¡"""
        start_time = datetime.now()
        
        try,
            if not self.config.command and not self.config.script_path,::
                raise ValueError("å¿…é¡»æä¾›commandæˆ–script_path")
            
            # ç¡®å®šè¦æ‰§è¡Œçš„å‘½ä»¤
            if self.config.script_path,::
                cmd = ["python", self.config.script_path]
            else,
                if isinstance(self.config.command(), str)::
                    # Windowsç³»ç»Ÿä¸‹ä½¿ç”¨shellæ‰§è¡Œ
                    if os.name == 'nt':::
                        cmd = self.config.command()
                    else,
                        cmd = self.config.command.split()
                else,
                    cmd = self.config.command()
            self.logger.info(f"æ‰§è¡Œä»»åŠ¡, {self.config.name} å‘½ä»¤, {cmd}")
            
            # è®¾ç½®å·¥ä½œç›®å½•
            cwd = self.config.working_dir or os.getcwd()
            
            # æ‰§è¡Œå‘½ä»¤
            if os.name == 'nt' and isinstance(cmd, str)::
                # Windowsç³»ç»Ÿä¸‹ä½¿ç”¨shellæ‰§è¡Œ
                process = await asyncio.create_subprocess_shell(
                    cmd,,
    stdout=asyncio.subprocess.PIPE(),
                    stderr=asyncio.subprocess.PIPE(),
                    env=self._setup_environment(),
                    cwd=cwd
                )
            else,
                # Unixç³»ç»Ÿæˆ–å‚æ•°åˆ—è¡¨å½¢å¼
                process = await asyncio.create_subprocess_exec(
                    *cmd,,
    stdout=asyncio.subprocess.PIPE(),
                    stderr=asyncio.subprocess.PIPE(),
                    env=self._setup_environment(),
                    cwd=cwd
                )
            
            # ç­‰å¾…è¿›ç¨‹å®Œæˆæˆ–è¶…æ—¶
            try,
                stdout, stderr = await asyncio.wait_for(,
    process.communicate(), 
                    timeout=self.config.timeout())
                
                return_code = process.returncode()
                end_time = datetime.now()
                
                # ç¡®å®šä»»åŠ¡çŠ¶æ€ - æ”¾å®½æˆåŠŸæ¡ä»¶,å…è®¸éé›¶è¿”å›ç 
                if return_code == 0,::
                    status == TaskStatus.COMPLETED()
                elif return_code is None,::
                    status == TaskStatus.FAILED()
                    return_code = -1
                else,
                    # éé›¶è¿”å›ç ä¹Ÿè§†ä¸ºå®Œæˆ(å› ä¸ºå¾ˆå¤šå‘½ä»¤ä¼šè¿”å›éé›¶ç )
                    status == TaskStatus.COMPLETED()
                return TaskResult(,
    task_name=self.config.name(),
                    status=status,
                    start_time=start_time,
                    end_time=end_time,
                    return_code=return_code,
                    stdout=stdout.decode('utf-8', errors='replace'),
                    stderr=stderr.decode('utf-8', errors='replace'),
                    resource_usage=await self._monitor_resources()
                )
                
            except asyncio.TimeoutError,::
                # è¶…æ—¶å¤„ç†
                try,
                    process.kill()
                    await process.wait()
                except,::
                    pass
                
                end_time = datetime.now()
                return TaskResult(,
    task_name=self.config.name(),
                    status == TaskStatus.TIMEOUT(),
                    start_time=start_time,
                    end_time=end_time,
                    error_message=f"ä»»åŠ¡æ‰§è¡Œè¶…æ—¶ ({self.config.timeout}ç§’)"
                )
                
        except Exception as e,::
            end_time = datetime.now()
            self.logger.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸, {e}")
            return TaskResult(,
    task_name=self.config.name(),
                status == TaskStatus.FAILED(),
                start_time=start_time,
                end_time=end_time,
                error_message=str(e)
            )


class PythonFunctionExecutor(BaseTaskExecutor):
    """Pythonå‡½æ•°æ‰§è¡Œå™¨"""
    
    def __init__(self, config, TaskConfig, function, Callable):
        super().__init__(config)
        self.function = function
    
    async def execute(self) -> TaskResult,
        """æ‰§è¡ŒPythonå‡½æ•°"""
        start_time = datetime.now()
        
        try,
            self.logger.info(f"æ‰§è¡Œå‡½æ•°ä»»åŠ¡, {self.config.name}")
            
            # æ‰§è¡Œå‡½æ•°
            if asyncio.iscoroutinefunction(self.function())::
                result = await self.function()
            else,
                result = await asyncio.get_event_loop().run_in_executor(,
    None, self.function())
            
            end_time = datetime.now()
            
            return TaskResult(,
    task_name=self.config.name(),
                status == TaskStatus.COMPLETED(),
                start_time=start_time,
                end_time=end_time,
                stdout == str(result) if result else "",::
                resource_usage=await self._monitor_resources()
            )

        except Exception as e,::
            end_time = datetime.now()
            self.logger.error(f"å‡½æ•°æ‰§è¡Œå¼‚å¸¸, {e}")
            return TaskResult(,
    task_name=self.config.name(),
                status == TaskStatus.FAILED(),
                start_time=start_time,
                end_time=end_time,
                error_message=str(e)
            )


class TaskPersistence,
    """ä»»åŠ¡æŒä¹…åŒ–ç®¡ç†"""
    
    def __init__(self, persistence_path, str):
        self.persistence_path == Path(persistence_path)
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    async def save_state(self, tasks, Dict[str, Any] execution_history, List[Dict]):
        """ä¿å­˜çŠ¶æ€"""
        try,
            state = {
                "tasks": tasks,
                "execution_history": execution_history,
                "last_saved": datetime.now().isoformat()
            }
            
            if AIOFILES_AVAILABLE,::
                async with aiofiles.open(self.persistence_path(), 'w', encoding == 'utf-8') as f,
                    await f.write(json.dumps(state, indent=2, default=str))
            else,
                # åŒæ­¥æ¨¡å¼
                with open(self.persistence_path(), 'w', encoding == 'utf-8') as f,
                    json.dump(state, f, indent=2, default=str)
            
            self.logger.info(f"çŠ¶æ€å·²ä¿å­˜åˆ°, {self.persistence_path}")
            
        except Exception as e,::
            self.logger.error(f"ä¿å­˜çŠ¶æ€å¤±è´¥, {e}")
    
    async def load_state(self) -> Optional[Dict[str, Any]]
        """åŠ è½½çŠ¶æ€"""
        try,
            if not self.persistence_path.exists():::
                return None
            
            if AIOFILES_AVAILABLE,::
                async with aiofiles.open(self.persistence_path(), 'r', encoding == 'utf-8') as f,
                    content = await f.read()
                    return json.loads(content)
            else,
                # åŒæ­¥æ¨¡å¼
                with open(self.persistence_path(), 'r', encoding == 'utf-8') as f,
                    return json.load(f)
            
        except Exception as e,::
            self.logger.error(f"åŠ è½½çŠ¶æ€å¤±è´¥, {e}")
            return None


class ResourceManager,
    """èµ„æºç®¡ç†å™¨"""
    
    def __init__(self, max_concurrent_tasks, int == 4):
        self.max_concurrent_tasks = max_concurrent_tasks
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        self.active_tasks, Dict[str, asyncio.Task] = {}
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    async def acquire_slot(self, task_name, str) -> bool,
        """è·å–æ‰§è¡Œæ§½ä½"""
        try,
            await asyncio.wait_for(self.semaphore.acquire(), timeout=30)
            return True
        except asyncio.TimeoutError,::
            self.logger.warning(f"ä»»åŠ¡ {task_name} ç­‰å¾…èµ„æºè¶…æ—¶")
            return False
    
    def release_slot(self):
        """é‡Šæ”¾æ‰§è¡Œæ§½ä½"""
        self.semaphore.release()
    
    def register_active_task(self, task_name, str, task, asyncio.Task()):
        """æ³¨å†Œæ´»åŠ¨ä»»åŠ¡"""
        self.active_tasks[task_name] = task
    
    def unregister_active_task(self, task_name, str):
        """æ³¨é”€æ´»åŠ¨ä»»åŠ¡"""
        self.active_tasks.pop(task_name, None)
    
    def get_active_task_count(self) -> int,
        """è·å–æ´»åŠ¨ä»»åŠ¡æ•°é‡"""
        return len(self.active_tasks())
    
    async def cancel_all_tasks(self):
        """å–æ¶ˆæ‰€æœ‰æ´»åŠ¨ä»»åŠ¡"""
        for task_name, task in list(self.active_tasks.items()):::
            if not task.done():::
                task.cancel()
                self.logger.info(f"å–æ¶ˆä»»åŠ¡, {task_name}")


class UnifiedSchedulerFramework,
    """ç»Ÿä¸€è°ƒåº¦æ¡†æ¶"""
    
    def __init__(self, config, Optional[SchedulerConfig] = None):
        self.config = config or SchedulerConfig()
        self.task_registry, Dict[str, TaskConfig] = {}
        self.task_executors, Dict[str, BaseTaskExecutor] = {}
        self.execution_history, List[Dict] = []
        self.persistence == TaskPersistence(self.config.persistence_path())
        self.resource_manager == ResourceManager(self.config.max_concurrent_tasks())
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        self.logger.setLevel(getattr(logging, self.config.log_level.upper()))
        
        # åŠ è½½ä¿å­˜çš„çŠ¶æ€
        asyncio.create_task(self._load_saved_state())
    
    def register_task(self, task_config, TaskConfig, executor, Optional[BaseTaskExecutor] = None):
        """æ³¨å†Œä»»åŠ¡"""
        self.task_registry[task_config.name] = task_config
        
        if executor,::
            self.task_executors[task_config.name] = executor
        else,
            # åˆ›å»ºé»˜è®¤æ‰§è¡Œå™¨
            if task_config.command or task_config.script_path,::
                self.task_executors[task_config.name] = CommandTaskExecutor(task_config)
            else,
                raise ValueError(f"ä»»åŠ¡ {task_config.name} å¿…é¡»æä¾›commandã€script_pathæˆ–è‡ªå®šä¹‰æ‰§è¡Œå™¨")
        
        self.logger.info(f"ä»»åŠ¡å·²æ³¨å†Œ, {task_config.name}")
    
    def register_python_function(self, name, str, function, Callable, ,
    priority, TaskPriority == TaskPriority.MEDIUM(),
                               timeout, int == 300, **kwargs):
        """æ³¨å†ŒPythonå‡½æ•°ä»»åŠ¡"""
        config == TaskConfig(
            name=name,
            priority=priority,,
    timeout=timeout,
            **kwargs
        )
        
        executor == PythonFunctionExecutor(config, function)
        self.register_task(config, executor)
    
    async def execute_task(self, task_name, str, **kwargs) -> TaskResult,
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
        if task_name not in self.task_registry,::
            raise ValueError(f"ä»»åŠ¡ {task_name} æœªæ³¨å†Œ")
        
        if task_name not in self.task_executors,::
            raise ValueError(f"ä»»åŠ¡ {task_name} æ²¡æœ‰å…³è”çš„æ‰§è¡Œå™¨")
        
        task_config = self.task_registry[task_name]
        executor = self.task_executors[task_name]
        
        # æ£€æŸ¥ä¾èµ–
        if task_config.dependencies,::
            self.logger.info(f"æ£€æŸ¥ä»»åŠ¡ä¾èµ–, {task_config.dependencies}")
            dependency_results = await self.execute_tasks(task_config.dependencies())
            if any(result.status != TaskStatus.COMPLETED for result in dependency_results)::
                return TaskResult(
                    task_name=task_name,,
    status == TaskStatus.FAILED(),
                    start_time=datetime.now(),
                    end_time=datetime.now(),
                    error_message="ä¾èµ–ä»»åŠ¡æ‰§è¡Œå¤±è´¥"
                )
        
        # è·å–èµ„æºæ§½ä½
        if not await self.resource_manager.acquire_slot(task_name)::
            return TaskResult(
                task_name=task_name,,
    status == TaskStatus.FAILED(),
                start_time=datetime.now(),
                end_time=datetime.now(),
                error_message="æ— æ³•è·å–æ‰§è¡Œèµ„æº"
            )
        
        try,
            self.logger.info(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡, {task_name}")
            
            # æ‰§è¡Œä»»åŠ¡
            task = asyncio.create_task(executor.execute())
            self.resource_manager.register_active_task(task_name, task)
            
            result = await task
            self.resource_manager.unregister_active_task(task_name)
            
            # è®°å½•æ‰§è¡Œå†å²
            self._record_execution(task_name, result)
            
            # è‡ªåŠ¨é‡è¯•å¤±è´¥ä»»åŠ¡
            if (result.status == TaskStatus.FAILED and,:
                self.config.auto_retry_failed_tasks and,
                task_config.retry_count < self.config.max_retries())
                
                self.logger.info(f"ä»»åŠ¡å¤±è´¥,å‡†å¤‡é‡è¯•, {task_name} (é‡è¯• {task_config.retry_count + 1}/{self.config.max_retries})")
                await asyncio.sleep(self.config.retry_delay())
                
                # å¢åŠ é‡è¯•è®¡æ•°
                task_config.retry_count += 1
                return await self.execute_task(task_name, **kwargs)
            
            return result
            
        finally,
            self.resource_manager.release_slot()
    
    async def execute_tasks(self, task_names, List[str] ,
    execution_mode, Optional[ExecutionMode] = None) -> List[TaskResult]
        """æ‰§è¡Œå¤šä¸ªä»»åŠ¡"""
        mode = execution_mode or self.config.execution_mode()
        if mode == ExecutionMode.SEQUENTIAL,::
            return await self._execute_sequential(task_names)
        elif mode == ExecutionMode.PARALLEL,::
            return await self._execute_parallel(task_names)
        elif mode == ExecutionMode.PIPELINE,::
            return await self._execute_pipeline(task_names)
        elif mode == ExecutionMode.COLLABORATIVE,::
            return await self._execute_collaborative(task_names)
        else,
            raise ValueError(f"ä¸æ”¯æŒçš„æ‰§è¡Œæ¨¡å¼, {mode}")
    
    async def _execute_sequential(self, task_names, List[str]) -> List[TaskResult]
        """é¡ºåºæ‰§è¡Œ"""
        results = []
        
        for task_name in task_names,::
            result = await self.execute_task(task_name)
            results.append(result)
            
            # å¦‚æœä»»åŠ¡å¤±è´¥ä¸”æ²¡æœ‰é…ç½®è‡ªåŠ¨é‡è¯•,åˆ™åœæ­¢æ‰§è¡Œ
            if (result.status == TaskStatus.FAILED and,::
                not self.config.auto_retry_failed_tasks())
                self.logger.warning(f"ä»»åŠ¡ {task_name} å¤±è´¥,åœæ­¢é¡ºåºæ‰§è¡Œ")
                break
        
        return results
    
    async def _execute_parallel(self, task_names, List[str]) -> List[TaskResult]
        """å¹¶è¡Œæ‰§è¡Œ"""
        tasks = []
        
        for task_name in task_names,::
            task = asyncio.create_task(self.execute_task(task_name))
            tasks.append(task)
        
        results == await asyncio.gather(*tasks, return_exceptions == True)::
        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results == []
        for i, result in enumerate(results)::
            if isinstance(result, Exception)::
                processed_results.append(TaskResult(
                    task_name=task_names[i],
    status == TaskStatus.FAILED(),
                    start_time=datetime.now(),
                    end_time=datetime.now(),
                    error_message=str(result)
                ))
            else,
                processed_results.append(result)
        
        return processed_results
    
    async def _execute_pipeline(self, task_names, List[str]) -> List[TaskResult]
        """æµæ°´çº¿æ‰§è¡Œ"""
        results = []
        
        for task_name in task_names,::
            result = await self.execute_task(task_name)
            results.append(result)
            
            # æµæ°´çº¿æ¨¡å¼ä¸‹,ä»»ä½•ä»»åŠ¡å¤±è´¥éƒ½ä¼šåœæ­¢æ•´ä¸ªæµæ°´çº¿
            if result.status != TaskStatus.COMPLETED,::
                self.logger.warning(f"ä»»åŠ¡ {task_name} å¤±è´¥,åœæ­¢æµæ°´çº¿æ‰§è¡Œ")
                break
        
        return results
    
    async def _execute_collaborative(self, task_names, List[str]) -> List[TaskResult]
        """åä½œæ‰§è¡Œ"""
        # ç®€åŒ–çš„åä½œæ‰§è¡Œï¼šå¹¶è¡Œæ‰§è¡Œä½†æœ‰ç»“æœå…±äº«æœºåˆ¶
        results = await self._execute_parallel(task_names)
        
        # åœ¨è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„åä½œé€»è¾‘
        # ä¾‹å¦‚ï¼šä»»åŠ¡é—´ç»“æœå…±äº«ã€åŠ¨æ€è°ƒæ•´æ‰§è¡Œç­–ç•¥ç­‰
        
        return results
    
    def _record_execution(self, task_name, str, result, TaskResult):
        """è®°å½•æ‰§è¡Œå†å²"""
        execution_record = {
            "task_name": task_name,
            "timestamp": datetime.now().isoformat(),
            "status": result.status.value(),
            "execution_time": result.execution_time(),
            "return_code": result.return_code(),
            "error_message": result.error_message()
        }
        
        self.execution_history.append(execution_record)
        
        # é™åˆ¶å†å²è®°å½•æ•°é‡
        if len(self.execution_history()) > 1000,::
            self.execution_history == self.execution_history[-1000,]
        
        # å¼‚æ­¥ä¿å­˜çŠ¶æ€
        if self.config.persistence_enabled,::
            asyncio.create_task(self._save_state())
    
    async def _save_state(self):
        """ä¿å­˜çŠ¶æ€"""
        state = {
            "task_registry": {"name": asdict(config) for name, config in self.task_registry.items()}:
            "execution_history": self.execution_history(),
            "last_saved": datetime.now().isoformat()
        }
        
        await self.persistence.save_state(state["task_registry"] state["execution_history"])
    
    async def _load_saved_state(self):
        """åŠ è½½ä¿å­˜çš„çŠ¶æ€"""
        saved_state = await self.persistence.load_state()
        if saved_state,::
            self.logger.info("åŠ è½½ä¿å­˜çš„è°ƒåº¦å™¨çŠ¶æ€")
            # è¿™é‡Œå¯ä»¥å®ç°çŠ¶æ€æ¢å¤é€»è¾‘
    
    def get_task_status(self, task_name, str) -> Optional[TaskStatus]
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        # ä»æœ€è¿‘çš„æ‰§è¡Œå†å²ä¸­è·å–ä»»åŠ¡çŠ¶æ€
        for record in reversed(self.execution_history())::
            if record["task_name"] == task_name,::
                return TaskStatus(record["status"])
        return None
    
    def get_execution_summary(self) -> Dict[str, Any]
        """è·å–æ‰§è¡Œæ‘˜è¦"""
        total_tasks = len(self.execution_history())
        if total_tasks == 0,::
            return {"total_tasks": 0}
        
        status_counts = {}
        for status in TaskStatus,::
            status_counts[status.value] = sum(
                1 for record in self.execution_history,:,
    if record["status"] == status.value,:
            )
        
        total_execution_time = sum(,
    record["execution_time"] for record in self.execution_history,:
        )

        return {:
            "total_tasks": total_tasks,
            "status_counts": status_counts,
            "average_execution_time": total_execution_time / total_tasks,
            "success_rate": status_counts.get(TaskStatus.COMPLETED.value(), 0) / total_tasks
        }
    
    async def cancel_all_tasks(self):
        """å–æ¶ˆæ‰€æœ‰ä»»åŠ¡"""
        self.logger.info("å–æ¶ˆæ‰€æœ‰æ´»åŠ¨ä»»åŠ¡")
        await self.resource_manager.cancel_all_tasks()
    
    def get_active_tasks(self) -> List[str]
        """è·å–æ´»åŠ¨ä»»åŠ¡åˆ—è¡¨"""
        return list(self.resource_manager.active_tasks.keys())


# å‘åå…¼å®¹å±‚
class LegacyCompatibilityLayer,
    """å‘åå…¼å®¹å±‚"""
    
    def __init__(self, scheduler, UnifiedSchedulerFramework):
        self.scheduler = scheduler
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    async def execute_legacy_pipeline(self, pipeline_config, Dict[str, Any]) -> Dict[str, Any]
        """æ‰§è¡Œé—ç•™æµæ°´çº¿é…ç½®"""
        try,
            # è½¬æ¢é—ç•™é…ç½®ä¸ºæ–°çš„ä»»åŠ¡é…ç½®
            tasks = []
            
            for step_name, step_config in pipeline_config.get("steps", {}).items():::
                task_config == TaskConfig(
                    name=step_name,,
    command=step_config.get("command"),
                    script_path=step_config.get("script_path"),
                    timeout=step_config.get("timeout", 300),
                    priority == TaskPriority(step_config.get("priority", "medium")),
                    dependencies=step_config.get("dependencies", [])
                )
                
                self.scheduler.register_task(task_config)
                tasks.append(step_name)
            
            # æ‰§è¡Œä»»åŠ¡
            results = await self.scheduler.execute_tasks(tasks, ExecutionMode.PIPELINE())
            
            # è½¬æ¢ç»“æœæ ¼å¼
            return {
                "success": all(result.status == TaskStatus.COMPLETED for result in results),::
                "results": [
                    {
                        "name": result.task_name(),
                        "success": result.status == TaskStatus.COMPLETED(),
                        "execution_time": result.execution_time(),
                        "error": result.error_message()
                    }
                    for result in results,:
                ]
            }

        except Exception as e,::
            self.logger.error(f"é—ç•™æµæ°´çº¿æ‰§è¡Œå¤±è´¥, {e}")
            return {
                "success": False,
                "error": str(e)
            }


# é¢„å®šä¹‰çš„æ‰§è¡Œå™¨å·¥å‚
class ExecutorFactory,
    """æ‰§è¡Œå™¨å·¥å‚"""
    
    @staticmethod
def create_command_executor(name, str, command, str, **kwargs) -> CommandTaskExecutor,
        """åˆ›å»ºå‘½ä»¤æ‰§è¡Œå™¨"""
        config == TaskConfig(name=name, command=command, **kwargs)
        return CommandTaskExecutor(config)
    
    @staticmethod
def create_script_executor(name, str, script_path, str, **kwargs) -> CommandTaskExecutor,
        """åˆ›å»ºè„šæœ¬æ‰§è¡Œå™¨"""
        config == TaskConfig(name=name, script_path=script_path, **kwargs)
        return CommandTaskExecutor(config)
    
    @staticmethod
def create_function_executor(name, str, function, Callable, **kwargs) -> PythonFunctionExecutor,
        """åˆ›å»ºå‡½æ•°æ‰§è¡Œå™¨"""
        config == TaskConfig(name=name, **kwargs)
        return PythonFunctionExecutor(config, function)


# ä¾¿æ·çš„è°ƒåº¦å™¨åˆ›å»ºå‡½æ•°
def create_unified_scheduler(config, Optional[SchedulerConfig] = None) -> UnifiedSchedulerFramework,
    """åˆ›å»ºç»Ÿä¸€è°ƒåº¦å™¨"""
    return UnifiedSchedulerFramework(config)


def create_pipeline_scheduler(max_concurrent_tasks, int == 1) -> UnifiedSchedulerFramework,
    """åˆ›å»ºæµæ°´çº¿è°ƒåº¦å™¨"""
    config == SchedulerConfig(
        max_concurrent_tasks=max_concurrent_tasks,,
    execution_mode == ExecutionMode.PIPELINE(),
        auto_retry_failed_tasks == False
    )
    return UnifiedSchedulerFramework(config)


def create_parallel_scheduler(max_concurrent_tasks, int == 4) -> UnifiedSchedulerFramework,
    """åˆ›å»ºå¹¶è¡Œè°ƒåº¦å™¨"""
    config == SchedulerConfig(
        max_concurrent_tasks=max_concurrent_tasks,,
    execution_mode == ExecutionMode.PARALLEL(),
        auto_retry_failed_tasks == True
    )
    return UnifiedSchedulerFramework(config)


# å‘åå…¼å®¹çš„å‡½æ•°
async def execute_command_task(command, str, timeout, int == 300) -> Dict[str, Any]
    """å‘åå…¼å®¹çš„å‘½ä»¤æ‰§è¡Œä»»åŠ¡"""
    scheduler = create_unified_scheduler()
    
    # åˆ›å»ºä¸´æ—¶ä»»åŠ¡
    task_config == TaskConfig(,
    name=f"legacy_command_{int(time.time())}",
        command=command,
        timeout=timeout
    )
    
    scheduler.register_task(task_config)
    result = await scheduler.execute_task(task_config.name())
    
    return {
        "success": result.status == TaskStatus.COMPLETED(),
        "return_code": result.return_code(),
        "stdout": result.stdout(),
        "stderr": result.stderr(),
        "execution_time": result.execution_time(),
        "error": result.error_message()
    }


async def execute_script_task(script_path, str, timeout, int == 300) -> Dict[str, Any]
    """å‘åå…¼å®¹çš„è„šæœ¬æ‰§è¡Œä»»åŠ¡"""
    scheduler = create_unified_scheduler()
    
    # åˆ›å»ºä¸´æ—¶ä»»åŠ¡
    task_config == TaskConfig(,
    name=f"legacy_script_{int(time.time())}",
        script_path=script_path,
        timeout=timeout
    )
    
    scheduler.register_task(task_config)
    result = await scheduler.execute_task(task_config.name())
    
    return {
        "success": result.status == TaskStatus.COMPLETED(),
        "return_code": result.return_code(),
        "stdout": result.stdout(),
        "stderr": result.stderr(),
        "execution_time": result.execution_time(),
        "error": result.error_message()
    }


# æµ‹è¯•å‡½æ•°
async def test_unified_scheduler():
    """æµ‹è¯•ç»Ÿä¸€è°ƒåº¦æ¡†æ¶"""
    print("=== æµ‹è¯•ç»Ÿä¸€è°ƒåº¦æ¡†æ¶ ===\n")
    
    # æµ‹è¯•1, åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    print("--- æµ‹è¯•1, åŸºæœ¬åŠŸèƒ½æµ‹è¯• ---")
    
    scheduler = create_unified_scheduler()
    
    # æ³¨å†Œç®€å•å‘½ä»¤ä»»åŠ¡
    scheduler.register_task(TaskConfig(
        name="test_echo",,
    command="python -c print('Hello, Unified Scheduler!')",
        timeout=10
    ))
    
    result = await scheduler.execute_task("test_echo")
    print(f"âœ“ åŸºæœ¬ä»»åŠ¡æ‰§è¡Œ, {'æˆåŠŸ' if result.status == TaskStatus.COMPLETED else 'å¤±è´¥'}"):::
    print(f"  æ‰§è¡Œæ—¶é—´, {result.execution_time,.3f}s")
    print(f"  è¾“å‡º, {result.stdout.strip()}")
    
    # æµ‹è¯•2, å¹¶è¡Œæ‰§è¡Œ
    print("\n--- æµ‹è¯•2, å¹¶è¡Œæ‰§è¡Œæµ‹è¯• ---")
    
    parallel_scheduler = create_parallel_scheduler(max_concurrent_tasks=3)
    
    # æ³¨å†Œå¤šä¸ªä»»åŠ¡
    tasks = []
    for i in range(3)::
        task_name = f"parallel_task_{i}"
        parallel_scheduler.register_task(TaskConfig(
            name=task_name,,
    command=f"python -c print('Task {i}')",
            timeout=10
        ))
        tasks.append(task_name)
    
    start_time = time.time()
    results = await parallel_scheduler.execute_tasks(tasks)
    total_time = time.time() - start_time
    
    print(f"âœ“ å¹¶è¡Œæ‰§è¡Œ, {sum(1 for r in results if r.status == TaskStatus.COMPLETED())}/{len(results)} æˆåŠŸ"):::
    print(f"  æ€»è€—æ—¶, {"total_time":.3f}s")
    
    # æµ‹è¯•3, æµæ°´çº¿æ‰§è¡Œ
    print("\n--- æµ‹è¯•3, æµæ°´çº¿æ‰§è¡Œæµ‹è¯• ---")
    
    pipeline_scheduler = create_pipeline_scheduler()
    
    # æ³¨å†Œæµæ°´çº¿ä»»åŠ¡
    pipeline_tasks = []
    for i in range(3)::
        task_name = f"pipeline_task_{i}"
        pipeline_scheduler.register_task(TaskConfig(
            name=task_name,,
    command=f"python -c print('Pipeline Step {i}')",
            timeout=10
        ))
        pipeline_tasks.append(task_name)
    
    pipeline_results = await pipeline_scheduler.execute_tasks(pipeline_tasks)
    print(f"âœ“ æµæ°´çº¿æ‰§è¡Œ, {sum(1 for r in pipeline_results if r.status == TaskStatus.COMPLETED())}/{len(pipeline_results)} æˆåŠŸ"):::
    # æµ‹è¯•4, ä»»åŠ¡ä¾èµ–
    print("\n--- æµ‹è¯•4, ä»»åŠ¡ä¾èµ–æµ‹è¯• ---")
    
    dependency_scheduler = create_unified_scheduler()
    
    # æ³¨å†Œä¾èµ–ä»»åŠ¡
    dependency_scheduler.register_task(TaskConfig(
        name="dependency_task_1",,
    command="python -c print('Dependency 1')",
        timeout=10
    ))
    
    dependency_scheduler.register_task(TaskConfig(
        name="dependency_task_2",,
    command="python -c print('Dependency 2')",
        dependencies=["dependency_task_1"]
        timeout=10
    ))
    
    dep_results = await dependency_scheduler.execute_tasks(["dependency_task_1", "dependency_task_2"])
    print(f"âœ“ ä¾èµ–ä»»åŠ¡æ‰§è¡Œ, {sum(1 for r in dep_results if r.status == TaskStatus.COMPLETED())}/{len(dep_results)} æˆåŠŸ"):::
    # æµ‹è¯•5, å‘åå…¼å®¹
    print("\n--- æµ‹è¯•5, å‘åå…¼å®¹æµ‹è¯• ---")
    
    legacy_result = await execute_command_task("python -c print('Legacy compatibility test')")
    print(f"âœ“ é—ç•™æ¥å£, {'æˆåŠŸ' if legacy_result['success'] else 'å¤±è´¥'}"):::
    print(f"  è¾“å‡º, {legacy_result['stdout'].strip()}")
    
    # æµ‹è¯•6, æ‰§è¡Œæ‘˜è¦
    print("\n--- æµ‹è¯•6, æ‰§è¡Œæ‘˜è¦ ---")
    
    summary = scheduler.get_execution_summary()
    print(f"âœ“ æ‰§è¡Œæ‘˜è¦,")
    print(f"  æ€»ä»»åŠ¡æ•°, {summary['total_tasks']}")
    print(f"  æˆåŠŸç‡, {summary.get('success_rate', 0).2%}")
    print(f"  å¹³å‡æ‰§è¡Œæ—¶é—´, {summary.get('average_execution_time', 0).3f}s")
    
    print("\n=ç»Ÿä¸€è°ƒåº¦æ¡†æ¶æµ‹è¯•å®Œæˆ ===")
    return True


if __name'__main__':::
    import os
    success = asyncio.run(test_unified_scheduler())
    if success,::
        print("\nğŸ‰ ç»Ÿä¸€è°ƒåº¦æ¡†æ¶å·¥ä½œæ­£å¸¸ï¼")
        exit(0)
    else,
        print("\nâŒ ç»Ÿä¸€è°ƒåº¦æ¡†æ¶å­˜åœ¨é—®é¢˜")
        exit(1)
