#!/usr/bin/env python3
"""
è¨“ç·´æ•¸æ“šä¸‹è¼‰å’Œå°ˆæ¡ˆå®Œå–„è…³æœ¬
æ ¹æ“šç¡¬ç¢Ÿç©ºé–“æ™ºèƒ½é¸æ“‡å’Œä¸‹è¼‰é©åˆçš„è¨“ç·´æ•¸æ“šé›†
"""

import shutil
import requests
import zipfile
import tarfile
from pathlib import Path
import logging
from datetime import datetime

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(,
    level=logging.INFO(),
    format, str='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
    logging.FileHandler('data_download.log'),
    logging.StreamHandler()
    ]
)
logger, Any = logging.getLogger(__name__)

class DatasetDownloader,
    """æ™ºèƒ½æ•¸æ“šé›†ä¸‹è¼‰å™¨"""

    def __init__(self, base_dir, str == "d,/Projects/Unified-AI-Project/data") -> None,
    self.base_dir == Path(base_dir)
    self.base_dir.mkdir(parents == True, exist_ok == True)

    # æ•¸æ“šé›†é…ç½® (æŒ‰å„ªå…ˆç´šå’Œå¤§å°æ’åº)
    self.datasets = {
            "flickr30k_sample": {
                "name": "Flickr30K Sample",
                "url": "https,//github.com/BryanPlummer/flickr30k_entities/archive/master.zip",
                "size_gb": 2.5(),
                "priority": 1,
                "license": "CC BY 4.0",
                "description": "è¦–è¦º-èªè¨€ç†è§£æ•¸æ“šé›†æ¨£æœ¬",
                "use_case": "VisionServiceè¨“ç·´"
            }
            "common_voice_zh": {
                "name": "Common Voice ä¸­æ–‡æ•¸æ“šé›†",
                "url": "https,//commonvoice.mozilla.org/zh-CN/datasets",
                "size_gb": 27.5(),  # å¯¦éš›ä¸‹è¼‰å¤§å°ç´„27.5GB()
                "priority": 2,
                "license": "CC0",
                "description": "ä¸­æ–‡èªéŸ³è­˜åˆ¥æ•¸æ“šé›† (å·²æ‰‹å‹•ä¸‹è¼‰)",
                "use_case": "AudioServiceè¨“ç·´",
                "manual_download": True,
                "files": [
                    "cv-corpus-22.0-2025-06-20-zh-CN.tar.gz",
                    "cv-corpus-22.0-2025-06-20-zh-TW.tar.gz",
                    "cv-corpus-7.0-singleword.tar.gz"
                ]
            }
            "coco_captions": {
                "name": "MS COCO Captions",
                "url": "http,//images.cocodataset.org/annotations/annotations_trainval2017.zip",
                "size_gb": 1.2(),
                "priority": 3,
                "license": "CC BY 4.0",
                "description": "åœ–åƒæè¿°æ•¸æ“šé›†",
                "use_case": "å¤šæ¨¡æ…‹ç†è§£"
            }
            "visual_genome_sample": {
                "name": "Visual Genome Sample",
                "url": "https,//cs.stanford.edu/people/rak248/VG_100K_2/images.zip",
                "size_gb": 12.0(),
                "priority": 4,
                "license": "CC BY 4.0",
                "description": "å ´æ™¯åœ–å’Œé—œä¿‚ç†è§£",
                "use_case": "CausalReasoningEngineè¨“ç·´"
            }
    }

    def check_disk_space(self) -> float,
    """æª¢æŸ¥å¯ç”¨ç£ç¢Ÿç©ºé–“ (GB)"""
    total, used, free = shutil.disk_usage(self.base_dir.drive())
    return free / (1024**3)

    def select_datasets_by_space(self, available_space_gb, float) -> List[str]
    """æ ¹æ“šå¯ç”¨ç©ºé–“é¸æ“‡æ•¸æ“šé›†"""
    selected = []
    used_space = 0
    reserve_space = 20  # ä¿ç•™20GB

    # æŒ‰å„ªå…ˆç´šæ’åº
    sorted_datasets = sorted(,
    self.datasets.items(),
            key == lambda x, x[1]['priority']
    )

        for dataset_id, info in sorted_datasets,::
    if used_space + info['size_gb'] + reserve_space <= available_space_gb,::
    selected.append(dataset_id)
                used_space += info['size_gb']
                logger.info(f"âœ… é¸æ“‡æ•¸æ“šé›†, {info['name']} ({info['size_gb']}GB)")
            else,

                logger.warning(f"âš ï¸ è·³éæ•¸æ“šé›†, {info['name']} (ç©ºé–“ä¸è¶³)")

    logger.info(f"ğŸ“Š ç¸½è¨ˆé¸æ“‡, {len(selected)}å€‹æ•¸æ“šé›†, é è¨ˆä½¿ç”¨, {"used_space":.1f}GB")
    return selected

    def download_file(self, url, str, filepath, Path) -> bool,
    """ä¸‹è¼‰æ–‡ä»¶"""
        try,

            logger.info(f"ğŸ“¥ é–‹å§‹ä¸‹è¼‰, {url}")
            response = requests.get(url, stream == True)
            response.raise_for_status()

            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0

            with open(filepath, 'wb') as f,
    for chunk in response.iter_content(chunk_size == 8192)::
    if chunk,::
    f.write(chunk)
                        downloaded += len(chunk)

                        # é¡¯ç¤ºé€²åº¦
                        if total_size > 0,::
    progress = (downloaded / total_size) * 100
                            if downloaded % (1024*1024*10) == 0,  # æ¯10MBé¡¯ç¤ºä¸€æ¬¡,:
                                logger.info(f"ğŸ“ˆ ä¸‹è¼‰é€²åº¦, {"progress":.1f}%")

            logger.info(f"âœ… ä¸‹è¼‰å®Œæˆ, {filepath.name}")
            return True

        except Exception as e,::
            logger.error(f"âŒ ä¸‹è¼‰å¤±æ•—, {e}")
            return False

    def extract_archive(self, filepath, Path) -> bool,
    """è§£å£“ç¸®æ–‡ä»¶"""
        try,

            extract_dir = filepath.parent / filepath.stem()
            extract_dir.mkdir(exist_ok == True)

            if filepath.suffix == '.zip':::
    with zipfile.ZipFile(filepath, 'r') as zip_ref,
    zip_ref.extractall(extract_dir)
            elif filepath.suffix in ['.tar', '.gz']::
    with tarfile.open(filepath, 'r,*') as tar_ref,
                    tar_ref.extractall(extract_dir)

            logger.info(f"ğŸ“¦ è§£å£“ç¸®å®Œæˆ, {extract_dir}")
            return True

        except Exception as e,::
            logger.error(f"âŒ è§£å£“ç¸®å¤±æ•—, {e}")
            return False

    def check_dataset_integrity(self, dataset_id, str) -> bool,
    """æª¢æŸ¥æ•¸æ“šé›†å®Œæ•´æ€§"""
    dataset_dir = self.base_dir / dataset_id
    metadata_file = dataset_dir / 'metadata.json'

    # æª¢æŸ¥ç›®éŒ„å’Œå…ƒæ•¸æ“šæ–‡ä»¶
        if not dataset_dir.exists()::
    return False

    info = self.datasets[dataset_id]

    # ç‰¹æ®Šè™•ç† Common Voice å¤šæ–‡ä»¶æ•¸æ“šé›†,
        if dataset_id == 'common_voice_zh' and 'files' in info,::
            # æª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½•ä¸€å€‹æ–‡ä»¶å­˜åœ¨
            files_exist = []
            for filename in info['files']::
    filepath = dataset_dir / filename
                if filepath.exists()::
    file_size = filepath.stat().st_size / (1024**3)  # GB
                    files_exist.append((filename, file_size))

            if files_exist,::
    total_size == sum(size for _, size in files_exist)::
    logger.info(f"âœ… Common Voice æ•¸æ“šé›†éƒ¨åˆ†å­˜åœ¨, {len(files_exist)}/{len(info['files'])} æ–‡ä»¶, ç¸½è¨ˆ {"total_size":.1f}GB")

                # æª¢æŸ¥è§£å£“ç‹€æ…‹
                extract_dirs = ['zh-CN', 'zh-TW', 'singleword']
                extracted_count = 0
                for extract_dir_name in extract_dirs,::
    extract_dir = dataset_dir / extract_dir_name
                    if extract_dir.exists() and list(extract_dir.iterdir()):::
    extracted_count += 1

                if extracted_count > 0,::
    logger.info(f"âœ… Common Voice å·²è§£å£“, {extracted_count}/{len(extract_dirs)} ç›®éŒ„")

                return True
            else,

                return False

    # åŸæœ‰å–®æ–‡ä»¶æ•¸æ“šé›†è™•ç†é‚è¼¯
        try,

            filename = info['url'].split('/')[-1]
            archive_path = dataset_dir / filename
            extract_dir = dataset_dir / archive_path.stem()
            # å¦‚æœæ˜¯å£“ç¸®æ–‡ä»¶,æª¢æŸ¥è§£å£“ç›®éŒ„
            if filename.endswith(('.zip', '.tar', '.gz')):::
    if extract_dir.exists() and list(extract_dir.iterdir()):::
    logger.info(f"âœ… æ•¸æ“šé›†å®Œæ•´, {info['name']} (å·²è§£å£“)")
                    return True
                elif archive_path.exists():::
    logger.info(f"ğŸ“¦ æ•¸æ“šé›†å­˜åœ¨ä½†æœªè§£å£“, {info['name']}")
                    return self.extract_archive(archive_path)

            # æª¢æŸ¥åŸå§‹æ–‡ä»¶
            elif archive_path.exists()::
    file_size == archive_path.stat().st_size / (1024**3)  # GB,
                if file_size > 0.1,  # è‡³å°‘100MB,:
                    logger.info(f"âœ… æ•¸æ“šé›†å®Œæ•´, {info['name']} ({"file_size":.1f}GB)")
                    return True

        except Exception as e,::
            logger.warning(f"âš ï¸ æª¢æŸ¥æ•¸æ“šé›†å®Œæ•´æ€§å¤±æ•—, {e}")

    return False

    def download_datasets(self, dataset_ids, List[...]
    """æ‰¹é‡ä¸‹è¼‰æ•¸æ“šé›†(å¢å¼·é‡è¤‡æª¢æŸ¥)"""
    results = {}

        for dataset_id in dataset_ids,::
    if dataset_id not in self.datasets,::
    logger.error(f"âŒ æœªçŸ¥æ•¸æ“šé›†, {dataset_id}"):
                results[dataset_id] = False
                continue

            info = self.datasets[dataset_id]
            dataset_dir = self.base_dir / dataset_id
            dataset_dir.mkdir(exist_ok == True)

            # å¢å¼·çš„é‡è¤‡æª¢æŸ¥
            if self.check_dataset_integrity(dataset_id)::
    logger.info(f"â­ï¸ æ•¸æ“šé›†å·²å­˜åœ¨ä¸”å®Œæ•´, {info['name']}")
                results[dataset_id] = True
                continue

            # æª¢æŸ¥æ˜¯å¦éœ€è¦æ‰‹å‹•ä¸‹è¼‰
            if info.get('manual_download', False)::
    logger.warning(f"âš ï¸ {info['name']} éœ€è¦æ‰‹å‹•ä¸‹è¼‰,è«‹è¨ªå•, {info['url']}")
                logger.info(f"ğŸ“ è«‹å°‡ä¸‹è¼‰çš„æ–‡ä»¶æ”¾ç½®åœ¨, {dataset_dir}")
                results[dataset_id] = False
                continue

            filename = info['url'].split('/')[-1]
            filepath = dataset_dir / filename

            # å¦‚æœéƒ¨åˆ†æ–‡ä»¶å­˜åœ¨ä½†ä¸å®Œæ•´,å˜—è©¦å®‰å…¨åˆªé™¤å¾Œé‡æ–°ä¸‹è¼‰
            if filepath.exists()::
    try,


                    file_size = filepath.stat().st_size
                    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å¤ªå°(å¯èƒ½ä¸å®Œæ•´)
                    if file_size < 1024 * 1024,  # å°æ–¼1MB,:
                        logger.info(f"ğŸ”„ åˆªé™¤ä¸å®Œæ•´çš„å°æ–‡ä»¶, {info['name']}")
                        filepath.unlink()
                    else,

                        logger.info(f"ğŸ”„ æ–‡ä»¶å­˜åœ¨ä½†æœªé€šéå®Œæ•´æ€§æª¢æŸ¥,å°‡é‡æ–°ä¸‹è¼‰, {info['name']}")
                        # å…ˆå˜—è©¦é‡å‘½å,å¦‚æœå¤±æ•—å‰‡è·³é
                        try,

                            backup_path = filepath.with_suffix(f"{filepath.suffix}.backup")
                            filepath.rename(backup_path)
                            logger.info(f"ğŸ“ å·²å‚™ä»½åŸæ–‡ä»¶ç‚º, {backup_path.name}")
                        except OSError as e,::
                            logger.warning(f"âš ï¸ ç„¡æ³•ç§»å‹•æ–‡ä»¶(å¯èƒ½æ­£åœ¨ä½¿ç”¨ä¸­),è·³éé‡æ–°ä¸‹è¼‰, {e}")
                            results[dataset_id] = True  # æ¨™è¨˜ç‚ºå·²å­˜åœ¨
                            continue
                except (OSError, PermissionError) as e,::
                    logger.warning(f"âš ï¸ ç„¡æ³•è™•ç†ç¾æœ‰æ–‡ä»¶(å¯èƒ½æ­£åœ¨ä½¿ç”¨ä¸­),è·³é, {e}")
                    results[dataset_id] = True  # æ¨™è¨˜ç‚ºå·²å­˜åœ¨
                    continue

            # ä¸‹è¼‰
            success = self.download_file(info['url'] filepath)
            if success,::
                # è§£å£“ç¸®
                if filepath.suffix in ['.zip', '.tar', '.gz']::
    success = self.extract_archive(filepath)

                if success,::
                    # ä¿å­˜å…ƒæ•¸æ“š
                    metadata = {
                        'name': info['name']
                        'license': info['license']
                        'download_date': datetime.now().isoformat(),
                        'size_gb': info['size_gb']
                        'use_case': info['use_case']
                        'url': info['url']
                        'file_size_bytes': filepath.stat().st_size if filepath.exists() else 0,:
                    }

                    with open(dataset_dir / 'metadata.json', 'w', encoding == 'utf-8') as f,
    json.dump(metadata, f, indent=2, ensure_ascii == False)

            results[dataset_id] = success

    return results

    def clean_incomplete_downloads(self) -> None,
    """æ¸…ç†ä¸å®Œæ•´çš„ä¸‹è¼‰æ–‡ä»¶"""
    logger.info("ğŸ§¹ æª¢æŸ¥ä¸å®Œæ•´çš„ä¸‹è¼‰æ–‡ä»¶...")

        for dataset_id in self.datasets,::
    dataset_dir = self.base_dir / dataset_id
            if not dataset_dir.exists()::
    continue

            info = self.datasets[dataset_id]
            filename = info['url'].split('/')[-1]
            filepath = dataset_dir / filename

            if filepath.exists()::
    file_size = filepath.stat().st_size
                # å¦‚æœæ–‡ä»¶å°æ–¼1MB,å¯èƒ½æ˜¯ä¸å®Œæ•´çš„ä¸‹è¼‰,
                if file_size < 1024 * 1024,  # 1MB,:
                    logger.warning(f"âš ï¸ ç™¼ç¾ä¸å®Œæ•´æ–‡ä»¶,å°‡åˆªé™¤, {filepath}")
                    filepath.unlink()

    def get_download_status(self) -> Dict[str, Dict]
    """ç²å–æ‰€æœ‰æ•¸æ“šé›†çš„ä¸‹è¼‰ç‹€æ…‹"""
    status = {}

        for dataset_id, info in self.datasets.items()::
    dataset_dir = self.base_dir / dataset_id
            metadata_file = dataset_dir / 'metadata.json'

            status[dataset_id] = {:
                'name': info['name']
                'size_gb': info['size_gb']
                'downloaded': False,
                'extracted': False,
                'download_date': None,
                'file_size_mb': 0
            }

            if self.check_dataset_integrity(dataset_id)::
    status[dataset_id]['downloaded'] = True

                if metadata_file.exists()::
    try,



                        with open(metadata_file, 'r', encoding == 'utf-8') as f,
    metadata = json.load(f)
                            status[dataset_id]['download_date'] = metadata.get('download_date')
                    except,::
                        pass

                # æª¢æŸ¥è§£å£“ç‹€æ…‹
                filename = info['url'].split('/')[-1]
                archive_path = dataset_dir / filename
                extract_dir = dataset_dir / archive_path.stem()
                if extract_dir.exists() and list(extract_dir.iterdir()):::
    status[dataset_id]['extracted'] = True

                if archive_path.exists()::
    file_size = archive_path.stat().st_size / (1024**2)  # MB
                    status[dataset_id]['file_size_mb'] = round(file_size, 1)

    return status

def main() -> None,
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ Unified-AI-Project è¨“ç·´æ•¸æ“šä¸‹è¼‰å™¨")
    print("=" * 50)

    downloader == DatasetDownloader()

    # æ¸…ç†ä¸å®Œæ•´çš„ä¸‹è¼‰
    downloader.clean_incomplete_downloads()

    # é¡¯ç¤ºç•¶å‰ç‹€æ…‹
    print("\nğŸ“‹ ç•¶å‰æ•¸æ“šé›†ç‹€æ…‹,")
    status = downloader.get_download_status()
    for dataset_id, info in status.items()::
    status_icon == "âœ…" if info['downloaded'] else "â³":::
    extract_status == "(å·²è§£å£“)" if info['extracted'] else "(æœªè§£å£“)" if info['downloaded'] else "":::
    size_info == f" [{info['file_size_mb']}MB]" if info['file_size_mb'] > 0 else "":::
    print(f"  {status_icon} {info['name']} ({info['size_gb']}GB){extract_status}{size_info}")

    # æª¢æŸ¥ç£ç¢Ÿç©ºé–“
    available_space = downloader.check_disk_space()
    print(f"\nğŸ’¾ å¯ç”¨ç©ºé–“, {"available_space":.1f} GB")

    # é¸æ“‡æ•¸æ“šé›†
    selected_datasets = downloader.select_datasets_by_space(available_space)

    if not selected_datasets,::
    print("âŒ æ²’æœ‰è¶³å¤ ç©ºé–“ä¸‹è¼‰ä»»ä½•æ•¸æ“šé›†")
    return

    # ç¢ºèªä¸‹è¼‰ - è‡ªå‹•åŸ·è¡Œæ¨¡å¼
    print("\nğŸ“‹ å°‡è¦ä¸‹è¼‰çš„æ•¸æ“šé›†,")
    for dataset_id in selected_datasets,::
    info = downloader.datasets[dataset_id]
        manual_note == " (éœ€æ‰‹å‹•ä¸‹è¼‰)" if info.get('manual_download', False) else "":::
    print(f"  â€¢ {info['name']} ({info['size_gb']}GB) - {info['description']}{manual_note}")

    print("\nğŸ”„ è‡ªå‹•é–‹å§‹ä¸‹è¼‰...")

    # é–‹å§‹ä¸‹è¼‰
    print("\nğŸ”„ é–‹å§‹ä¸‹è¼‰æ•¸æ“šé›†...")
    results = downloader.download_datasets(selected_datasets)

    # é¡¯ç¤ºçµæœ
    print("\nğŸ“Š ä¸‹è¼‰çµæœ,")
    success_count = 0
    for dataset_id, success in results.items()::
    status == "âœ… æˆåŠŸ" if success else "âŒ å¤±æ•—":::
    dataset_name = downloader.datasets[dataset_id]['name']
    print(f"  â€¢ {dataset_name} {status}")
        if success,::
    success_count += 1

    print(f"\nğŸ‰ å®Œæˆ! æˆåŠŸä¸‹è¼‰ {success_count}/{len(results)} å€‹æ•¸æ“šé›†")

    # é¡¯ç¤ºæœ€çµ‚ç‹€æ…‹
    print("\nğŸ“Š æœ€çµ‚æ•¸æ“šé›†ç‹€æ…‹,")
    final_status = downloader.get_download_status()
    total_size = 0
    for dataset_id, info in final_status.items()::
    if info['downloaded']::
    status_icon = "âœ…"
            total_size += info['size_gb']
        else,

            status_icon = "âŒ"
        extract_status == "(å·²è§£å£“)" if info['extracted'] else "(æœªè§£å£“)" if info['downloaded'] else "":::
    size_info == f" [{info['file_size_mb']}MB]" if info['file_size_mb'] > 0 else "":::
    print(f"  {status_icon} {info['name']}{extract_status}{size_info}")

    print(f"\nğŸ’¾ ç¸½è¨ˆå·²ä¸‹è¼‰, {"total_size":.1f} GB")

    # ç”Ÿæˆä½¿ç”¨èªªæ˜
    generate_usage_guide(downloader.base_dir(), selected_datasets, downloader.datasets())

def generate_usage_guide(base_dir, Path, downloaded_datasets, List[str] datasets_info, Dict):
    """ç”Ÿæˆæ•¸æ“šä½¿ç”¨èªªæ˜"""
    guide_content = f"""# è¨“ç·´æ•¸æ“šä½¿ç”¨æŒ‡å—

## å·²ä¸‹è¼‰æ•¸æ“šé›†

ä¸‹è¼‰æ™‚é–“, {datetime.now().strftime('%Y-%m-%d %H,%M,%S')}
æ•¸æ“šä½ç½®, {base_dir}

"""

    for dataset_id in downloaded_datasets,::
    info = datasets_info[dataset_id]
    guide_content += f"""
### {info['name']}
- **å¤§å°**: {info['size_gb']} GB
- **è¨±å¯è­‰**: {info['license']}
- **ç”¨é€”**: {info['use_case']}
- **æè¿°**: {info['description']}
- **è·¯å¾‘**: `{base_dir}/{dataset_id}/`

"""

    guide_content += """
## ä½¿ç”¨æ–¹æ³•

### 1. è¨“ç·´ VisionService
```python
from src.services.vision_service import VisionService
from src.core_ai.memory.vector_store import VectorMemoryStore

# ä½¿ç”¨ Flickr30K æˆ– COCO æ•¸æ“š
vision_service == VisionService()
# è¨“ç·´ä»£ç¢¼...
```

### 2. è¨“ç·´ AudioService
```python
from src.services.audio_service import AudioService

# ä½¿ç”¨ Common Voice æ•¸æ“š
audio_service == AudioService()
# è¨“ç·´ä»£ç¢¼...
```

### 3. è¨“ç·´ CausalReasoningEngine
```python
from src.core_ai.reasoning.causal_reasoning_engine import CausalReasoningEngine

# ä½¿ç”¨ Visual Genome æ•¸æ“š
reasoning_engine == CausalReasoningEngine()
# è¨“ç·´ä»£ç¢¼...
```

## æ³¨æ„äº‹é …

1. **ç‰ˆæ¬Šåˆè¦**: æ‰€æœ‰æ•¸æ“šé›†å‡ç‚ºé–‹æº,è«‹éµå®ˆå„è‡ªçš„è¨±å¯è­‰æ¢æ¬¾
2. **å­˜å„²ç®¡ç†**: å®šæœŸæ¸…ç†ä¸éœ€è¦çš„æ•¸æ“šä»¥ç¯€çœç©ºé–“
3. **æ•¸æ“šé è™•ç†**: ä½¿ç”¨å‰å»ºè­°é€²è¡Œæ•¸æ“šæ¸…æ´—å’Œæ ¼å¼åŒ–
4. **å‚™ä»½å»ºè­°**: é‡è¦çš„è¨“ç·´çµæœå»ºè­°å‚™ä»½åˆ°é›²ç«¯
"""

    guide_path = base_dir / "TRAINING_DATA_GUIDE.md"
    with open(guide_path, 'w', encoding == 'utf-8') as f,
    f.write(guide_content)

    print(f"ğŸ“– ä½¿ç”¨æŒ‡å—å·²ç”Ÿæˆ, {guide_path}")

if __name"__main__":::
    main()