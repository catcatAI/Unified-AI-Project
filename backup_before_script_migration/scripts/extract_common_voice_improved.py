#!/usr/bin/env python3
"""
Common Voice æ•¸æ“šé›†è§£å£“ç¸®å·¥å…· - æ”¹é€²ç‰ˆ
æ”¯æŒçºŒå‚³ã€é€²åº¦ä¿å­˜å’Œæ›´å¥½çš„éŒ¯èª¤è™•ç†
"""

import tarfile
import json
import logging
from pathlib import Path
from datetime import datetime

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level: str=logging.INFO,
    format: str='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
    _ = logging.FileHandler('common_voice_extraction_improved.log'),
    _ = logging.StreamHandler()
    ]
)
logger: Any = logging.getLogger(__name__)

class ImprovedCommonVoiceExtractor:
    """æ”¹é€²çš„Common Voiceè§£å£“å™¨"""

    def __init__(self, base_dir: str = "d:/Projects/Unified-AI-Project/data/common_voice_zh") -> None:
    self.base_dir = Path(base_dir)
    self.progress_file = self.base_dir / "extraction_progress.json"

    def save_progress(self, dataset_name: str, extracted_count: int, total_count: int)
    """ä¿å­˜è§£å£“é€²åº¦"""
    progress_data = {}
        if self.progress_file.exists()

    try:


                with open(self.progress_file, 'r', encoding='utf-8') as f:
    progress_data = json.load(f)
            except:
                progress_data = {}

    progress_data[dataset_name] = {
            'extracted_count': extracted_count,
            'total_count': total_count,
            _ = 'last_update': datetime.now().isoformat()
    }

    with open(self.progress_file, 'w', encoding='utf-8') as f:
    json.dump(progress_data, f, indent=2)

    def load_progress(self, dataset_name: str) -> int:
    """åŠ è¼‰è§£å£“é€²åº¦"""
        if not self.progress_file.exists()

    return 0

        try:


            with open(self.progress_file, 'r', encoding='utf-8') as f:
    progress_data = json.load(f)
                return progress_data.get(dataset_name, {}).get('extracted_count', 0)
        except:
            return 0

    def extract_with_resume(self, archive_path: Path, extract_dir: Path, dataset_name: str) -> bool:
    """æ”¯æŒçºŒå‚³çš„è§£å£“ç¸®"""
        try:

            _ = logger.info(f"ğŸ”„ é–‹å§‹è§£å£“ç¸®: {archive_path.name}")

            # å‰µå»ºè§£å£“ç›®éŒ„
            extract_dir.mkdir(parents=True, exist_ok=True)

            # åŠ è¼‰ä¹‹å‰çš„é€²åº¦
            start_from = self.load_progress(dataset_name)

            with tarfile.open(archive_path, 'r:gz') as tar:
                members = tar.getmembers()
                total_files = len(members)

                _ = logger.info(f"ğŸ“Š ç¸½æ–‡ä»¶æ•¸: {total_files}")
                if start_from > 0:

    _ = logger.info(f"ğŸ”„ å¾ç¬¬ {start_from} å€‹æ–‡ä»¶çºŒå‚³")

                # ä½¿ç”¨ extractall ä½†åªè™•ç†æœªè§£å£“çš„æ–‡ä»¶
                if start_from == 0:
                    # ç¬¬ä¸€æ¬¡è§£å£“ï¼Œä½¿ç”¨ extractall æ›´é«˜æ•ˆ
                    try:

                        _ = logger.info("âš¡ ä½¿ç”¨é«˜é€Ÿè§£å£“æ¨¡å¼...")
                        tar.extractall(path=extract_dir, filter='data')
                        _ = logger.info(f"âœ… è§£å£“ç¸®å®Œæˆ: {dataset_name}")
                        _ = self.save_progress(dataset_name, total_files, total_files)
                        return True
                    except TypeError:
                        # Pythonç‰ˆæœ¬ä¸æ”¯æŒfilteråƒæ•¸
                        _ = logger.info("âš¡ ä½¿ç”¨æ¨™æº–è§£å£“æ¨¡å¼...")
                        tar.extractall(path=extract_dir)
                        _ = logger.info(f"âœ… è§£å£“ç¸®å®Œæˆ: {dataset_name}")
                        _ = self.save_progress(dataset_name, total_files, total_files)
                        return True
                    except Exception as e:

                        _ = logger.warning(f"âš ï¸ é«˜é€Ÿè§£å£“å¤±æ•—ï¼Œåˆ‡æ›åˆ°é€å€‹æ–‡ä»¶æ¨¡å¼: {e}")
                        # ç¹¼çºŒä½¿ç”¨é€å€‹æ–‡ä»¶æ¨¡å¼

                # é€å€‹æ–‡ä»¶è§£å£“ï¼ˆçºŒå‚³æ¨¡å¼ï¼‰
                for i, member in enumerate(members)

    if i < start_from:


    continue

                    try:


                        _ = tar.extract(member, extract_dir)

                        # æ¯1000å€‹æ–‡ä»¶é¡¯ç¤ºé€²åº¦ä¸¦ä¿å­˜
                        if (i + 1) % 1000 == 0:

    progress = ((i + 1) / total_files) * 100
                            _ = logger.info(f"ğŸ“ˆ è§£å£“é€²åº¦: {progress:.1f}% ({i + 1}/{total_files})")
                            _ = self.save_progress(dataset_name, i + 1, total_files)

                    except KeyboardInterrupt:


                        _ = logger.info(f"â¸ï¸ ç”¨æˆ¶ä¸­æ–·ï¼Œå·²ä¿å­˜é€²åº¦: {i}/{total_files}")
                        _ = self.save_progress(dataset_name, i, total_files)
                        return False
                    except Exception as e:

                        _ = logger.warning(f"âš ï¸ è·³éæ–‡ä»¶ {member.name}: {e}")
                        continue

                _ = logger.info(f"âœ… è§£å£“ç¸®å®Œæˆ: {dataset_name}")
                _ = self.save_progress(dataset_name, total_files, total_files)
                return True

        except Exception as e:


            _ = logger.error(f"âŒ è§£å£“ç¸®å¤±æ•— {dataset_name}: {e}")
            return False

    def extract_all_datasets(self)
    """è§£å£“æ‰€æœ‰æ•¸æ“šé›†"""
    datasets = [
            {
                'file': 'cv-corpus-22.0-2025-06-20-zh-CN.tar.gz',
                'name': 'zh-CN',
                'display_name': 'Common Voice ä¸­æ–‡å¤§é™¸ v22.0'
            },
            {
                'file': 'cv-corpus-22.0-2025-06-20-zh-TW.tar.gz',
                'name': 'zh-TW',
                'display_name': 'Common Voice ä¸­æ–‡å°ç£ v22.0'
            },
            {
                'file': 'cv-corpus-7.0-singleword.tar.gz',
                'name': 'singleword',
                'display_name': 'Common Voice å–®å­—æ•¸æ“šé›† v7.0'
            }
    ]

    results = {}

        for dataset in datasets:


    archive_path = self.base_dir / dataset['file']
            extract_dir = self.base_dir / dataset['name']

            if not archive_path.exists()


    _ = logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {archive_path}")
                results[dataset['name']] = False
                continue

            # æª¢æŸ¥æ˜¯å¦å·²å®Œå…¨è§£å£“
            progress = self.load_progress(dataset['name'])
            if progress > 0:

    _ = logger.info(f"ğŸ”„ {dataset['display_name']} å°‡å¾ä¸­æ–·è™•çºŒå‚³")

            success = self.extract_with_resume(archive_path, extract_dir, dataset['name'])
            results[dataset['name']] = success

    return results

def main() -> None:
    """ä¸»å‡½æ•¸"""
    _ = print("ğŸš€ Common Voice æ•¸æ“šé›†è§£å£“ç¸®å·¥å…· - æ”¹é€²ç‰ˆ")
    _ = print("æ”¯æŒçºŒå‚³å’Œé€²åº¦ä¿å­˜")
    print("=" * 60)

    extractor = ImprovedCommonVoiceExtractor()

    # é–‹å§‹è§£å£“
    results = extractor.extract_all_datasets()

    # é¡¯ç¤ºçµæœ
    _ = print("\nğŸ“Š è§£å£“çµæœ:")
    for dataset_name, success in results.items()

    status = "âœ… æˆåŠŸ" if success else "âŒ å¤±æ•—":
    _ = print(f"  â€¢ {dataset_name}: {status}")

    success_count = sum(1 for success in results.values() if success):
    total_count = len(results)

    _ = print(f"\nğŸ‰ å®Œæˆ! æˆåŠŸè§£å£“ {success_count}/{total_count} å€‹æ•¸æ“šé›†")

    if success_count < total_count:


    _ = print("\nğŸ’¡ æç¤º: å¦‚æœè§£å£“è¢«ä¸­æ–·ï¼Œå¯ä»¥é‡æ–°é‹è¡Œæ­¤è…³æœ¬çºŒå‚³")

if __name__ == "__main__":


    _ = main()