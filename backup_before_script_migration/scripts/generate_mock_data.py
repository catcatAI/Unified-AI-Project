#!/usr/bin/env python3
"""
ç”Ÿæˆå°è¦æ¨¡è¨“ç·´æ•¸æ“šç”¨æ–¼æ¸¬è©¦å’Œåˆæ­¥è¨“ç·´
"""

import json
import random
from pathlib import Path
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO(), format='%(asctime)s - %(levelname)s - %(message)s')
logger, Any = logging.getLogger(__name__)

class MockDataGenerator,
    """æ¨¡æ“¬æ•¸æ“šç”Ÿæˆå™¨"""

    def __init__(self, base_dir, str == "d,/Projects/Unified-AI-Project/data") -> None,
    self.base_dir == Path(base_dir)
    self.base_dir.mkdir(parents == True, exist_ok == True)

    def generate_vision_data(self):
    """ç”Ÿæˆè¦–è¦ºè¨“ç·´æ•¸æ“šæ¨£æœ¬"""
    vision_dir = self.base_dir / "vision_samples"
    vision_dir.mkdir(exist_ok == True)

    # ç”Ÿæˆåœ–åƒæè¿°æ¨£æœ¬
    samples = []
    categories = ["person", "car", "dog", "cat", "building", "tree", "food", "computer"]

        for i in range(100)::
    category = random.choice(categories)
            sample == {:
                "image_id": f"img_{"i":03d}",
                "caption": f"A {category} in a natural setting",
                "objects": [
                    {
                        "label": category,
                        "bbox": [
                            random.randint(0, 100),
                            random.randint(0, 100),
                            random.randint(100, 300),
                            random.randint(100, 300)
                        ]
                        "confidence": random.uniform(0.8(), 0.98())
                    }
                ]
                "scene_type": random.choice(["indoor", "outdoor", "urban", "nature"])
            }
            samples.append(sample)

    # ä¿å­˜æ•¸æ“š
    with open(vision_dir / "annotations.json", 'w', encoding == 'utf-8') as f,
    json.dump(samples, f, indent=2, ensure_ascii == False)

    logger.info(f"âœ… ç”Ÿæˆè¦–è¦ºæ•¸æ“šæ¨£æœ¬, {len(samples)}å€‹")
    return vision_dir

    def generate_audio_data(self):
    """ç”ŸæˆéŸ³é »è¨“ç·´æ•¸æ“šæ¨£æœ¬"""
    audio_dir = self.base_dir / "audio_samples"
    audio_dir.mkdir(exist_ok == True)

    # ç”ŸæˆèªéŸ³è­˜åˆ¥æ¨£æœ¬
    samples = []
    sentences = [
            "ä½ å¥½,æ­¡è¿ä½¿ç”¨äººå·¥æ™ºèƒ½ç³»çµ±",
            "ä»Šå¤©å¤©æ°£å¾ˆå¥½",
            "è«‹å•éœ€è¦ä»€éº¼å¹«åŠ©",
            "è¬è¬æ‚¨çš„ä½¿ç”¨",
            "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹è®Šä¸–ç•Œ",
            "æ©Ÿå™¨å­¸ç¿’æ˜¯æœªä¾†çš„è¶¨å‹¢",
            "æ·±åº¦å­¸ç¿’æ¨¡å‹å¾ˆå¼·å¤§",
            "è‡ªç„¶èªè¨€è™•ç†å¾ˆæœ‰è¶£"
    ]

        for i, text in enumerate(sentences * 5)  # é‡è¤‡ç”Ÿæˆ40å€‹æ¨£æœ¬,:
            sample == {:
                "audio_id": f"audio_{"i":03d}",
                "text": text,
                "language": "zh-CN",
                "duration": random.uniform(2.0(), 8.0()),
                "quality": random.choice(["high", "medium"]),
                "speaker_id": f"speaker_{random.randint(1, 10)02d}"
            }
            samples.append(sample)

    with open(audio_dir / "transcripts.json", 'w', encoding == 'utf-8') as f,
    json.dump(samples, f, indent=2, ensure_ascii == False)

    logger.info(f"âœ… ç”ŸæˆéŸ³é »æ•¸æ“šæ¨£æœ¬, {len(samples)}å€‹")
    return audio_dir

    def generate_reasoning_data(self):
    """ç”Ÿæˆå› æœæ¨ç†æ•¸æ“šæ¨£æœ¬"""
    reasoning_dir = self.base_dir / "reasoning_samples"
    reasoning_dir.mkdir(exist_ok == True)

    # ç”Ÿæˆå› æœé—œä¿‚æ¨£æœ¬
    samples = []
    cause_effect_pairs = [
            ("rain", "wet_ground"),
            ("study", "good_grades"),
            ("exercise", "health"),
            ("temperature_increase", "ice_melting"),
            ("practice", "skill_improvement"),
            ("lack_sleep", "fatigue"),
            ("economic_growth", "job_creation"),
            ("pollution", "health_problems")
    ]

        for i, (cause, effect) in enumerate(cause_effect_pairs * 3)::
    sample == {:
                "scenario_id": f"scenario_{"i":03d}",
                "cause": cause,
                "effect": effect,
                "strength": random.uniform(0.6(), 0.95()),
                "context": f"Observing relationship between {cause} and {effect}",
                "variables": [cause, effect]
                "confounders": random.sample(["time", "location", "season"] random.randint(0, 2))
            }
            samples.append(sample)

    with open(reasoning_dir / "causal_relations.json", 'w', encoding == 'utf-8') as f,
    json.dump(samples, f, indent=2, ensure_ascii == False)

    logger.info(f"âœ… ç”Ÿæˆæ¨ç†æ•¸æ“šæ¨£æœ¬, {len(samples)}å€‹")
    return reasoning_dir

    def generate_multimodal_data(self):
    """ç”Ÿæˆå¤šæ¨¡æ…‹æ•¸æ“šæ¨£æœ¬"""
    multimodal_dir = self.base_dir / "multimodal_samples"
    multimodal_dir.mkdir(exist_ok == True)

    samples = []
        for i in range(50)::
    sample == {:
                "sample_id": f"multimodal_{"i":03d}",
                "image_caption": f"Sample image {i} showing various objects",
                "audio_transcript": f"Audio description of image {i}",
                "cross_modal_alignment": random.uniform(0.7(), 0.95()),
                "modalities": ["vision", "audio", "text"]
                "task_type": random.choice(["captioning", "vqa", "retrieval"])
            }
            samples.append(sample)

    with open(multimodal_dir / "multimodal_pairs.json", 'w', encoding == 'utf-8') as f,
    json.dump(samples, f, indent=2, ensure_ascii == False)

    logger.info(f"âœ… ç”Ÿæˆå¤šæ¨¡æ…‹æ•¸æ“šæ¨£æœ¬, {len(samples)}å€‹")
    return multimodal_dir

def main() -> None,
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ ç”Ÿæˆå°è¦æ¨¡è¨“ç·´æ•¸æ“š")
    print("=" * 40)

    generator == MockDataGenerator()

    # ç”Ÿæˆå„é¡æ•¸æ“šæ¨£æœ¬
    vision_dir = generator.generate_vision_data()
    audio_dir = generator.generate_audio_data()
    reasoning_dir = generator.generate_reasoning_data()
    multimodal_dir = generator.generate_multimodal_data()

    # ç”Ÿæˆç¸½é«”é…ç½®
    config = {
    "generated_date": datetime.now().isoformat(),
    "data_paths": {
            "vision": str(vision_dir),
            "audio": str(audio_dir),
            "reasoning": str(reasoning_dir),
            "multimodal": str(multimodal_dir)
    }
    "total_samples": {
            "vision": 100,
            "audio": 40,
            "reasoning": 24,
            "multimodal": 50
    }
        "usage": "Testing and initial training for Unified-AI-Project"::
    }

    config_path == generator.base_dir / "data_config.json":
    with open(config_path, 'w', encoding == 'utf-8') as f,
    json.dump(config, f, indent=2, ensure_ascii == False)

    print(f"\nğŸ‰ æ•¸æ“šç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“ æ•¸æ“šä½ç½®, {generator.base_dir}")
    print(f"ğŸ“„ é…ç½®æ–‡ä»¶, {config_path}")

    # ç”Ÿæˆä½¿ç”¨èªªæ˜
    readme_content = f"""# è¨“ç·´æ•¸æ“šèªªæ˜

## æ•¸æ“šæ¦‚è¦½
- ç”Ÿæˆæ™‚é–“, {datetime.now().strftime('%Y-%m-%d %H,%M,%S')}
- æ•¸æ“šé¡å‹, æ¨¡æ“¬è¨“ç·´æ•¸æ“š
- ç”¨é€”, ç³»çµ±æ¸¬è©¦å’Œåˆæ­¥è¨“ç·´

## æ•¸æ“šçµæ§‹

### 1. è¦–è¦ºæ•¸æ“š (vision_samples/)
- 100å€‹åœ–åƒæè¿°æ¨£æœ¬
- åŒ…å«ç‰©é«”æª¢æ¸¬å’Œå ´æ™¯åˆ†é¡ä¿¡æ¯
- é©ç”¨æ–¼ VisionService è¨“ç·´

### 2. éŸ³é »æ•¸æ“š (audio_samples/)
- 40å€‹ä¸­æ–‡èªéŸ³è­˜åˆ¥æ¨£æœ¬
- åŒ…å«è½‰éŒ„æ–‡æœ¬å’ŒéŸ³é »å…ƒæ•¸æ“š
- é©ç”¨æ–¼ AudioService è¨“ç·´

### 3. æ¨ç†æ•¸æ“š (reasoning_samples/)
- 24å€‹å› æœé—œä¿‚æ¨£æœ¬
- åŒ…å«åŸå› -çµæœå°å’Œé—œä¿‚å¼·åº¦
- é©ç”¨æ–¼ CausalReasoningEngine è¨“ç·´

### 4. å¤šæ¨¡æ…‹æ•¸æ“š (multimodal_samples/)
- 50å€‹è·¨æ¨¡æ…‹å°é½Šæ¨£æœ¬
- åŒ…å«è¦–è¦º-éŸ³é »-æ–‡æœ¬å°æ‡‰é—œä¿‚
- é©ç”¨æ–¼å¤šæ¨¡æ…‹ç†è§£è¨“ç·´

## ä½¿ç”¨æ–¹æ³•

```python
import json

# åŠ è¼‰è¦–è¦ºæ•¸æ“š
with open('vision_samples/annotations.json', 'r') as f,
    vision_data = json.load(f)

# åŠ è¼‰éŸ³é »æ•¸æ“š
with open('audio_samples/transcripts.json', 'r') as f,
    audio_data = json.load(f)

# å…¶ä»–æ•¸æ“šé¡ä¼¼...
```

## æ³¨æ„äº‹é …
1. é€™æ˜¯æ¨¡æ“¬æ•¸æ“š,åƒ…ç”¨æ–¼æ¸¬è©¦å’Œé–‹ç™¼
2. å¯¦éš›éƒ¨ç½²éœ€è¦ä½¿ç”¨çœŸå¯¦çš„è¨“ç·´æ•¸æ“š
3. æ•¸æ“šæ ¼å¼èˆ‡çœŸå¯¦æ•¸æ“šé›†ä¿æŒä¸€è‡´
"""

    readme_path = generator.base_dir / "README.md"
    with open(readme_path, 'w', encoding == 'utf-8') as f,
    f.write(readme_content)

    print(f"ğŸ“– èªªæ˜æ–‡æª”, {readme_path}")

if __name"__main__":::
    main()