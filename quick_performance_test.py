#!/usr/bin/env python3
"""
å¿«é€Ÿæ€§èƒ½æµ‹è¯•
è¿è¡Œæ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡çš„ç®€åŒ–æµ‹è¯•
"""

import asyncio
import time
import json
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
sys.path.append('apps/backend/src')
sys.path.append('.')

from unified_scheduler_framework import (
    create_unified_scheduler, create_parallel_scheduler, TaskConfig
)


class QuickPerformanceTester:
    """å¿«é€Ÿæ€§èƒ½æµ‹è¯•å™¨"""
    
    async def run_quick_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œå¿«é€Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹AGIç³»ç»Ÿå¿«é€Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        benchmark_start = time.time()
        results = {
            "test_name": "AGIç³»ç»Ÿå¿«é€Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•",
            "start_time": datetime.now().isoformat(),
            "benchmarks": {},
            "overall_score": 0.0
        }
        
        try:
            # 1. å“åº”æ—¶é—´æµ‹è¯•
            print("\nğŸ“Š 1. å“åº”æ—¶é—´æµ‹è¯•")
            results["benchmarks"]["response_time"] = await self._quick_response_time_test()
            
            # 2. ååé‡æµ‹è¯•
            print("\nğŸ“ˆ 2. ååé‡æµ‹è¯•")
            results["benchmarks"]["throughput"] = await self._quick_throughput_test()
            
            # 3. å¹¶å‘æµ‹è¯•
            print("\nâš¡ 3. å¹¶å‘æµ‹è¯•")
            results["benchmarks"]["concurrency"] = await self._quick_concurrency_test()
            
            # è®¡ç®—æ€»ä½“è¯„åˆ†
            benchmark_end = time.time()
            results["total_execution_time"] = benchmark_end - benchmark_start
            results["overall_score"] = self._calculate_overall_score(results["benchmarks"])
            
            print(f"\nâœ… å¿«é€Ÿæ€§èƒ½æµ‹è¯•å®Œæˆ")
            print(f"æ€»æ‰§è¡Œæ—¶é—´: {results['total_execution_time']:.2f}ç§’")
            print(f"æ€»ä½“æ€§èƒ½è¯„åˆ†: {results['overall_score']:.1f}/10.0")
            
            return results
            
        except Exception as e:
            print(f"\nâŒ å¿«é€Ÿæ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            results["error"] = str(e)
            return results
    
    async def _quick_response_time_test(self) -> Dict[str, Any]:
        """å¿«é€Ÿå“åº”æ—¶é—´æµ‹è¯•"""
        print("  æµ‹è¯•åŸºæœ¬å“åº”æ—¶é—´...")
        
        scheduler = create_unified_scheduler()
        
        # ç®€å•ä»»åŠ¡æµ‹è¯•
        task_config = TaskConfig(
            name="quick_response_test",
            command="python -c \"print('Quick test')\"",
            timeout=10
        )
        scheduler.register_task(task_config)
        
        # æµ‹è¯•3æ¬¡å–å¹³å‡
        response_times = []
        for i in range(3):
            start_time = time.time()
            result = await scheduler.execute_task(task_config.name)
            end_time = time.time()
            
            if result.status.value == "completed":
                response_time = end_time - start_time
                response_times.append(response_time)
        
        if response_times:
            avg_response_time = sum(response_times) / len(response_times)
            performance_score = max(0, 10 - avg_response_time * 10)  # å“åº”æ—¶é—´è¶ŠçŸ­åˆ†æ•°è¶Šé«˜
            
            return {
                "success": True,
                "average_response_time": avg_response_time,
                "max_response_time": max(response_times),
                "min_response_time": min(response_times),
                "performance_score": performance_score,
                "meets_requirement": avg_response_time < 2.0  # 2ç§’è¦æ±‚
            }
        else:
            return {
                "success": False,
                "error": "æ‰€æœ‰å“åº”æ—¶é—´æµ‹è¯•å¤±è´¥"
            }
    
    async def _quick_throughput_test(self) -> Dict[str, Any]:
        """å¿«é€Ÿååé‡æµ‹è¯•"""
        print("  æµ‹è¯•åŸºæœ¬ååé‡...")
        
        scheduler = create_parallel_scheduler(max_concurrent_tasks=3)
        
        # åˆ›å»º5ä¸ªç®€å•å¹¶å‘ä»»åŠ¡
        task_configs = []
        for i in range(5):
            task_config = TaskConfig(
                name=f"quick_throughput_{i}",
                command=f"python -c \"print('Task {i}')\"",
                timeout=10
            )
            scheduler.register_task(task_config)
            task_configs.append(task_config)
        
        start_time = time.time()
        task_names = [tc.name for tc in task_configs]
        results = await scheduler.execute_tasks(task_names)
        end_time = time.time()
        
        total_time = end_time - start_time
        successful_tasks = sum(1 for r in results if r.status.value == "completed")
        throughput = successful_tasks / total_time if total_time > 0 else 0
        
        performance_score = min(10, throughput * 5)  # åŸºç¡€ååé‡è¯„åˆ†
        
        return {
            "success": successful_tasks > 0,
            "total_tasks": len(results),
            "successful_tasks": successful_tasks,
            "total_execution_time": total_time,
            "throughput_per_second": throughput,
            "performance_score": performance_score,
            "meets_requirement": throughput > 1.0  # æ¯ç§’è‡³å°‘1ä¸ªä»»åŠ¡
        }
    
    async def _quick_concurrency_test(self) -> Dict[str, Any]:
        """å¿«é€Ÿå¹¶å‘æµ‹è¯•"""
        print("  æµ‹è¯•åŸºæœ¬å¹¶å‘èƒ½åŠ›...")
        
        # æµ‹è¯•å¹¶å‘çº§åˆ«2
        scheduler = create_parallel_scheduler(max_concurrent_tasks=2)
        
        task_configs = []
        for i in range(4):
            task_config = TaskConfig(
                name=f"quick_concurrent_{i}",
                command=f"python -c \"import time; time.sleep(0.05); print('Task {i}')\"",
                timeout=10
            )
            scheduler.register_task(task_config)
            task_configs.append(task_config)
        
        start_time = time.time()
        task_names = [tc.name for tc in task_configs]
        results = await scheduler.execute_tasks(task_names)
        end_time = time.time()
        
        successful_tasks = sum(1 for r in results if r.status.value == "completed")
        total_time = end_time - start_time
        
        # å¹¶å‘æ•ˆç‡è®¡ç®—
        sequential_time_estimate = 4 * 0.05  # 4ä¸ªä»»åŠ¡ï¼Œæ¯ä¸ª0.05ç§’
        actual_time = total_time
        efficiency = (sequential_time_estimate / actual_time) * 100 if actual_time > 0 else 0
        
        performance_score = min(10, efficiency / 10)  # æ•ˆç‡è½¬æ¢ä¸ºåˆ†æ•°
        
        return {
            "success": successful_tasks > 0,
            "total_tasks": len(results),
            "successful_tasks": successful_tasks,
            "total_execution_time": total_time,
            "concurrent_efficiency": efficiency,
            "performance_score": performance_score,
            "meets_requirement": efficiency > 50  # 50%æ•ˆç‡è¦æ±‚
        }
    
    def _calculate_overall_score(self, benchmarks: Dict[str, Any]) -> float:
        """è®¡ç®—æ€»ä½“æ€§èƒ½è¯„åˆ†"""
        scores = []
        
        for benchmark_result in benchmarks.values():
            if benchmark_result.get("success", False):
                if "performance_score" in benchmark_result:
                    scores.append(benchmark_result["performance_score"])
                elif "concurrent_efficiency" in benchmark_result:
                    scores.append(benchmark_result["concurrent_efficiency"] / 10)  # è½¬æ¢ä¸º10åˆ†åˆ¶
        
        return sum(scores) / len(scores) if scores else 0.0


def create_quick_performance_tester() -> QuickPerformanceTester:
    """åˆ›å»ºå¿«é€Ÿæ€§èƒ½æµ‹è¯•å™¨"""
    return QuickPerformanceTester()


async def run_quick_performance_benchmark():
    """è¿è¡Œå¿«é€Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    tester = create_quick_performance_tester()
    
    try:
        results = await tester.run_quick_benchmark()
        
        # ç”Ÿæˆç®€åŒ–æŠ¥å‘Š
        report = generate_quick_performance_report(results)
        print("\n" + "="*50)
        print(report)
        print("="*50)
        
        # ä¿å­˜ç»“æœ
        results_file = "quick_performance_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“Š å¿«é€Ÿæ€§èƒ½æ•°æ®å·²ä¿å­˜åˆ°: {results_file}")
        
        return results["overall_score"] >= 6.0  # 6åˆ†ä»¥ä¸Šè®¤ä¸ºæ€§èƒ½åŸºæœ¬åˆæ ¼
        
    except Exception as e:
        print(f"\nâŒ å¿«é€Ÿæ€§èƒ½æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return False


def generate_quick_performance_report(results: Dict[str, Any]) -> str:
    """ç”Ÿæˆå¿«é€Ÿæ€§èƒ½æŠ¥å‘Š"""
    report = []
    report.append("AGIç³»ç»Ÿå¿«é€Ÿæ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
    report.append("=" * 40)
    report.append(f"æµ‹è¯•æ—¶é—´: {results.get('start_time', 'Unknown')}")
    report.append(f"æ€»æ‰§è¡Œæ—¶é—´: {results.get('total_execution_time', 0):.2f}ç§’")
    report.append(f"æ€»ä½“æ€§èƒ½è¯„åˆ†: {results.get('overall_score', 0):.1f}/10.0")
    report.append("")
    
    benchmarks = results.get("benchmarks", {})
    
    for benchmark_name, benchmark_result in benchmarks.items():
        if benchmark_result.get("success", False):
            if benchmark_name == "response_time":
                report.append(f"ğŸ“Š å“åº”æ—¶é—´: {benchmark_result.get('average_response_time', 0):.3f}ç§’")
                report.append(f"   è¯„åˆ†: {benchmark_result.get('performance_score', 0):.1f}/10.0")
            
            elif benchmark_name == "throughput":
                report.append(f"ğŸ“ˆ ååé‡: {benchmark_result.get('throughput_per_second', 0):.2f} ä»»åŠ¡/ç§’")
                report.append(f"   è¯„åˆ†: {benchmark_result.get('performance_score', 0):.1f}/10.0")
            
            elif benchmark_name == "concurrency":
                report.append(f"âš¡ å¹¶å‘æ•ˆç‡: {benchmark_result.get('concurrent_efficiency', 0):.1f}%")
                report.append(f"   è¯„åˆ†: {benchmark_result.get('performance_score', 0):.1f}/10.0")
        else:
            report.append(f"âŒ {benchmark_name}: æµ‹è¯•å¤±è´¥")
    
    overall_score = results.get("overall_score", 0)
    if overall_score >= 8.0:
        report.append(f"\nğŸ¯ æ€§èƒ½è¯„ä»·: ä¼˜ç§€ ({overall_score:.1f}/10.0)")
    elif overall_score >= 6.0:
        report.append(f"\nğŸ¯ æ€§èƒ½è¯„ä»·: è‰¯å¥½ ({overall_score:.1f}/10.0)")
    elif overall_score >= 4.0:
        report.append(f"\nğŸ¯ æ€§èƒ½è¯„ä»·: ä¸€èˆ¬ ({overall_score:.1f}/10.0)")
    else:
        report.append(f"\nğŸ¯ æ€§èƒ½è¯„ä»·: éœ€è¦ä¼˜åŒ– ({overall_score:.1f}/10.0)")
    
    return "\n".join(report)


if __name__ == '__main__':
    import asyncio
    
    print("ğŸš€ å¯åŠ¨AGIç³»ç»Ÿå¿«é€Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
    success = asyncio.run(run_quick_performance_benchmark())
    
    if success:
        print("\nğŸ‰ å¿«é€Ÿæ€§èƒ½æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿæ€§èƒ½åŸºæœ¬è¾¾æ ‡")
        exit(0)
    else:
        print("\nâŒ å¿«é€Ÿæ€§èƒ½æµ‹è¯•æœªé€šè¿‡ï¼Œå»ºè®®æ£€æŸ¥ç³»ç»Ÿé…ç½®")
        exit(1)
