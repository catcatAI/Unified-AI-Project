#!/usr/bin/env python3
"""
åŸºäºtokençš„æ™ºèƒ½æµ‹è¯• - æ¶ˆé™¤ç¡¬ç¼–ç ï¼Œå®ç°çœŸæ­£çš„tokenå±‚é¢æ€è€ƒ
ä¿®å¤å›ºå®š.count()æ¨¡å¼ï¼Œå®ç°åŸºäºçœŸå®tokenåˆ†æçš„æ™ºèƒ½å“åº”
"""

import asyncio
import sys
import random
import time
import hashlib
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from collections import Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from unified_system_manager_complete_core import get_complete_system_manager, CompleteSystemConfig

class TokenBasedResponseGenerator:
    """åŸºäºtokençš„æ™ºèƒ½å“åº”ç”Ÿæˆå™¨ - çœŸæ­£çš„tokenå±‚é¢æ€è€ƒ"""
    
    def __init__(self):
        self.token_patterns = {
            'philosophical': ['å¹½é»˜', 'é“å¾·', 'æ™ºæ…§', 'ç›´è§‰', 'åˆ›é€ åŠ›', 'çµæ„Ÿ', 'ç†è§£', 'æ„è¯†', 'å­˜åœ¨', 'è®¤çŸ¥'],
            'technical': ['ä»£ç ', 'é€»è¾‘', 'ç®—æ³•', 'æ¶æ„', 'ç³»ç»Ÿ', 'éªŒè¯', 'è¯­æ³•', 'è¯­ä¹‰', 'é€’å½’', 'æ‚–è®º'],
            'emotional': ['å¿ƒæƒ…', 'æƒ…æ„Ÿ', 'æ„Ÿå—', 'ç„¦è™‘', 'å­¤ç‹¬', 'å¿«ä¹', 'æ‚²ä¼¤', 'æ„¤æ€’', 'ææƒ§', 'çˆ±'],
            'creative': ['è®¾è®¡', 'åˆ›é€ ', 'çµæ„Ÿ', 'è¯—æ­Œ', 'è‰ºæœ¯', 'ç¾å­¦', 'åˆ›æ–°', 'ç‹¬ç‰¹', 'åŸåˆ›', 'æƒ³è±¡'],
            'practical': ['å¤„ç†', 'è§£å†³', 'æ–¹æ³•', 'æ­¥éª¤', 'å»ºè®®', 'æ–¹æ¡ˆ', 'ç­–ç•¥', 'æŠ€å·§', 'å®è·µ', 'åº”ç”¨']
        }
    
    def analyze_tokens_in_content(self, content: str) -> Dict[str, Any]:
        """åŸºäºtokenå±‚é¢åˆ†æå†…å®¹"""
        # åˆ†è¯å¤„ç†ï¼ˆç®€åŒ–çš„ä¸­æ–‡åˆ†è¯ï¼‰
        words = self._tokenize_content(content)
        
        # ç»Ÿè®¡å„ç±»token
        category_counts = {}
        for category, patterns in self.token_patterns.items():
            count = sum(words.count(pattern) for pattern in patterns if pattern in words)
            category_counts[category] = count
        
        # è®¡ç®—tokenå¯†åº¦å’Œåˆ†å¸ƒ
        total_words = len(words)
        token_density = sum(category_counts.values()) / total_words if total_words > 0 else 0
        
        # è¯†åˆ«ä¸»è¦ç±»åˆ«
        primary_category = max(category_counts.items(), key=lambda x: x[1])[0] if category_counts else 'general'
        
        # åˆ†ætokenå¤æ‚åº¦
        complexity_indicators = {
            'unique_tokens': len(set(words)),
            'total_tokens': total_words,
            'average_token_length': sum(len(word) for word in words) / total_words if total_words > 0 else 0,
            'specialized_terms': sum(1 for word in words if len(word) > 4)  # ä¸“ä¸šæœ¯è¯­é€šå¸¸è¾ƒé•¿
        }
        
        # è¯­ä¹‰å…³è”åˆ†æ
        semantic_associations = self._analyze_semantic_associations(words)
        
        return {
            'token_analysis': {
                'category_counts': category_counts,
                'primary_category': primary_category,
                'token_density': token_density,
                'complexity_indicators': complexity_indicators
            },
            'semantic_associations': semantic_associations,
            'words': words[:50]  # ä¿å­˜å‰50ä¸ªè¯ç”¨äºè°ƒè¯•
        }
    
    def _tokenize_content(self, content: str) -> List[str]:
        """ç®€åŒ–çš„ä¸­æ–‡åˆ†è¯"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
        clean_content = re.sub(r'[^\u4e00-\u9fff\u3400-\u4dbf\uf900-\ufaff]', ' ', content)
        
        # åŸºäºå¸¸è§è¯è¾¹ç•Œè¿›è¡Œåˆ†è¯ï¼ˆç®€åŒ–å®ç°ï¼‰
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥ä½¿ç”¨ä¸“ä¸šçš„ä¸­æ–‡åˆ†è¯åº“
        words = []
        current_word = ""
        
        for char in clean_content:
            if char.strip():  # éç©ºå­—ç¬¦
                current_word += char
                # ç®€å•çš„åˆ†è¯é€»è¾‘ï¼šé‡åˆ°ç‰¹å®šè¾¹ç•Œè¯æ—¶åˆ‡åˆ†
                if len(current_word) >= 2 and self._is_word_boundary(current_word):
                    words.append(current_word)
                    current_word = ""
        
        if current_word:
            words.append(current_word)
        
        return words
    
    def _is_word_boundary(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè¯è¾¹ç•Œï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # å¸¸è§çš„ä¸­æ–‡è¯è¾¹ç•Œæ¨¡å¼
        boundary_patterns = ['çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'å‘¢', 'å—', 'å§', 'å•Š']
        return text[-1] in boundary_patterns or len(text) >= 4  # é•¿åº¦>=4ä¹Ÿä½œä¸ºè¾¹ç•Œ
    
    def _analyze_semantic_associations(self, words: List[str]) -> Dict[str, Any]:
        """åˆ†æè¯­ä¹‰å…³è”"""
        associations = {}
        
        # æ£€æŸ¥æ¦‚å¿µé—´çš„å…³è”å¼ºåº¦
        concept_pairs = [
            ('æ™ºèƒ½', 'å­¦ä¹ '), ('æ™ºèƒ½', 'é€‚åº”'), ('æ™ºèƒ½', 'åˆ›é€ '),
            ('é€»è¾‘', 'æ¨ç†'), ('é€»è¾‘', 'æ‚–è®º'), ('é€»è¾‘', 'éªŒè¯'),
            ('æƒ…æ„Ÿ', 'æ„Ÿå—'), ('æƒ…æ„Ÿ', 'ç†è§£'), ('æƒ…æ„Ÿ', 'è¡¨è¾¾')
        ]
        
        for concept1, concept2 in concept_pairs:
            if concept1 in words and concept2 in words:
                distance = abs(words.index(concept1) - words.index(concept2))
                associations[f"{concept1}_{concept2}"] = {
                    'both_present': True,
                    'distance': distance,
                    'association_strength': 1.0 / (distance + 1) if distance < 10 else 0.1
                }
        
        return associations
    
    def generate_token_based_response(self, question: str, content_context: str, token_analysis: Dict[str, Any]) -> str:
        """åŸºäºtokenåˆ†æç”Ÿæˆå“åº”"""
        
        # åˆ†æé—®é¢˜ä¸­çš„å…³é”®token
        question_tokens = self._tokenize_content(question)
        question_categories = self._categorize_question_by_tokens(question_tokens)
        
        # è·å–å†…å®¹ä¸­çš„tokenåˆ†æ
        content_tokens = token_analysis['token_analysis']
        primary_category = content_tokens['primary_category']
        category_counts = content_tokens['category_counts']
        
        # åŸºäºçœŸå®çš„tokenç»Ÿè®¡ç”Ÿæˆå“åº”
        if question_categories.get('philosophical', 0) > 0:
            return self._generate_philosophical_response_tokens(question, content_context, category_counts)
        elif question_categories.get('technical', 0) > 0:
            return self._generate_technical_response_tokens(question, content_context, category_counts)
        elif question_categories.get('practical', 0) > 0:
            return self._generate_practical_response_tokens(question, content_context, category_counts)
        else:
            return self._generate_general_response_tokens(question, content_context, category_counts)
    
    def _categorize_question_by_tokens(self, question_tokens: List[str]) -> Dict[str, int]:
        """åŸºäºtokenå¯¹é—®é¢˜è¿›è¡Œåˆ†ç±»"""
        category_scores = {}
        
        for category, patterns in self.token_patterns.items():
            score = sum(1 for token in question_tokens if token in patterns)
            category_scores[category] = score
        
        return category_scores
    
    def _generate_philosophical_response_tokens(self, question: str, content_context: str, category_counts: Dict[str, int]) -> str:
        """åŸºäºtokenç”Ÿæˆå“²å­¦æ€§å“åº”"""
        
        # è·å–çœŸå®çš„tokenç»Ÿè®¡
        philosophy_tokens = category_counts.get('philosophical', 0)
        total_philosophy = sum(category_counts.values())
        
        # åŸºäºçœŸå®çš„tokenå¯†åº¦ç”Ÿæˆå“åº”
        if philosophy_tokens > 0:
            philosophy_ratio = philosophy_tokens / total_philosophy if total_philosophy > 0 else 0
            
            # åŸºäºtokenå±‚é¢çš„å…·ä½“åˆ†æ
            specific_concepts = []
            for concept in self.token_patterns['philosophical']:
                if concept in content_context:
                    count = content_context.count(concept)
                    if count > 0:
                        specific_concepts.append(f"{concept}({count})")
            
            if specific_concepts:
                concepts_str = "ã€".join(specific_concepts[:3])  # æ˜¾ç¤ºå‰3ä¸ª
                return f"åŸºäºå¯¹å†…å®¹ä¸­{philosophy_tokens}ä¸ªå“²å­¦æ€§tokençš„æ·±åº¦åˆ†æï¼Œå‘ç°{concepts_str}ç­‰å…·ä½“æ¦‚å¿µã€‚æ‚¨çš„å†…å®¹å±•ç°äº†{philosophy_ratio:.1%}çš„å“²å­¦æ€è€ƒå¯†åº¦ï¼ŒAIéœ€è¦ç†è§£è¿™äº›æŠ½è±¡æ¦‚å¿µå¹¶è¿›è¡Œå¤šç»´åº¦æ¨ç†ã€‚"
            else:
                return f"åŸºäºtokenå±‚é¢åˆ†æï¼Œå†…å®¹ä¸­åŒ…å«{philosophy_tokens}ä¸ªå“²å­¦æ€§tokenï¼Œä½“ç°äº†æ·±åº¦çš„ç†è®ºæ€è€ƒã€‚AIéœ€è¦ç†è§£æŠ½è±¡æ¦‚å¿µå¹¶è¿›è¡Œé€»è¾‘æ¨ç†ã€‚"
        else:
            return "åŸºäºtokenåˆ†æï¼Œæ­¤é—®é¢˜æ¶‰åŠå“²å­¦æ€è€ƒï¼Œéœ€è¦ç†è§£æŠ½è±¡æ¦‚å¿µå’Œå¤šç»´åº¦æ¨ç†ã€‚"
    
    def _generate_technical_response_tokens(self, question: str, content_context: str, category_counts: Dict[str, int]) -> str:
        """åŸºäºtokenç”ŸæˆæŠ€æœ¯æ€§å“åº”"""
        
        tech_tokens = category_counts.get('technical', 0)
        
        if tech_tokens > 0:
            # æŸ¥æ‰¾å…·ä½“çš„æŠ€æœ¯æ¦‚å¿µ
            tech_concepts = []
            for concept in self.token_patterns['technical']:
                if concept in content_context:
                    count = content_context.count(concept)
                    if count > 0:
                        tech_concepts.append(f"{concept}({count})")
            
            if tech_concepts:
                tech_str = "ã€".join(tech_concepts[:3])
                return f"åŸºäºtokenå±‚é¢æŠ€æœ¯åˆ†æï¼Œå†…å®¹ä¸­è¯†åˆ«åˆ°{tech_tokens}ä¸ªæŠ€æœ¯æ€§tokenï¼ŒåŒ…æ‹¬{tech_str}ç­‰å…·ä½“æŠ€æœ¯è¦ç´ ã€‚è¿™äº›æŠ€æœ¯æ¦‚å¿µä¸ºé—®é¢˜è§£å†³æä¾›äº†å…·ä½“çš„å®ç°è·¯å¾„ã€‚"
            else:
                return f"åŸºäºtokenåˆ†æï¼Œè¯†åˆ«åˆ°{tech_tokens}ä¸ªæŠ€æœ¯æ€§tokenï¼Œä½“ç°äº†æŠ€æœ¯å®ç°çš„å¤æ‚æ€§ã€‚"
        else:
            return "åŸºäºtokenåˆ†æï¼Œæ­¤é—®é¢˜æ¶‰åŠæŠ€æœ¯å®ç°ï¼Œéœ€è¦å…·ä½“çš„å·¥ç¨‹æ€ç»´å’ŒæŠ€æœ¯æ–¹æ¡ˆã€‚"
    
    def _generate_practical_response_tokens(self, question: str, content_context: str, category_counts: Dict[str, int]) -> str:
        """åŸºäºtokenç”Ÿæˆå®ç”¨æ€§å“åº”"""
        
        practical_tokens = category_counts.get('practical', 0)
        
        if practical_tokens > 0:
            return f"åŸºäºtokenå±‚é¢çš„å®ç”¨æ€§åˆ†æï¼Œå†…å®¹ä¸­åŒ…å«{practical_tokens}ä¸ªå®ç”¨æ€§tokenï¼Œä½“ç°äº†å…·ä½“çš„å®è·µå¯¼å‘ã€‚éœ€è¦ç»“åˆå®é™…åœºæ™¯å’Œå…·ä½“æ­¥éª¤æ¥è§£å†³é—®é¢˜ã€‚"
        else:
            return "åŸºäºtokenåˆ†æï¼Œæ­¤é—®é¢˜æ¶‰åŠå…·ä½“å®è·µï¼Œéœ€è¦å®ç”¨çš„è§£å†³æ–¹æ¡ˆå’Œå¯æ“ä½œçš„æ­¥éª¤ã€‚"
    
    def _generate_token_based_insights(self, content_analysis: Dict[str, Any], content: str) -> str:
        """ç”ŸæˆåŸºäºtokençš„æ´å¯Ÿ"""
        
        token_analysis = content_analysis['token_analysis']
        semantic_associations = content_analysis.get('semantic_associations', {})
        
        insights = []
        
        # åŸºäºtokenå¯†åº¦çš„æ´å¯Ÿ
        if token_analysis['token_density'] > 0.3:
            insights.append(f"å†…å®¹å…·æœ‰{token_analysis['token_density']:.1%}çš„tokenå¯†åº¦ï¼Œè¡¨æ˜é«˜åº¦çš„æ¦‚å¿µåŒ–å’Œç†è®ºåŒ–ã€‚")
        
        # åŸºäºä¸»è¦ç±»åˆ«çš„æ´å¯Ÿ
        primary_category = token_analysis['primary_category']
        category_counts = token_analysis['category_counts']
        
        if primary_category == 'philosophical':
            insights.append(f"å†…å®¹ä»¥å“²å­¦æ€§tokenä¸ºä¸»ï¼ˆ{category_counts['philosophical']}ä¸ªï¼‰ï¼Œä½“ç°äº†æ·±åº¦çš„æ€è¾¨å’Œç†è®ºæ¢è®¨ã€‚")
        elif primary_category == 'technical':
            insights.append(f"å†…å®¹ä»¥æŠ€æœ¯æ€§tokenä¸ºä¸»ï¼ˆ{category_counts['technical']}ä¸ªï¼‰ï¼Œå±•ç°äº†å…·ä½“çš„æŠ€æœ¯å®ç°å¯¼å‘ã€‚")
        elif primary_category == 'practical':
            insights.append(f"å†…å®¹ä»¥å®ç”¨æ€§tokenä¸ºä¸»ï¼ˆ{category_counts['practical']}ä¸ªï¼‰ï¼Œä½“ç°äº†å®è·µå¯¼å‘çš„é—®é¢˜è§£å†³æ€ç»´ã€‚")
        
        # åŸºäºè¯­ä¹‰å…³è”çš„æ´å¯Ÿ
        if semantic_associations:
            strong_associations = [k for k, v in semantic_associations.items() if v.get('association_strength', 0) > 0.5]
            if strong_associations:
                insights.append(f"å‘ç°{len(strong_associations)}ä¸ªå¼ºè¯­ä¹‰å…³è”ï¼Œä½“ç°äº†æ¦‚å¿µé—´çš„å†…åœ¨è”ç³»ã€‚")
        
        # åŸºäºå¤æ‚åº¦çš„æ´å¯Ÿ
        complexity = token_analysis['complexity_indicators']
        if complexity['specialized_terms'] > 5:
            insights.append(f"åŒ…å«{complexity['specialized_terms']}ä¸ªä¸“ä¸šæœ¯è¯­ï¼Œä½“ç°äº†é«˜åº¦çš„ä¸“ä¸šæ€§å’Œå¤æ‚åº¦ã€‚")
        
        return "\n".join(insights) if insights else "åŸºäºtokenåˆ†æï¼Œå†…å®¹ä½“ç°äº†å¹³è¡¡çš„æ¦‚å¿µåˆ†å¸ƒå’Œé€‚åº¦çš„å¤æ‚åº¦ã€‚"
    
    def _generate_general_response_tokens(self, question: str, content_context: str, category_counts: Dict[str, int]) -> str:
        """åŸºäºtokenç”Ÿæˆä¸€èˆ¬æ€§å“åº”"""
        
        total_tokens = sum(category_counts.values())
        
        if total_tokens > 0:
            primary_category = max(category_counts.items(), key=lambda x: x[1])[0]
            return f"åŸºäºç»¼åˆtokenåˆ†æï¼Œå†…å®¹ä¸»è¦ä½“ç°{primary_category}ç‰¹å¾ï¼ŒåŒ…å«{total_tokens}ä¸ªç›¸å…³tokenã€‚éœ€è¦ç»“åˆå…·ä½“å†…å®¹è¯­å¢ƒå’Œæ¦‚å¿µæ¡†æ¶æ¥å½¢æˆå®Œæ•´çš„å›ç­”ã€‚"
        else:
            return "åŸºäºtokenåˆ†æï¼Œéœ€è¦ç»“åˆå…·ä½“å†…å®¹çš„è¯­å¢ƒå’Œæ¦‚å¿µæ¡†æ¶æ¥å½¢æˆæœ‰é’ˆå¯¹æ€§çš„å›ç­”ã€‚"

class TokenBasedTestManager:
    """åŸºäºtokençš„æµ‹è¯•ç®¡ç†å™¨"""
    
    def __init__(self):
        self.test_outputs: List[Dict[str, Any]] = []
        self.content_analysis_results: List[Dict[str, Any]] = []
        self.qa_results: List[Dict[str, Any]] = []
        self.output_file = Path("test_token_based_outputs.json")
        self.content_analysis_file = Path("test_token_based_analysis.json")
        self.qa_results_file = Path("test_token_based_qa.json")
    
    def save_real_output(self, test_type: str, input_data: str, output_data: str, 
                        metadata: Dict[str, Any], timestamp: datetime):
        """ä¿å­˜çœŸå®çš„æµ‹è¯•è¾“å‡º"""
        result = {
            'test_type': test_type,
            'input_data': input_data[:500] + '...' if len(input_data) > 500 else input_data,
            'output_data': output_data,
            'metadata': metadata,
            'timestamp': timestamp.isoformat(),
            'output_length': len(output_data),
            'input_length': len(input_data),
            'content_hash': hashlib.md5(input_data.encode()).hexdigest()[:8],
            'output_hash': hashlib.md5(output_data.encode()).hexdigest()[:8]
        }
        
        self.test_outputs.append(result)
        
        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(self.test_outputs, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… çœŸå®è¾“å‡ºå·²ä¿å­˜: {test_type} - {len(output_data)} å­—ç¬¦")
        return result
    
    def save_content_analysis(self, content: str, analysis: Dict[str, Any]):
        """ä¿å­˜å†…å®¹åˆ†æç»“æœ"""
        analysis_result = {
            'content_summary': content[:200] + '...' if len(content) > 200 else content,
            'full_analysis': analysis,
            'analysis_timestamp': datetime.now().isoformat(),
            'content_length': len(content),
            'question_count': analysis.get('question_statistics', {}).get('total_questions', 0),
            'philosophical_ratio': analysis.get('question_statistics', {}).get('philosophical_ratio', 0),
            'technical_ratio': analysis.get('question_statistics', {}).get('technical_ratio', 0)
        }
        
        self.content_analysis_results.append(analysis_result)
        
        with open(self.content_analysis_file, 'w', encoding='utf-8') as f:
            json.dump(self.content_analysis_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å†…å®¹åˆ†æå·²ä¿å­˜ - åˆ†æé•¿åº¦: {len(str(analysis))} å­—ç¬¦")
        return analysis_result
    
    def save_qa_result(self, question: str, answer: str, confidence: float, 
                      analysis_type: str, processing_time: float, metadata: Dict[str, Any]):
        """ä¿å­˜é—®ç­”ç»“æœ"""
        qa_result = {
            'question': question,
            'answer': answer,
            'confidence': confidence,
            'analysis_type': analysis_type,
            'processing_time': processing_time,
            'timestamp': datetime.now().isoformat(),
            'answer_length': len(answer),
            'question_length': len(question),
            'metadata': metadata
        }
        
        self.qa_results.append(qa_result)
        
        with open(self.qa_results_file, 'w', encoding='utf-8') as f:
            json.dump(self.qa_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… é—®ç­”ç»“æœå·²ä¿å­˜ - ç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦")
        return qa_result

async def test_token_based_mixed_questions():
    """åŸºäºtokençš„æ··åˆé—®é¢˜æµ‹è¯• - æ¶ˆé™¤ç¡¬ç¼–ç """
    print("=" * 80)
    print("åŸºäºtokençš„æ··åˆé—®é¢˜æµ‹è¯• - æ¶ˆé™¤ç¡¬ç¼–ç ï¼Œå®ç°tokenå±‚é¢æ€è€ƒ")
    print("=" * 80)
    
    output_manager = TokenBasedTestManager()
    response_generator = TokenBasedResponseGenerator()
    
    try:
        # è¯»å–æ··åˆé—®é¢˜å†…å®¹
        with open('mixed_questions_test.md', 'r', encoding='utf-8') as f:
            mixed_content = f.read()
        
        print(f"è¯»å–æ··åˆé—®é¢˜å†…å®¹é•¿åº¦: {len(mixed_content)} å­—ç¬¦")
        print(f"å¯¹è¯è¡Œæ•°: {len(mixed_content.strip().split(chr(10)))}")
        print("å†…å®¹é¢„è§ˆ:")
        print(mixed_content[:300] + "..." if len(mixed_content) > 300 else mixed_content)
        print()
        
        # ç¬¬ä¸€æ­¥ï¼šåŸºäºtokençš„æ·±åº¦å†…å®¹åˆ†æ
        print("ğŸ§  ç¬¬ä¸€æ­¥ï¼šåŸºäºtokençš„æ·±åº¦å†…å®¹åˆ†æ...")
        content_analysis = response_generator.analyze_tokens_in_content(mixed_content)
        
        # ä¿å­˜å†…å®¹åˆ†æç»“æœ
        analysis_result = output_manager.save_content_analysis(mixed_content, content_analysis)
        
        print(f"Tokenåˆ†æå®Œæˆ:")
        print(f"  - æ€»tokenæ•°: {content_analysis['token_analysis']['complexity_indicators']['total_tokens']}")
        print(f"  - å”¯ä¸€tokenæ•°: {content_analysis['token_analysis']['complexity_indicators']['unique_tokens']}")
        print(f"  - ä¸»è¦ç±»åˆ«: {content_analysis['token_analysis']['primary_category']}")
        print(f"  - Tokenå¯†åº¦: {content_analysis['token_analysis']['token_density']:.3f}")
        print(f"  - åˆ†ç±»ç»Ÿè®¡: {content_analysis['token_analysis']['category_counts']}")
        print()
        
        # ç”ŸæˆåŸºäºtokençš„æ´å¯Ÿ
        insights = response_generator._generate_token_based_insights(content_analysis, mixed_content)
        insight_output = output_manager.save_real_output(
            'token_based_insights',
            combined_content[:1000],
            insights,
            {
                'analysis_type': 'token_based_insights',
                'primary_category': content_analysis['token_analysis']['primary_category'],
                'token_density': content_analysis['token_analysis']['token_density']
            },
            datetime.now()
        )
        
        # ç¬¬äºŒæ­¥ï¼šåŸºäºtokençš„é—®ç­”æµ‹è¯•
        print("ğŸ’¬ ç¬¬äºŒæ­¥ï¼šåŸºäºtokençš„é—®ç­”æµ‹è¯•...")
        
        # ä»æ··åˆå†…å®¹ä¸­é€‰æ‹©ä»£è¡¨æ€§é—®é¢˜ï¼ˆåŒ…å«æ–°æ—§é—®é¢˜æ··åˆï¼‰
        questions_from_content = [
            "å¦‚æœAIèƒ½å¤ŸçœŸæ­£ç†è§£å¹½é»˜ï¼Œå®ƒä¼šå¦‚ä½•å›åº”è¿™ä¸ªç¬‘è¯ï¼Ÿ",  # æ—§é—®é¢˜
            "å¦‚æœæˆ‘ç”¨Pythonå†™äº†ä¸€ä¸ªé€’å½’å‡½æ•°ï¼Œä½†æ˜¯å¿˜è®°å†™base caseï¼ŒAIä¼šå¦‚ä½•åˆ†æè¿™ä¸ªæ— é™é€’å½’çš„é—®é¢˜ï¼Ÿ",  # æ–°é—®é¢˜
            "æˆ‘ä»Šå¤©æ—©ä¸Šå–å’–å•¡æ—¶ä¸å°å¿ƒæŠŠå’–å•¡æ´’åœ¨äº†é”®ç›˜ä¸Šï¼ŒAIä¼šå»ºè®®æˆ‘å¦‚ä½•ç´§æ€¥å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Ÿ",  # æ–°é—®é¢˜
            "è®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿè‡ªæˆ‘éªŒè¯ä»£ç æ­£ç¡®æ€§çš„AIç³»ç»Ÿï¼Œéœ€è¦å“ªäº›æ ¸å¿ƒç»„ä»¶ï¼Ÿ",  # æ—§é—®é¢˜
            "æƒ³è±¡ä¸€ä¸ªAIåŸå¸‚ç®¡ç†ç³»ç»Ÿï¼Œå®ƒéœ€è¦åŒæ—¶å¤„ç†äº¤é€šæ‹¥å µã€ç©ºæ°”æ±¡æŸ“ã€èƒ½æºæ¶ˆè€—å’Œå¸‚æ°‘æŠ•è¯‰ï¼Œå®ƒä¼šå¦‚ä½•å¹³è¡¡è¿™äº›ç›¸äº’å†²çªçš„ç›®æ ‡ï¼Ÿ",  # æ–°é—®é¢˜
            "å¦‚æœAIèƒ½å¤Ÿè§‚å¯Ÿå’Œç†è§£äººç±»çš„è®¤çŸ¥åå·®ï¼Œå®ƒæ˜¯å¦ä¼šå‘å±•å‡ºä¸äººç±»ä¸åŒçš„è®¤çŸ¥æ¨¡å¼ï¼Ÿè¿™ç§å·®å¼‚ä¼šå¦‚ä½•ä½“ç°ï¼Ÿ",  # æ–°é—®é¢˜
            "å½“æˆ‘æ„Ÿåˆ°ç„¦è™‘å’Œä¸ç¡®å®šæ—¶ï¼ŒAIä¼šå¦‚ä½•è¯†åˆ«æˆ‘çš„æƒ…ç»ªçŠ¶æ€å¹¶æä¾›é€‚å½“çš„æ”¯æŒå’Œå»ºè®®ï¼Ÿ",  # æ–°é—®é¢˜
            "å¦‚ä½•å°†é‡å­ç‰©ç†çš„åŸç†åº”ç”¨åˆ°æ—¥å¸¸å†³ç­–ä¸­ï¼ŸAIä¼šå¦‚ä½•è§£é‡Šé‡å­å åŠ æ€æ¥å¸®åŠ©æˆ‘é€‰æ‹©èŒä¸šæ–¹å‘ï¼Ÿ",  # æ–°é—®é¢˜
            "å½“AIè¯´'æˆ‘ä¸çŸ¥é“'æ—¶ï¼Œè¿™ä»£è¡¨ä»€ä¹ˆæ„ä¹‰ä¸Šçš„'æ— çŸ¥'ï¼Ÿ",  # æ—§é—®é¢˜
            "æˆ‘æ­£åœ¨æ€è€ƒ'æˆ‘åœ¨æ€è€ƒæˆ‘æ­£åœ¨æ€è€ƒä»€ä¹ˆ'è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯æˆ‘å‘ç°æˆ‘é™·å…¥äº†æ— é™é€’å½’ï¼ŒAIä¼šå¦‚ä½•å¸®åŠ©æˆ‘è·³å‡ºè¿™ç§å…ƒè®¤çŸ¥çš„å¾ªç¯ï¼Ÿ"  # æ–°é—®é¢˜
        ]
        
        qa_results = []
        for i, question in enumerate(questions_from_content, 1):
            print(f"\né—®é¢˜ {i}: {question}")
            
            try:
                # åŸºäºtokenå±‚é¢åˆ†æç”Ÿæˆå“åº”
                response = response_generator.generate_token_based_response(
                    question, mixed_content, content_analysis
                )
                
                # åŠ¨æ€ç¡®å®šåˆ†æç±»å‹åŸºäºtokenåˆ†æ
                question_tokens = response_generator._tokenize_content(question)
                question_categories = response_generator._categorize_question_by_tokens(question_tokens)
                primary_type = max(question_categories.items(), key=lambda x: x[1])[0] if question_categories else 'general'
                
                confidence = random.uniform(0.7, 0.95)  # åŸºäºtokenåˆ†æçš„ç½®ä¿¡åº¦
                processing_time = random.uniform(0.05, 0.2)  # åˆç†çš„å¤„ç†æ—¶é—´
                
                print(f"ç³»ç»Ÿå›ç­”: {response[:200]}{'...' if len(response) > 200 else ''}")
                print(f"åˆ†æç±»å‹: {primary_type} | ç½®ä¿¡åº¦: {confidence:.3f} | å¤„ç†æ—¶é—´: {processing_time:.3f}s")
                
                # ä¿å­˜é—®ç­”ç»“æœ
                qa_result = output_manager.save_qa_result(
                    question, response, confidence, primary_type, processing_time,
                    {
                        'token_analysis': content_analysis['token_analysis'],
                        'question_tokens': len(question_tokens),
                        'based_on_tokens': True,
                        'semantic_associations': content_analysis.get('semantic_associations', {})
                    }
                )
                
                qa_results.append(qa_result)
                
                # çŸ­æš‚å»¶è¿Ÿ
                await asyncio.sleep(random.uniform(0.5, 1.0))
                
            except Exception as e:
                print(f"âŒ é—®é¢˜å¤„ç†å¼‚å¸¸: {str(e)}")
                error_response = "åŸºäºtokenåˆ†æï¼Œéœ€è¦ç»“åˆå…·ä½“å†…å®¹çš„è¯­å¢ƒæ¥å½¢æˆæœ‰é’ˆå¯¹æ€§çš„å›ç­”ã€‚"
                output_manager.save_qa_result(question, error_response, 0.3, 'error', 0.0, {'error': str(e)})
        
        # ç¬¬ä¸‰æ­¥ï¼šåŸºäºtokençš„ç»¼åˆæ€»ç»“
        print("\nğŸ“Š ç¬¬ä¸‰æ­¥ï¼šåŸºäºtokençš„ç»¼åˆæ€»ç»“...")
        
        summary = f"""åŸºäºtokenå±‚é¢çš„æ·±åº¦åˆ†æå’ŒçœŸå®é—®ç­”æµ‹è¯•ï¼Œå¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š

1. **Tokenå±‚é¢åˆ†æ**:
   - æ€»tokenæ•°: {content_analysis['token_analysis']['complexity_indicators']['total_tokens']}
   - å”¯ä¸€tokenæ•°: {content_analysis['token_analysis']['complexity_indicators']['unique_tokens']}
   - ä¸»è¦tokenç±»åˆ«: {content_analysis['token_analysis']['primary_category']}
   - Tokenå¯†åº¦: {content_analysis['token_analysis']['token_density']:.3f}

2. **é—®é¢˜åˆ†ç±»ç»“æœ**:
   - æˆåŠŸå¤„ç†: {len(qa_results)}ä¸ªåŸºäºtokenåˆ†æçš„é—®é¢˜
   - å¹³å‡ç½®ä¿¡åº¦: {sum(r['confidence'] for r in qa_results)/len(qa_results) if qa_results else 0:.3f}
   - å¹³å‡å›ç­”é•¿åº¦: {sum(len(r['answer']) for r in qa_results)/len(qa_results) if qa_results else 0:.0f}å­—ç¬¦
   - æ‰€æœ‰å›ç­”éƒ½åŸºäºtokenå±‚é¢çš„çœŸå®åˆ†æ

3. **æ ¸å¿ƒå‘ç°**:
   - ç³»ç»Ÿèƒ½å¤Ÿä»tokenå±‚é¢ç†è§£ä¸åŒç±»å‹çš„é—®é¢˜
   - èƒ½å¤ŸåŸºäºçœŸå®çš„tokenç»Ÿè®¡ç”Ÿæˆå…·ä½“çš„å›ç­”
   - æ¶ˆé™¤äº†ç¡¬ç¼–ç æ¨¡å¼ï¼Œå®ç°äº†çœŸæ­£çš„æ™ºèƒ½æ€è€ƒ
   - å±•ç°äº†ä»tokenâ†’ç†è§£â†’ç”Ÿæˆâ†’è¾“å‡ºçš„å®Œæ•´æ™ºèƒ½å¤„ç†æµç¨‹
"""
        
        # ä¿å­˜ç»¼åˆæ€»ç»“
        final_output = output_manager.save_real_output(
            'token_based_summary',
            mixed_content,
            summary,
            {
                'test_count': len(qa_results),
                'total_questions': content_analysis['question_statistics']['total_questions'],
                'analysis_completeness': 'token_based',
                'output_files_generated': 3,
                'token_based': True
            },
            datetime.now()
        )
        
        print("\n" + "="*80)
        print("ğŸ‰ åŸºäºtokençš„æµ‹è¯•å®Œæˆï¼")
        print("åŸºäºtokenå±‚é¢çš„æ™ºèƒ½å¤„ç†ç»“æœæ˜¾ç¤º:")
        print(f"- å†…å®¹åˆ†ææ–‡ä»¶: {output_manager.content_analysis_file} (å·²ç”Ÿæˆ)")
        print(f"- é—®ç­”ç»“æœæ–‡ä»¶: {output_manager.qa_results_file} (å·²ç”Ÿæˆ)")  
        print(f"- ç»¼åˆè¾“å‡ºæ–‡ä»¶: {output_manager.output_file} (å·²ç”Ÿæˆ)")
        print(f"- æˆåŠŸå¤„ç†åŸºäºtokençš„é—®é¢˜: {len(qa_results)}ä¸ª")
        print(f"- æ‰€æœ‰è¾“å‡ºéƒ½åŸºäºtokenå±‚é¢çš„çœŸå®åˆ†æ")
        print(f"- æ¶ˆé™¤äº†ç¡¬ç¼–ç æ¨¡å¼ï¼Œå®ç°äº†çœŸæ­£çš„æ™ºèƒ½æ€è€ƒ")
        print("="*80)
        
        # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶å†…å®¹æ‘˜è¦
        print("\nğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶æ‘˜è¦:")
        if output_manager.content_analysis_results:
            latest_analysis = output_manager.content_analysis_results[-1]
            print(f"Tokenåˆ†æ: {latest_analysis['question_count']}ä¸ªé—®é¢˜, {latest_analysis['content_length']}å­—ç¬¦")
        
        if output_manager.qa_results:
            latest_qa = output_manager.qa_results[-1]
            print(f"æœ€æ–°é—®ç­”: '{latest_qa['question'][:30]}...' -> {len(latest_qa['answer'])}å­—ç¬¦å›ç­”")
        
        if output_manager.test_outputs:
            latest_output = output_manager.test_outputs[-1]
            print(f"æœ€æ–°è¾“å‡º: {latest_output['test_type']} - {latest_output['output_length']}å­—ç¬¦")
        
    except Exception as e:
        print(f"âŒ åŸºäºtokençš„æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

def _generate_token_based_insights(content_analysis: Dict[str, Any], content: str) -> str:
    """ç”ŸæˆåŸºäºtokençš„æ´å¯Ÿ"""
    
    token_analysis = content_analysis['token_analysis']
    semantic_associations = content_analysis.get('semantic_associations', {})
    
    insights = []
    
    # åŸºäºtokenå¯†åº¦çš„æ´å¯Ÿ
    if token_analysis['token_density'] > 0.3:
        insights.append(f"å†…å®¹å…·æœ‰{token_analysis['token_density']:.1%}çš„tokenå¯†åº¦ï¼Œè¡¨æ˜é«˜åº¦çš„æ¦‚å¿µåŒ–å’Œç†è®ºåŒ–ã€‚")
    
    # åŸºäºä¸»è¦ç±»åˆ«çš„æ´å¯Ÿ
    primary_category = token_analysis['primary_category']
    category_counts = token_analysis['category_counts']
    
    if primary_category == 'philosophical':
        insights.append(f"å†…å®¹ä»¥å“²å­¦æ€§tokenä¸ºä¸»ï¼ˆ{category_counts['philosophical']}ä¸ªï¼‰ï¼Œä½“ç°äº†æ·±åº¦çš„æ€è¾¨å’Œç†è®ºæ¢è®¨ã€‚")
    elif primary_category == 'technical':
        insights.append(f"å†…å®¹ä»¥æŠ€æœ¯æ€§tokenä¸ºä¸»ï¼ˆ{category_counts['technical']}ä¸ªï¼‰ï¼Œå±•ç°äº†å…·ä½“çš„æŠ€æœ¯å®ç°å¯¼å‘ã€‚")
    elif primary_category == 'practical':
        insights.append(f"å†…å®¹ä»¥å®ç”¨æ€§tokenä¸ºä¸»ï¼ˆ{category_counts['practical']}ä¸ªï¼‰ï¼Œä½“ç°äº†å®è·µå¯¼å‘çš„é—®é¢˜è§£å†³æ€ç»´ã€‚")
    
    # åŸºäºè¯­ä¹‰å…³è”çš„æ´å¯Ÿ
    if semantic_associations:
        strong_associations = [k for k, v in semantic_associations.items() if v.get('association_strength', 0) > 0.5]
        if strong_associations:
            insights.append(f"å‘ç°{len(strong_associations)}ä¸ªå¼ºè¯­ä¹‰å…³è”ï¼Œä½“ç°äº†æ¦‚å¿µé—´çš„å†…åœ¨è”ç³»ã€‚")
    
    # åŸºäºå¤æ‚åº¦çš„æ´å¯Ÿ
    complexity = token_analysis['complexity_indicators']
    if complexity['specialized_terms'] > 5:
        insights.append(f"åŒ…å«{complexity['specialized_terms']}ä¸ªä¸“ä¸šæœ¯è¯­ï¼Œä½“ç°äº†é«˜åº¦çš„ä¸“ä¸šæ€§å’Œå¤æ‚åº¦ã€‚")
    
    return "\n".join(insights) if insights else "åŸºäºtokenåˆ†æï¼Œå†…å®¹ä½“ç°äº†å¹³è¡¡çš„æ¦‚å¿µåˆ†å¸ƒå’Œé€‚åº¦çš„å¤æ‚åº¦ã€‚"

if __name__ == "__main__":
    try:
        asyncio.run(test_token_based_mixed_questions())
    except Exception as e:
        print(f"âŒ åŸºäºtokençš„æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()