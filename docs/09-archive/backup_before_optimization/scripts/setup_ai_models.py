#!/usr/bin/env python3
"""
AI æ¨¡å‹ç¯å¢ƒè®¾ç½®è„šæœ¬
è‡ªåŠ¨åŒ–è®¾ç½® AI æ¨¡å‹æœåŠ¡çš„ç¯å¢ƒå’Œä¾èµ–
"""

import os
import sys
import subprocess
import json
from pathlib import Path

def check_python_version():
    """æ£€æŸ¥ Python ç‰ˆæœ¬"""
    if sys.version_info < (3, 8):
        print("âŒ éœ€è¦ Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬")
        sys.exit(1)
    print(f"âœ… Python ç‰ˆæœ¬: {sys.version}")

def install_dependencies():
    """å®‰è£…ä¾èµ–é¡¹"""
    print("ğŸ“¦ å®‰è£…ä¾èµ–é¡¹...")
    
    try:
        subprocess.run([
            sys.executable, "-m", "pip", "install", "-r", "requirements.txt"
        ], check=True)
        print("âœ… ä¾èµ–é¡¹å®‰è£…å®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"âŒ ä¾èµ–é¡¹å®‰è£…å¤±è´¥: {e}")
        return False
    
    return True

def setup_env_file():
    """è®¾ç½®ç¯å¢ƒå˜é‡æ–‡ä»¶"""
    print("ğŸ”§ è®¾ç½®ç¯å¢ƒå˜é‡...")
    
    env_example = Path(".env.example")
    env_file = Path(".env")
    
    if not env_example.exists():
        print("âŒ .env.example æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    if not env_file.exists():
        # å¤åˆ¶ç¤ºä¾‹æ–‡ä»¶
        with open(env_example, 'r', encoding='utf-8') as f:
            content = f.read()
        
        with open(env_file, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print("âœ… å·²åˆ›å»º .env æ–‡ä»¶")
        print("âš ï¸  è¯·ç¼–è¾‘ .env æ–‡ä»¶ï¼Œæ·»åŠ ä½ çš„ API å¯†é’¥")
    else:
        print("âœ… .env æ–‡ä»¶å·²å­˜åœ¨")
    
    return True

def check_config_files():
    """æ£€æŸ¥é…ç½®æ–‡ä»¶"""
    print("ğŸ“ æ£€æŸ¥é…ç½®æ–‡ä»¶...")
    
    config_files = [
        "configs/multi_llm_config.json",
        "configs/api_keys.yaml"
    ]
    
    all_exist = True
    for config_file in config_files:
        if Path(config_file).exists():
            print(f"âœ… {config_file}")
        else:
            print(f"âŒ {config_file} ä¸å­˜åœ¨")
            all_exist = False
    
    return all_exist

def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åŸºæœ¬åŠŸèƒ½...")
    
    try:
        # æµ‹è¯•å¯¼å…¥
        sys.path.insert(0, str(Path.cwd()))
        from src.services.multi_llm_service import MultiLLMService
        print("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•é…ç½®åŠ è½½
        config_path = "configs/multi_llm_config.json"
        if Path(config_path).exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(config.get('models', {}))} ä¸ªæ¨¡å‹")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def check_optional_services():
    """æ£€æŸ¥å¯é€‰æœåŠ¡"""
    print("ğŸ”Œ æ£€æŸ¥å¯é€‰ AI æœåŠ¡...")
    
    services = [
        ("openai", "OpenAI GPT"),
        ("anthropic", "Anthropic Claude"),
        ("google.generativeai", "Google Gemini"),
        ("cohere", "Cohere"),
    ]
    
    available_services = []
    
    for module, name in services:
        try:
            __import__(module)
            print(f"âœ… {name}")
            available_services.append(name)
        except ImportError:
            print(f"âš ï¸  {name} - æœªå®‰è£…")
    
    if available_services:
        print(f"ğŸ“Š å¯ç”¨æœåŠ¡: {len(available_services)}/{len(services)}")
    else:
        print("âš ï¸  æ²¡æœ‰å¯ç”¨çš„ AI æœåŠ¡ï¼Œè¯·å®‰è£…ç›¸å…³ä¾èµ–")
    
    return available_services

def setup_ollama():
    """è®¾ç½® Ollamaï¼ˆå¯é€‰ï¼‰"""
    print("\nğŸ¦™ Ollama æœ¬åœ°æ¨¡å‹è®¾ç½®ï¼ˆå¯é€‰ï¼‰:")
    
    try:
        # æ£€æŸ¥ Ollama æ˜¯å¦å®‰è£…
        result = subprocess.run(["ollama", "--version"], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… Ollama å·²å®‰è£…")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹
            result = subprocess.run(["ollama", "list"], 
                                  capture_output=True, text=True)
            if "llama2" in result.stdout:
                print("âœ… å·²æœ‰ Llama2 æ¨¡å‹")
            else:
                print("ğŸ’¡ å»ºè®®å®‰è£… Llama2 æ¨¡å‹:")
                print("   ollama pull llama2:7b")
        else:
            print("âš ï¸  Ollama æœªå®‰è£…")
            print("ğŸ’¡ å®‰è£… Ollama: https://ollama.ai/")
    
    except FileNotFoundError:
        print("âš ï¸  Ollama æœªå®‰è£…")
        print("ğŸ’¡ å®‰è£… Ollama: https://ollama.ai/")

def print_usage_guide():
    """æ‰“å°ä½¿ç”¨æŒ‡å—"""
    print("\n" + "="*60)
    print("ğŸ‰ è®¾ç½®å®Œæˆï¼")
    print("\nğŸ“š ä½¿ç”¨æŒ‡å—:")
    print("1. é…ç½® API å¯†é’¥:")
    print("   ç¼–è¾‘ .env æ–‡ä»¶ï¼Œæ·»åŠ ä½ çš„ API å¯†é’¥")
    print("\n2. åˆ—å‡ºå¯ç”¨æ¨¡å‹:")
    print("   python scripts/ai_models.py list")
    print("\n3. å•æ¬¡æŸ¥è¯¢:")
    print("   python scripts/ai_models.py query 'ä½ å¥½' --model gpt-4")
    print("\n4. è¿›å…¥èŠå¤©æ¨¡å¼:")
    print("   python scripts/ai_models.py chat --model gemini-pro --stream")
    print("\n5. å¥åº·æ£€æŸ¥:")
    print("   python scripts/ai_models.py health")
    print("\n6. æŸ¥çœ‹æ–‡æ¡£:")
    print("   cat README_AI_MODELS.md")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ AI æ¨¡å‹ç¯å¢ƒè®¾ç½®")
    print("="*60)
    
    # æ£€æŸ¥ Python ç‰ˆæœ¬
    check_python_version()
    
    # å®‰è£…ä¾èµ–
    if not install_dependencies():
        sys.exit(1)
    
    # è®¾ç½®ç¯å¢ƒæ–‡ä»¶
    if not setup_env_file():
        sys.exit(1)
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not check_config_files():
        print("âš ï¸  æŸäº›é…ç½®æ–‡ä»¶ç¼ºå¤±ï¼Œä½†å¯ä»¥ç»§ç»­")
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    if not test_basic_functionality():
        print("âš ï¸  åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
    
    # æ£€æŸ¥å¯é€‰æœåŠ¡
    available_services = check_optional_services()
    
    # è®¾ç½® Ollama
    setup_ollama()
    
    # æ‰“å°ä½¿ç”¨æŒ‡å—
    print_usage_guide()

if __name__ == "__main__":
    main()