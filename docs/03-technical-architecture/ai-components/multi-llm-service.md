# å¤šæ¨¡å‹ LLM æœåŠ¡

## æ¦‚è¿°

å¤šæ¨¡å‹ LLM æœåŠ¡æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¥å£ï¼Œæ”¯æŒå¤šç§ä¸»æµ AI å¤§æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š

- **OpenAI GPT** (GPT-4, GPT-3.5-turbo)
- **Google Gemini** (Gemini Pro, Gemini Pro Vision)
- **Anthropic Claude** (Claude-3 Opus, Sonnet, Haiku)
- **Ollama** (æœ¬åœ°æ¨¡å‹ï¼šLlama2, CodeLlama, Mistral ç­‰)
- **Azure OpenAI** (ä¼ä¸šçº§ GPT æœåŠ¡)
- **Cohere** (Command ç³»åˆ—)
- **Hugging Face** (å¼€æºæ¨¡å‹)

## æ ¸å¿ƒèŒè´£ä¸åŠŸèƒ½

1.  **ç»Ÿä¸€çš„ LLM æ¥å£**:
    *   ä¸ºæ‰€æœ‰æ”¯æŒçš„ LLM æä¾›å•†æä¾›ä¸€è‡´çš„ `chat_completion` å’Œ `stream_completion` æ–¹æ³•ã€‚
    *   æŠ½è±¡åŒ–äº†æä¾›å•†ç‰¹å®šçš„ API è°ƒç”¨ã€æ¶ˆæ¯æ ¼å¼å’Œå“åº”ç»“æ„ã€‚

2.  **å¤šæä¾›å•†æ”¯æŒ**:
    *   é›†æˆäº†å¹¿æ³›çš„å•†ä¸šå’Œå¼€æº LLM æä¾›å•†ï¼Œä½¿ AI èƒ½å¤Ÿä¸ºç‰¹å®šä»»åŠ¡æˆ–ä¸Šä¸‹æ–‡é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹ã€‚
    *   æ”¯æŒçš„æä¾›å•†åŒ…æ‹¬ OpenAIã€Google Geminiã€Anthropic Claudeã€Ollama (æœ¬åœ°æ¨¡å‹)ã€Azure OpenAIã€Cohere å’Œ Hugging Faceã€‚

3.  **çµæ´»çš„æ¨¡å‹é…ç½®**:
    *   ä»å¤–éƒ¨ JSON æ–‡ä»¶ (ä¾‹å¦‚ `configs/multi_llm_config.json`) åŠ è½½æ¨¡å‹é…ç½®ã€‚
    *   é…ç½®åŒ…æ‹¬ `model_name`ã€`api_key` (ä»ç¯å¢ƒå˜é‡åŠ è½½)ã€`max_tokens`ã€`temperature`ã€`top_p`ã€`cost_per_1k_tokens` å’Œ `enabled` çŠ¶æ€ã€‚

4.  **ä½¿ç”¨ç»Ÿè®¡å’Œæˆæœ¬è¿½è¸ª**:
    *   è·Ÿè¸ªæ¯ä¸ªæ¨¡å‹çš„è¯¦ç»†ä½¿ç”¨ç»Ÿè®¡ï¼ŒåŒ…æ‹¬ `total_requests`ã€`total_tokens` (æç¤ºå’Œå®Œæˆ)ã€`total_cost`ã€`average_latency` å’Œ `error_count`ã€‚
    *   å®ç°ä¸åŒ LLM ä¹‹é—´çš„æˆæœ¬ä¼˜åŒ–å’Œæ€§èƒ½ç›‘æ§ã€‚

5.  **å¥åº·æ£€æŸ¥**:
    *   æä¾› `health_check` æ–¹æ³•ï¼Œé€šè¿‡å‘é€ç®€å•çš„æµ‹è¯•æ¶ˆæ¯æ¥éªŒè¯å·²é…ç½® LLM çš„å¯ç”¨æ€§å’Œå“åº”èƒ½åŠ›ã€‚

6.  **å¼‚æ­¥æ“ä½œ**:
    *   ä¸ LLM çš„æ‰€æœ‰äº¤äº’éƒ½æ˜¯å¼‚æ­¥çš„ï¼Œç¡®ä¿éé˜»å¡æ“ä½œå’Œå¹¶å‘è¯·æ±‚çš„é«˜æ•ˆå¤„ç†ã€‚

7.  **å…¨å±€å®ä¾‹ç®¡ç†**:
    *   ç®¡ç† `MultiLLMService` çš„å…¨å±€å•ä¾‹å®ä¾‹ï¼Œä»¥ç¡®ä¿åº”ç”¨ç¨‹åºä¸­ä¸€è‡´çš„è®¿é—®å’Œèµ„æºç®¡ç†ã€‚

## ğŸ“Š ç›‘æ§å’Œç»Ÿè®¡

- **ä½¿ç”¨ç»Ÿè®¡**: è¯¦ç»†çš„ token ä½¿ç”¨å’Œæˆæœ¬ç»Ÿè®¡
- **æ€§èƒ½ç›‘æ§**: å»¶è¿Ÿã€é”™è¯¯ç‡ç­‰æ€§èƒ½æŒ‡æ ‡
- **ä½¿ç”¨å†å²**: å®Œæ•´çš„ä½¿ç”¨å†å²è®°å½•

## ğŸ”§ é…ç½®ç®¡ç†

- **çµæ´»é…ç½®**: JSON é…ç½®æ–‡ä»¶æ”¯æŒ
- **ç¯å¢ƒå˜é‡**: å®‰å…¨çš„ API å¯†é’¥ç®¡ç†
- **åŠ¨æ€é…ç½®**: è¿è¡Œæ—¶é…ç½®æ›´æ–°

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. é…ç½® API å¯†é’¥

å¤åˆ¶ç¯å¢ƒå˜é‡æ¨¡æ¿ï¼š
```bash
cp .env.example .env
```

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼Œæ·»åŠ ä½ çš„ API å¯†é’¥ï¼š
```bash
# OpenAI
OPENAI_API_KEY=sk-your-openai-key

# Google Gemini
GEMINI_API_KEY=your-gemini-key

# Anthropic Claude
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key

# å…¶ä»–æœåŠ¡...
```

### 3. ä½¿ç”¨ CLI å·¥å…·

#### åˆ—å‡ºå¯ç”¨æ¨¡å‹
```bash
python scripts/ai_models.py list
```

#### å•æ¬¡æŸ¥è¯¢
```bash
python scripts/ai_models.py query "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±" --model gpt-4
```

#### è¿›å…¥èŠå¤©æ¨¡å¼
```bash
python scripts/ai_models.py chat --model gemini-pro --stream
```

#### å¥åº·æ£€æŸ¥
```bash
python scripts/ai_models.py health
```

#### æ¯”è¾ƒæ¨¡å‹
```bash
python scripts/ai_models.py compare "è§£é‡Šé‡å­è®¡ç®—" --models gpt-4 claude-3-sonnet gemini-pro
```

## ç¼–ç¨‹æ¥å£

### åŸºæœ¬ç”¨æ³•

```python
import asyncio
from src.services.multi_llm_service import MultiLLMService, ChatMessage

async def main():
    # åˆå§‹åŒ–æœåŠ¡
    service = MultiLLMService('configs/multi_llm_config.json')
    
    # åˆ›å»ºæ¶ˆæ¯
    messages = [
        ChatMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹"),
        ChatMessage(role="user", content="ä½ å¥½ï¼")
    ]
    
    # å‘é€è¯·æ±‚
    response = await service.chat_completion(messages, model_id="gpt-4")
    print(response.content)
    
    # å…³é—­æœåŠ¡
    await service.close()

asyncio.run(main())
```

### æµå¼å“åº”

```python
async def stream_example():
    service = MultiLLMService('configs/multi_llm_config.json')
    
    messages = [ChatMessage(role="user", content="å†™ä¸€é¦–å…³äºAIçš„è¯—")]
    
    async for chunk in service.stream_completion(messages, model_id="claude-3-sonnet"):
        print(chunk, end="", flush=True)
    
    await service.close()
```

### æ¨¡å‹æ¯”è¾ƒ

```python
async def compare_models():
    service = MultiLLMService('configs/multi_llm_config.json')
    
    query = "è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"
    models = ["gpt-4", "claude-3-sonnet", "gemini-pro"]
    
    for model in models:
        messages = [ChatMessage(role="user", content=query)]
        response = await service.chat_completion(messages, model_id=model)
        
        print(f"\n{model}:")
        print(response.content)
        print(f"æˆæœ¬: ${response.cost:.4f}, å»¶è¿Ÿ: {response.latency:.2f}s")
    
    await service.close()
```

## é…ç½®æ–‡ä»¶

### æ¨¡å‹é…ç½® (configs/multi_llm_config.json)

```json
{
  "default_model": "gpt-4",
  "models": {
    "gpt-4": {
      "provider": "openai",
      "model_name": "gpt-4",
      "api_key_env": "OPENAI_API_KEY",
      "max_tokens": 8192,
      "temperature": 0.7,
      "cost_per_1k_tokens": 0.03,
      "enabled": true
    },
    "gemini-pro": {
      "provider": "google",
      "model_name": "gemini-pro",
      "api_key_env": "GEMINI_API_KEY",
      "max_tokens": 8192,
      "temperature": 0.7,
      "cost_per_1k_tokens": 0.0005,
      "enabled": true
    }
  }
}
```

## æ”¯æŒçš„æ¨¡å‹

### OpenAI
- `gpt-4`: æœ€å¼ºå¤§çš„ GPT æ¨¡å‹
- `gpt-3.5-turbo`: å¿«é€Ÿä¸”ç»æµçš„é€‰æ‹©

### Google Gemini
- `gemini-pro`: å¤šæ¨¡æ€å¤§æ¨¡å‹
- `gemini-pro-vision`: æ”¯æŒå›¾åƒç†è§£

### Anthropic Claude
- `claude-3-opus`: æœ€å¼ºå¤§çš„ Claude æ¨¡å‹
- `claude-3-sonnet`: å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬
- `claude-3-haiku`: å¿«é€Ÿå“åº”

### Ollama (æœ¬åœ°æ¨¡å‹)
- `llama2-7b/13b`: Meta çš„å¼€æºæ¨¡å‹
- `codellama`: ä¸“é—¨ç”¨äºä»£ç ç”Ÿæˆ
- `mistral-7b`: é«˜æ•ˆçš„å¼€æºæ¨¡å‹

### å…¶ä»–
- `azure-gpt-4`: Azure OpenAI æœåŠ¡
- `cohere-command`: Cohere çš„å¯¹è¯æ¨¡å‹
- `huggingface-llama`: Hugging Face æ‰˜ç®¡æ¨¡å‹

## æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©
- **åˆ›æ„å†™ä½œ**: Claude-3 Opus, GPT-4
- **ä»£ç ç”Ÿæˆ**: CodeLlama, GPT-4
- **å¿«é€Ÿé—®ç­”**: GPT-3.5-turbo, Claude-3 Haiku
- **å¤šè¯­è¨€**: Gemini Pro
- **æœ¬åœ°éƒ¨ç½²**: Ollama æ¨¡å‹

### 2. æˆæœ¬ä¼˜åŒ–
- ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹è¿›è¡Œç®€å•ä»»åŠ¡
- è®¾ç½®åˆç†çš„ max_tokens é™åˆ¶
- ç›‘æ§ä½¿ç”¨ç»Ÿè®¡ï¼Œä¼˜åŒ–æ¨¡å‹é€‰æ‹©

### 3. æ€§èƒ½ä¼˜åŒ–
- ä½¿ç”¨æµå¼å“åº”æå‡ç”¨æˆ·ä½“éªŒ
- å®æ–½ç¼“å­˜æœºåˆ¶å‡å°‘é‡å¤è¯·æ±‚
- é…ç½®è´Ÿè½½å‡è¡¡åˆ†æ•£è¯·æ±‚

### 4. é”™è¯¯å¤„ç†
- é…ç½®å¤‡ç”¨æ¨¡å‹é“¾
- å®æ–½é‡è¯•æœºåˆ¶
- ç›‘æ§æ¨¡å‹å¥åº·çŠ¶æ€

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **API å¯†é’¥é”™è¯¯**
   - æ£€æŸ¥ `.env` æ–‡ä»¶ä¸­çš„å¯†é’¥æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤å¯†é’¥æœ‰è¶³å¤Ÿçš„æƒé™å’Œä½™é¢

2. **æ¨¡å‹ä¸å¯ç”¨**
   - è¿è¡Œå¥åº·æ£€æŸ¥: `python scripts/ai_models.py health`
   - æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®

3. **Ollama è¿æ¥å¤±è´¥**
   - ç¡®è®¤ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ
   - æ£€æŸ¥ `OLLAMA_BASE_URL` é…ç½®

4. **ä¾èµ–å®‰è£…é—®é¢˜**
   - ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ: `python -m venv venv`
   - æ›´æ–° pip: `pip install --upgrade pip`

### æ—¥å¿—å’Œè°ƒè¯•

å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼š
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

æŸ¥çœ‹ä½¿ç”¨ç»Ÿè®¡ï¼š
```bash
python scripts/ai_models.py stats
```

## æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„æ¨¡å‹æä¾›å•†

1. ç»§æ‰¿ `BaseLLMProvider` ç±»
2. å®ç° `chat_completion` å’Œ `stream_completion` æ–¹æ³•
3. åœ¨ `MultiLLMService._create_provider` ä¸­æ³¨å†Œ
4. æ›´æ–°é…ç½®æ–‡ä»¶å’Œæ–‡æ¡£

### è‡ªå®šä¹‰åŠŸèƒ½

- å®ç°è‡ªå®šä¹‰çš„æ¶ˆæ¯é¢„å¤„ç†
- æ·»åŠ ç‰¹å®šé¢†åŸŸçš„æç¤ºæ¨¡æ¿
- é›†æˆå¤–éƒ¨å·¥å…·å’Œæ’ä»¶

## ç›¸å…³æ–‡æ¡£

- [API å‚è€ƒ](../api/multi-llm-api.md)
- [é…ç½®æŒ‡å—](../configuration/llm-config.md)
- [éƒ¨ç½²æŒ‡å—](../deployment/llm-deployment.md)