# ExperienceReplayBuffer: Experience Replay Buffer for Learning

This document provides an overview of the `ExperienceReplayBuffer` module (`src/core_ai/learning/experience_replay.py`).

## Purpose

The `ExperienceReplayBuffer` is a fundamental component in reinforcement learning designed to store and sample past AI experiences. Its primary purpose is to enable the AI to learn more effectively from its past interactions by breaking correlations in the observation sequence and allowing for efficient re-training on important experiences. This helps in stabilizing learning algorithms and improving overall learning performance.

## Key Responsibilities and Features

*   **Buffer Management**: Stores individual experiences, typically as tuples containing `(state, action, reward, next_state, done, error)`.
*   **Capacity Control**: Maintains a fixed `capacity` for the buffer. When the buffer is full, adding new experiences overwrites the oldest ones, ensuring the buffer always contains recent experiences.
*   **Priority Calculation (`_calculate_priority`)**: Assigns a priority score to each experience. The current implementation is a placeholder that assigns a high priority (1.0) to experiences that resulted in an `error` and a default priority (0.5) otherwise. This mechanism is crucial for implementing Prioritized Experience Replay (PER), allowing the learning agent to focus on more informative or challenging experiences.
*   **Experience Addition (`add_experience`)**: Adds a new experience to the buffer. It calculates the experience's priority and manages the buffer's circular overwrite mechanism.
*   **Batch Sampling (`sample_batch`)**: Samples a batch of experiences from the buffer for use in training. The sampling process is biased by the experiences' priorities, meaning experiences with higher priorities are more likely to be selected. This helps in efficient learning by focusing on critical data.

## How it Works

The `ExperienceReplayBuffer` operates by maintaining a collection of past experiences. When a new experience is generated by the AI (e.g., after taking an action in an environment), it is added to the buffer, and a priority is calculated for it. When the learning algorithm needs data for training, it requests a `batch_size` number of experiences. The buffer then samples these experiences, giving preference to those with higher priorities. This process helps decorrelate sequential experiences and allows the learning agent to revisit and learn from important past events multiple times, leading to more stable and efficient learning.

## Integration with Other Modules

*   **`numpy`**: Utilized for numerical operations, particularly in the calculation of probabilities for prioritized sampling and general array manipulations.
*   **`datetime`**: Used to timestamp experiences when they are added to the buffer, which can be useful for tracking recency or for time-based prioritization.
*   **Reinforcement Learning Agents/Algorithms**: This module is a core utility for any reinforcement learning agent or algorithm. Agents would use `add_experience` to store their interactions with the environment, and the learning algorithms would use `sample_batch` to retrieve data for training their models.

## Code Location

`src/core_ai/learning/experience_replay.py`
