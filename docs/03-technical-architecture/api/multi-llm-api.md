# ğŸ¤– å¤šæ¨¡å‹ LLM API åƒè€ƒæ–‡æª”

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æª”è©³ç´°æè¿°å¤šæ¨¡å‹ LLM æœå‹™çš„ API æ¥å£ï¼ŒåŒ…æ‹¬è«‹æ±‚æ ¼å¼ã€éŸ¿æ‡‰çµæ§‹ã€éŒ¯èª¤è™•ç†å’Œä½¿ç”¨ç¤ºä¾‹ã€‚

---

## ğŸ”— åŸºç¤ä¿¡æ¯

### API åŸºç¤ URL
```
http://localhost:8000/api/llm
```

### èªè­‰æ–¹å¼
- **API Key**: é€šé `X-API-Key` æ¨™é ­å‚³é
- **Bearer Token**: é€šé `Authorization: Bearer <token>` æ¨™é ­å‚³é

### å…§å®¹é¡å‹
- **è«‹æ±‚**: `application/json`
- **éŸ¿æ‡‰**: `application/json` æˆ– `text/event-stream` (æµå¼)

---

## ğŸš€ æ ¸å¿ƒ API ç«¯é»

### 1. æ–‡æœ¬ç”Ÿæˆ API

#### POST `/generate`
ç”Ÿæˆæ–‡æœ¬éŸ¿æ‡‰çš„ä¸»è¦ç«¯é»ã€‚

**è«‹æ±‚æ ¼å¼**:
```json
{
  "model": "gemini-pro",
  "messages": [
    {
      "role": "user",
      "content": "ä½ å¥½ï¼Œè«‹ä»‹ç´¹ä¸€ä¸‹äººå·¥æ™ºèƒ½"
    }
  ],
  "parameters": {
    "temperature": 0.7,
    "max_tokens": 1000,
    "top_p": 0.9,
    "stream": false
  }
}
```

**éŸ¿æ‡‰æ ¼å¼**:
```json
{
  "id": "req_12345",
  "model": "gemini-pro",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è¨ˆç®—æ©Ÿç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 150,
    "total_tokens": 165
  },
  "cost": {
    "input_cost": 0.0001,
    "output_cost": 0.0003,
    "total_cost": 0.0004
  }
}
```

### 2. æµå¼ç”Ÿæˆ API

#### POST `/generate/stream`
æ”¯æŒå¯¦æ™‚æµå¼è¼¸å‡ºçš„æ–‡æœ¬ç”Ÿæˆã€‚

**è«‹æ±‚æ ¼å¼**:
```json
{
  "model": "gpt-4",
  "messages": [
    {
      "role": "user", 
      "content": "å¯«ä¸€å€‹é—œæ–¼æ˜¥å¤©çš„è©©"
    }
  ],
  "parameters": {
    "temperature": 0.8,
    "max_tokens": 500,
    "stream": true
  }
}
```

**éŸ¿æ‡‰æ ¼å¼** (Server-Sent Events):
```
data: {"id":"stream_123","choices":[{"delta":{"content":"æ˜¥"}}]}

data: {"id":"stream_123","choices":[{"delta":{"content":"å¤©"}}]}

data: {"id":"stream_123","choices":[{"delta":{"content":"ä¾†"}}]}

data: [DONE]
```

### 3. æ¨¡å‹ç®¡ç† API

#### GET `/models`
ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ã€‚

**éŸ¿æ‡‰æ ¼å¼**:
```json
{
  "models": [
    {
      "id": "gemini-pro",
      "name": "Google Gemini Pro",
      "provider": "google",
      "type": "text",
      "status": "available",
      "capabilities": ["text-generation", "conversation"],
      "limits": {
        "max_tokens": 32768,
        "context_window": 32768
      },
      "pricing": {
        "input_per_1k_tokens": 0.0005,
        "output_per_1k_tokens": 0.0015
      }
    }
  ]
}
```

#### GET `/models/{model_id}/status`
æª¢æŸ¥ç‰¹å®šæ¨¡å‹çš„ç‹€æ…‹ã€‚

**éŸ¿æ‡‰æ ¼å¼**:
```json
{
  "model_id": "gpt-4",
  "status": "available",
  "health": "healthy",
  "response_time_ms": 1250,
  "last_check": "2025-01-01T12:00:00Z",
  "error_rate": 0.02
}
```

### 4. å¥åº·æª¢æŸ¥ API

#### GET `/health`
æª¢æŸ¥æœå‹™æ•´é«”å¥åº·ç‹€æ³ã€‚

**éŸ¿æ‡‰æ ¼å¼**:
```json
{
  "status": "healthy",
  "timestamp": "2025-01-01T12:00:00Z",
  "services": {
    "llm_service": "healthy",
    "model_manager": "healthy",
    "cost_tracker": "healthy"
  },
  "models": {
    "available": 7,
    "healthy": 6,
    "degraded": 1,
    "unavailable": 0
  }
}
```

---

## ğŸ“Š çµ±è¨ˆå’Œç›£æ§ API

### 1. ä½¿ç”¨çµ±è¨ˆ

#### GET `/stats/usage`
ç²å–ä½¿ç”¨çµ±è¨ˆä¿¡æ¯ã€‚

**æŸ¥è©¢åƒæ•¸**:
- `start_date`: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
- `end_date`: çµæŸæ—¥æœŸ (YYYY-MM-DD)
- `model`: ç‰¹å®šæ¨¡å‹ (å¯é¸)

**éŸ¿æ‡‰æ ¼å¼**:
```json
{
  "period": {
    "start": "2025-01-01",
    "end": "2025-01-31"
  },
  "total_requests": 15420,
  "total_tokens": 2847392,
  "total_cost": 45.67,
  "by_model": {
    "gemini-pro": {
      "requests": 8500,
      "tokens": 1547392,
      "cost": 23.45
    },
    "gpt-4": {
      "requests": 3200,
      "tokens": 847392,
      "cost": 18.92
    }
  }
}
```

### 2. æ€§èƒ½æŒ‡æ¨™

#### GET `/stats/performance`
ç²å–æ€§èƒ½æŒ‡æ¨™ã€‚

**éŸ¿æ‡‰æ ¼å¼**:
```json
{
  "response_times": {
    "average_ms": 1850,
    "p50_ms": 1200,
    "p95_ms": 3500,
    "p99_ms": 5200
  },
  "success_rate": 0.987,
  "error_rate": 0.013,
  "throughput": {
    "requests_per_minute": 45,
    "tokens_per_minute": 12500
  }
}
```

---

## ğŸ”§ é…ç½® API

### 1. æ¨¡å‹é…ç½®

#### PUT `/config/models/{model_id}`
æ›´æ–°æ¨¡å‹é…ç½®ã€‚

**è«‹æ±‚æ ¼å¼**:
```json
{
  "enabled": true,
  "priority": 1,
  "rate_limit": {
    "requests_per_minute": 60,
    "tokens_per_minute": 10000
  },
  "fallback_models": ["gemini-pro", "gpt-3.5-turbo"],
  "parameters": {
    "default_temperature": 0.7,
    "max_tokens": 2000
  }
}
```

### 2. è² è¼‰å‡è¡¡é…ç½®

#### PUT `/config/load-balancing`
é…ç½®è² è¼‰å‡è¡¡ç­–ç•¥ã€‚

**è«‹æ±‚æ ¼å¼**:
```json
{
  "strategy": "round_robin",
  "health_check_interval": 30,
  "failure_threshold": 3,
  "recovery_threshold": 2,
  "weights": {
    "gemini-pro": 0.4,
    "gpt-4": 0.3,
    "claude-3": 0.3
  }
}
```

---

## âŒ éŒ¯èª¤è™•ç†

### éŒ¯èª¤éŸ¿æ‡‰æ ¼å¼
```json
{
  "error": {
    "code": "MODEL_UNAVAILABLE",
    "message": "The requested model is currently unavailable",
    "details": {
      "model": "gpt-4",
      "reason": "rate_limit_exceeded",
      "retry_after": 60
    },
    "request_id": "req_12345"
  }
}
```

### å¸¸è¦‹éŒ¯èª¤ä»£ç¢¼

| éŒ¯èª¤ä»£ç¢¼ | HTTP ç‹€æ…‹ | æè¿° |
|---------|----------|------|
| `INVALID_REQUEST` | 400 | è«‹æ±‚æ ¼å¼éŒ¯èª¤ |
| `UNAUTHORIZED` | 401 | èªè­‰å¤±æ•— |
| `MODEL_NOT_FOUND` | 404 | æ¨¡å‹ä¸å­˜åœ¨ |
| `MODEL_UNAVAILABLE` | 503 | æ¨¡å‹æš«æ™‚ä¸å¯ç”¨ |
| `RATE_LIMIT_EXCEEDED` | 429 | è¶…å‡ºé€Ÿç‡é™åˆ¶ |
| `INTERNAL_ERROR` | 500 | å…§éƒ¨æœå‹™éŒ¯èª¤ |

---

## ğŸ” èªè­‰å’Œæˆæ¬Š

### API Key èªè­‰
```bash
curl -X POST "http://localhost:8000/api/llm/generate" \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-pro",
    "messages": [{"role": "user", "content": "Hello"}]
  }'
```

### Bearer Token èªè­‰
```bash
curl -X POST "http://localhost:8000/api/llm/generate" \
  -H "Authorization: Bearer your-jwt-token" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4",
    "messages": [{"role": "user", "content": "Hello"}]
  }'
```

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### Python å®¢æˆ¶ç«¯ç¤ºä¾‹
```python
import requests
import json

class LLMClient:
    def __init__(self, base_url, api_key):
        self.base_url = base_url
        self.headers = {
            'X-API-Key': api_key,
            'Content-Type': 'application/json'
        }
    
    def generate(self, model, messages, **kwargs):
        payload = {
            'model': model,
            'messages': messages,
            'parameters': kwargs
        }
        
        response = requests.post(
            f"{self.base_url}/generate",
            headers=self.headers,
            json=payload
        )
        
        return response.json()
    
    def stream_generate(self, model, messages, **kwargs):
        payload = {
            'model': model,
            'messages': messages,
            'parameters': {**kwargs, 'stream': True}
        }
        
        response = requests.post(
            f"{self.base_url}/generate/stream",
            headers=self.headers,
            json=payload,
            stream=True
        )
        
        for line in response.iter_lines():
            if line.startswith(b'data: '):
                data = line[6:].decode('utf-8')
                if data != '[DONE]':
                    yield json.loads(data)

# ä½¿ç”¨ç¤ºä¾‹
client = LLMClient('http://localhost:8000/api/llm', 'your-api-key')

# æ™®é€šç”Ÿæˆ
result = client.generate(
    model='gemini-pro',
    messages=[{'role': 'user', 'content': 'ä½ å¥½'}],
    temperature=0.7,
    max_tokens=100
)

print(result['choices'][0]['message']['content'])

# æµå¼ç”Ÿæˆ
for chunk in client.stream_generate(
    model='gpt-4',
    messages=[{'role': 'user', 'content': 'å¯«ä¸€é¦–è©©'}],
    temperature=0.8
):
    if 'choices' in chunk and chunk['choices']:
        delta = chunk['choices'][0].get('delta', {})
        if 'content' in delta:
            print(delta['content'], end='', flush=True)
```

### JavaScript å®¢æˆ¶ç«¯ç¤ºä¾‹
```javascript
class LLMClient {
    constructor(baseUrl, apiKey) {
        this.baseUrl = baseUrl;
        this.apiKey = apiKey;
    }
    
    async generate(model, messages, parameters = {}) {
        const response = await fetch(`${this.baseUrl}/generate`, {
            method: 'POST',
            headers: {
                'X-API-Key': this.apiKey,
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                model,
                messages,
                parameters
            })
        });
        
        return await response.json();
    }
    
    async* streamGenerate(model, messages, parameters = {}) {
        const response = await fetch(`${this.baseUrl}/generate/stream`, {
            method: 'POST',
            headers: {
                'X-API-Key': this.apiKey,
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                model,
                messages,
                parameters: { ...parameters, stream: true }
            })
        });
        
        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        
        while (true) {
            const { done, value } = await reader.read();
            if (done) break;
            
            const chunk = decoder.decode(value);
            const lines = chunk.split('\n');
            
            for (const line of lines) {
                if (line.startsWith('data: ')) {
                    const data = line.slice(6);
                    if (data !== '[DONE]') {
                        yield JSON.parse(data);
                    }
                }
            }
        }
    }
}

// ä½¿ç”¨ç¤ºä¾‹
const client = new LLMClient('http://localhost:8000/api/llm', 'your-api-key');

// æ™®é€šç”Ÿæˆ
const result = await client.generate(
    'gemini-pro',
    [{ role: 'user', content: 'ä½ å¥½' }],
    { temperature: 0.7, max_tokens: 100 }
);

console.log(result.choices[0].message.content);

// æµå¼ç”Ÿæˆ
for await (const chunk of client.streamGenerate(
    'gpt-4',
    [{ role: 'user', content: 'å¯«ä¸€é¦–è©©' }],
    { temperature: 0.8 }
)) {
    if (chunk.choices && chunk.choices[0].delta.content) {
        process.stdout.write(chunk.choices[0].delta.content);
    }
}
```

---

## ğŸ”„ ç‰ˆæœ¬æ§åˆ¶

### API ç‰ˆæœ¬
- **ç•¶å‰ç‰ˆæœ¬**: v1
- **ç‰ˆæœ¬æ ¼å¼**: `/api/v1/llm/...`
- **å‘å¾Œå…¼å®¹**: æ”¯æŒèˆŠç‰ˆæœ¬ API

### ç‰ˆæœ¬æ›´æ–°ç­–ç•¥
- **ä¸»ç‰ˆæœ¬**: ç ´å£æ€§è®Šæ›´
- **æ¬¡ç‰ˆæœ¬**: æ–°åŠŸèƒ½æ·»åŠ 
- **ä¿®è¨‚ç‰ˆæœ¬**: éŒ¯èª¤ä¿®å¾©

---

## ğŸ“š ç›¸é—œæ–‡æª”

- [å¤šæ¨¡å‹ LLM æœå‹™æ¦‚è¿°](../ai-components/multi-llm-service.md)
- [é…ç½®æŒ‡å—](../configuration/llm-config.md)
- [éƒ¨ç½²æŒ‡å—](../deployment/llm-deployment.md)
- [æ•…éšœæ’é™¤](../../05-development/debugging/troubleshooting.md)

---

*æ–‡æª”ç‰ˆæœ¬: 1.0*  
*æœ€å¾Œæ›´æ–°: 2025å¹´1æœˆ*  
*ç¶­è­·è€…: Unified AI Project åœ˜éšŠ*