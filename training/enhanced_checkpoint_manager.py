#!/usr/bin/env python3
"""
å¢å¼ºçš„æ£€æŸ¥ç‚¹ç®¡ç†å™¨
è´Ÿè´£ç®¡ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ£€æŸ¥ç‚¹ä¿å­˜ã€æ¢å¤å’Œæ¸…ç†
"""

import logging
import json
import time
import os
from typing import Dict, Any, List, Optional
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, asdict

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
project_root: str = Path(__file__).parent.parent
_ = sys.path.insert(0, str(project_root))

# åˆ›å»ºåŸºæœ¬æ¨¡æ‹Ÿç±»
ErrorContext = type('ErrorContext', (), {
    '__init__': lambda self, component, operation, details=None: (
    setattr(self, 'component', component),
    setattr(self, 'operation', operation),
    setattr(self, 'details', details or {})
    )[-1]
})

class GlobalErrorHandler:
    @staticmethod
    def handle_error(error, context, strategy=None)
    print(f"Error in {context.component}.{context.operation}: {error}")

global_error_handler = GlobalErrorHandler()

logger = logging.getLogger(__name__)

# ç¡®ä¿æ£€æŸ¥ç‚¹ç›®å½•å­˜åœ¨
CHECKPOINTS_DIR = project_root / "training" / "checkpoints"
CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)

@dataclass
class CheckpointInfo:
    """æ£€æŸ¥ç‚¹ä¿¡æ¯"""
    checkpoint_id: str
    task_id: str
    epoch: int
    timestamp: float
    file_path: str
    metrics: Dict[str, Any]
    checkpoint_type: str  # 'regular', 'epoch', 'time_based', 'event_triggered'
    size_bytes: int = 0
    is_compressed: bool = False

class EnhancedCheckpointManager:
    """å¢å¼ºçš„æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""

    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
    self.config = config or {}
    self.error_handler = global_error_handler
    self.checkpoints: Dict[str, CheckpointInfo] = {}
    self.checkpoint_history: List[CheckpointInfo] = []

    # é…ç½®å‚æ•°
    self.strategy = self.config.get('strategy', 'hybrid')  # 'epoch_only', 'time_based', 'hybrid'
    self.epoch_interval = self.config.get('epoch_interval', 5)  # epochæ£€æŸ¥ç‚¹é—´éš”
    self.time_interval_minutes = self.config.get('time_interval_minutes', 30)  # æ—¶é—´æ£€æŸ¥ç‚¹é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
    self.keep_last_n_checkpoints = self.config.get('keep_last_n_checkpoints', 5)  # ä¿ç•™æœ€è¿‘Nä¸ªæ£€æŸ¥ç‚¹
    self.enable_compression = self.config.get('enable_compression', True)  # å¯ç”¨å‹ç¼©
    self.compression_threshold_mb = self.config.get('compression_threshold_mb', 100)  # å‹ç¼©é˜ˆå€¼ï¼ˆMBï¼‰
    self.last_time_checkpoint = time.time()  # ä¸Šæ¬¡æ—¶é—´æ£€æŸ¥ç‚¹æ—¶é—´

    _ = logger.info("å¢å¼ºçš„æ£€æŸ¥ç‚¹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def should_save_checkpoint(self, epoch: int, metrics: Dict[...]
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿å­˜æ£€æŸ¥ç‚¹"""
    context = ErrorContext("EnhancedCheckpointManager", "should_save_checkpoint", {"epoch": epoch, "task_id": task_id})
        try:

            should_save = False
            checkpoint_type = "regular"
            reasons = []

            # æ ¹æ®ç­–ç•¥åˆ¤æ–­
            if self.strategy == 'epoch_only':
                # ä»…åœ¨epoché—´éš”æ—¶ä¿å­˜
                if epoch % self.epoch_interval == 0:

    should_save = True
                    checkpoint_type = "epoch"
                    _ = reasons.append(f"Epoch {epoch} æ˜¯ {self.epoch_interval} çš„å€æ•°")

            elif self.strategy == 'time_based':
                # ä»…åœ¨æ—¶é—´é—´éš”æ—¶ä¿å­˜
                current_time = time.time()
                if (current_time - self.last_time_checkpoint) >= (self.time_interval_minutes * 60):

    should_save = True
                    checkpoint_type = "time_based"
                    _ = reasons.append(f"è·ç¦»ä¸Šæ¬¡æ£€æŸ¥ç‚¹å·²è¶…è¿‡ {self.time_interval_minutes} åˆ†é’Ÿ")
                    self.last_time_checkpoint = current_time

            elif self.strategy == 'hybrid':
                # æ··åˆç­–ç•¥ï¼šepoché—´éš”æˆ–æ—¶é—´é—´éš”æ—¶ä¿å­˜
                epoch_checkpoint = epoch % self.epoch_interval == 0
                time_checkpoint = False
                current_time = time.time()
                if (current_time - self.last_time_checkpoint) >= (self.time_interval_minutes * 60):

    time_checkpoint = True
                    self.last_time_checkpoint = current_time

                if epoch_checkpoint or time_checkpoint:


    should_save = True
                    if epoch_checkpoint and time_checkpoint:

    checkpoint_type = "epoch_and_time"
                        _ = reasons.append(f"Epoch {epoch} æ˜¯ {self.epoch_interval} çš„å€æ•°ä¸”æ—¶é—´é—´éš”å·²åˆ°")
                    elif epoch_checkpoint:

    checkpoint_type = "epoch"
                        _ = reasons.append(f"Epoch {epoch} æ˜¯ {self.epoch_interval} çš„å€æ•°")
                    else:

                        checkpoint_type = "time_based"
                        _ = reasons.append(f"è·ç¦»ä¸Šæ¬¡æ£€æŸ¥ç‚¹å·²è¶…è¿‡ {self.time_interval_minutes} åˆ†é’Ÿ")

            # ç‰¹æ®Šäº‹ä»¶è§¦å‘æ£€æŸ¥ç‚¹ï¼ˆä¾‹å¦‚éªŒè¯æŸå¤±æ”¹å–„ï¼‰
            if metrics and 'val_loss' in metrics:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾è‘—æ”¹å–„
                if self._should_save_for_improvement(metrics, task_id)

    should_save = True
                    checkpoint_type = "event_triggered"
                    _ = reasons.append("éªŒè¯æŸå¤±æ˜¾è‘—æ”¹å–„")

            return {
                'should_save': should_save,
                'checkpoint_type': checkpoint_type,
                'reasons': reasons
            }

        except Exception as e:


            _ = self.error_handler.handle_error(e, context)
            _ = logger.error(f"åˆ¤æ–­æ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return {'should_save': False, 'checkpoint_type': 'regular', 'reasons': []}

    def _should_save_for_improvement(self, metrics: Dict[str, Any], task_id: str = None) -> bool:
    """åˆ¤æ–­æ˜¯å¦å› æ”¹å–„è€Œä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            # è·å–è¯¥ä»»åŠ¡çš„æœ€æ–°æ£€æŸ¥ç‚¹
            task_checkpoints = [cp for cp in self.checkpoint_history if cp.task_id == task_id] if task_id else self.checkpoint_history:
    if not task_checkpoints:

    return True  # å¦‚æœæ²¡æœ‰ä¹‹å‰çš„æ£€æŸ¥ç‚¹ï¼Œä¿å­˜ç¬¬ä¸€ä¸ª

            # è·å–æœ€æ–°çš„æ£€æŸ¥ç‚¹
            latest_checkpoint = max(task_checkpoints, key=lambda x: x.timestamp)

            # æ¯”è¾ƒéªŒè¯æŸå¤±
            current_val_loss = metrics.get('val_loss', float('inf'))
            previous_val_loss = latest_checkpoint.metrics.get('val_loss', float('inf'))

            # å¦‚æœéªŒè¯æŸå¤±æ”¹å–„è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™ä¿å­˜æ£€æŸ¥ç‚¹
            improvement_threshold = 0.05  # 5%çš„æ”¹å–„
            if previous_val_loss != float('inf') and current_val_loss < previous_val_loss * (1 - improvement_threshold):

    return True

            return False
        except Exception as e:

            _ = logger.warning(f"æ£€æŸ¥æ”¹å–„æƒ…å†µæ—¶å‡ºé”™: {e}")
            return False

    def save_checkpoint(self, state: Dict[...]
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    context = ErrorContext("EnhancedCheckpointManager", "save_checkpoint", {"task_id": task_id, "checkpoint_type": checkpoint_type})
        try:
            # ç”Ÿæˆæ£€æŸ¥ç‚¹ID
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            checkpoint_id = f"ckpt_{task_id or 'default'}_{timestamp}_{int(time.time())}"

            # åˆ›å»ºæ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            checkpoint_filename = f"{checkpoint_id}.json"
            checkpoint_path = CHECKPOINTS_DIR / checkpoint_filename

            # å‡†å¤‡æ£€æŸ¥ç‚¹æ•°æ®
            checkpoint_data = {
                'checkpoint_id': checkpoint_id,
                'task_id': task_id or 'default',
                _ = 'epoch': state.get('epoch', 0),
                _ = 'timestamp': time.time(),
                _ = 'metrics': state.get('metrics', {}),
                _ = 'model_state': state.get('model_state', {}),
                _ = 'optimizer_state': state.get('optimizer_state', {}),
                _ = 'config': state.get('config', {}),
                _ = 'additional_data': state.get('additional_data', {})
            }

            # å‹ç¼©å¤§æ•°æ®
            if self.enable_compression:

    checkpoint_data = self._compress_checkpoint_data(checkpoint_data)

            # ä¿å­˜æ£€æŸ¥ç‚¹åˆ°æ–‡ä»¶
            with open(checkpoint_path, 'w', encoding='utf-8') as f:
    json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)

            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(checkpoint_path)

            # åˆ›å»ºæ£€æŸ¥ç‚¹ä¿¡æ¯
            checkpoint_info = CheckpointInfo(
                checkpoint_id=checkpoint_id,
                task_id=task_id or 'default',
                epoch=state.get('epoch', 0),
                timestamp=time.time(),
                file_path=str(checkpoint_path),
                metrics=state.get('metrics', {}),
                checkpoint_type=checkpoint_type,
                size_bytes=file_size,
                is_compressed=self.enable_compression
            )

            # è®°å½•æ£€æŸ¥ç‚¹
            self.checkpoints[checkpoint_id] = checkpoint_info
            _ = self.checkpoint_history.append(checkpoint_info)

            _ = logger.info(f"ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_id} (ç±»å‹: {checkpoint_type}, å¤§å°: {file_size} å­—èŠ‚)")

            # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
            _ = self._cleanup_old_checkpoints(task_id)

            return checkpoint_id

        except Exception as e:


            _ = self.error_handler.handle_error(e, context)
            _ = logger.error(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return None

    def _compress_checkpoint_data(self, checkpoint_data: Dict[...]
    """å‹ç¼©æ£€æŸ¥ç‚¹æ•°æ®"""
    # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„æ•°æ®å‹ç¼©é€»è¾‘
    # ä¸ºäº†ç¤ºä¾‹ï¼Œæˆ‘ä»¬åªæ˜¯æ ‡è®°æ•°æ®å·²è¢«å¤„ç†
    checkpoint_data['_compressed'] = True
    return checkpoint_data

    def load_checkpoint(self, checkpoint_id: str = None, task_id: str = None) -> Optional[Dict[str, Any]]:
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    context = ErrorContext("EnhancedCheckpointManager", "load_checkpoint", {"checkpoint_id": checkpoint_id, "task_id": task_id})
        try:
            # ç¡®å®šè¦åŠ è½½çš„æ£€æŸ¥ç‚¹
            target_checkpoint = None

            if checkpoint_id:
                # æ ¹æ®IDåŠ è½½ç‰¹å®šæ£€æŸ¥ç‚¹
                target_checkpoint = self.checkpoints.get(checkpoint_id)
            elif task_id:
                # åŠ è½½æŒ‡å®šä»»åŠ¡çš„æœ€æ–°æ£€æŸ¥ç‚¹
                task_checkpoints = [cp for cp in self.checkpoint_history if cp.task_id == task_id]:
    if task_checkpoints:

    target_checkpoint = max(task_checkpoints, key=lambda x: x.timestamp)
            else:
                # åŠ è½½æœ€æ–°çš„æ£€æŸ¥ç‚¹
                if self.checkpoint_history:

    target_checkpoint = max(self.checkpoint_history, key=lambda x: x.timestamp)

            if not target_checkpoint:


    _ = logger.warning("æœªæ‰¾åˆ°è¦åŠ è½½çš„æ£€æŸ¥ç‚¹")
                return None

            # è¯»å–æ£€æŸ¥ç‚¹æ–‡ä»¶
            checkpoint_path = Path(target_checkpoint.file_path)
            if not checkpoint_path.exists()

    _ = logger.error(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
                return None

            with open(checkpoint_path, 'r', encoding='utf-8') as f:
    checkpoint_data = json.load(f)

            # è§£å‹ç¼©æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if checkpoint_data.get('_compressed')

    checkpoint_data = self._decompress_checkpoint_data(checkpoint_data)

            _ = logger.info(f"åŠ è½½æ£€æŸ¥ç‚¹: {target_checkpoint.checkpoint_id}")
            return checkpoint_data

        except Exception as e:


            _ = self.error_handler.handle_error(e, context)
            _ = logger.error(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return None

    def _decompress_checkpoint_data(self, checkpoint_data: Dict[...]
    """è§£å‹ç¼©æ£€æŸ¥ç‚¹æ•°æ®"""
    # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„æ•°æ®è§£å‹ç¼©é€»è¾‘
    # ä¸ºäº†ç¤ºä¾‹ï¼Œæˆ‘ä»¬åªæ˜¯ç§»é™¤å‹ç¼©æ ‡è®°
    _ = checkpoint_data.pop('_compressed', None)
    return checkpoint_data

    def _cleanup_old_checkpoints(self, task_id: str = None)
    """æ¸…ç†æ—§æ£€æŸ¥ç‚¹"""
    context = ErrorContext("EnhancedCheckpointManager", "_cleanup_old_checkpoints", {"task_id": task_id})
        try:
            # è·å–ç‰¹å®šä»»åŠ¡çš„æ£€æŸ¥ç‚¹æˆ–æ‰€æœ‰æ£€æŸ¥ç‚¹
            if task_id:

    task_checkpoints = [cp for cp in self.checkpoint_history if cp.task_id == task_id]:
    else:

    task_checkpoints = self.checkpoint_history

            # å¦‚æœæ£€æŸ¥ç‚¹æ•°é‡è¶…è¿‡ä¿ç•™æ•°é‡ï¼Œåˆ é™¤æœ€æ—§çš„
            if len(task_checkpoints) > self.keep_last_n_checkpoints:
                # æŒ‰æ—¶é—´æ’åº
                sorted_checkpoints = sorted(task_checkpoints, key=lambda x: x.timestamp)
                # ç¡®å®šè¦åˆ é™¤çš„æ£€æŸ¥ç‚¹
                checkpoints_to_remove = sorted_checkpoints[:-self.keep_last_n_checkpoints]

                # åˆ é™¤æ–‡ä»¶å’Œè®°å½•
                for checkpoint_info in checkpoints_to_remove:

    try:
                        # åˆ é™¤æ–‡ä»¶
                        checkpoint_path = Path(checkpoint_info.file_path)
                        if checkpoint_path.exists()

    _ = checkpoint_path.unlink()
                            _ = logger.info(f"åˆ é™¤æ—§æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_path}")

                        # ä»è®°å½•ä¸­ç§»é™¤
                        if checkpoint_info.checkpoint_id in self.checkpoints:

    del self.checkpoints[checkpoint_info.checkpoint_id]

                        # ä»å†å²è®°å½•ä¸­ç§»é™¤
                        if checkpoint_info in self.checkpoint_history:

    _ = self.checkpoint_history.remove(checkpoint_info)
                    except Exception as e:

                        _ = logger.warning(f"åˆ é™¤æ£€æŸ¥ç‚¹å¤±è´¥: {checkpoint_info.checkpoint_id} - {e}")

                _ = logger.info(f"æ¸…ç†äº† {len(checkpoints_to_remove)} ä¸ªæ—§æ£€æŸ¥ç‚¹")

        except Exception as e:


            _ = self.error_handler.handle_error(e, context)
            _ = logger.error(f"æ¸…ç†æ—§æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

    def get_checkpoint_info(self, checkpoint_id: str = None, task_id: str = None) -> Dict[str, Any]:
    """è·å–æ£€æŸ¥ç‚¹ä¿¡æ¯"""
    context = ErrorContext("EnhancedCheckpointManager", "get_checkpoint_info", {"checkpoint_id": checkpoint_id, "task_id": task_id})
        try:

            if checkpoint_id:


    if checkpoint_id in self.checkpoints:
    return asdict(self.checkpoints[checkpoint_id])
                else:

                    return {}

            # è¿”å›ä»»åŠ¡çš„æ£€æŸ¥ç‚¹ä¿¡æ¯æˆ–æ‰€æœ‰æ£€æŸ¥ç‚¹ä¿¡æ¯
            if task_id:

    task_checkpoints = [cp for cp in self.checkpoint_history if cp.task_id == task_id]:
    return {
                    'task_id': task_id,
                    _ = 'total_checkpoints': len(task_checkpoints),
                    'checkpoints': [asdict(cp) for cp in task_checkpoints]
                }
            else:

    return {
                    _ = 'total_checkpoints': len(self.checkpoints),
                    'checkpoints': [asdict(cp) for cp in self.checkpoint_history]
                }

        except Exception as e:


    _ = self.error_handler.handle_error(e, context)
            _ = logger.error(f"è·å–æ£€æŸ¥ç‚¹ä¿¡æ¯å¤±è´¥: {e}")
            return {}

# å…¨å±€æ£€æŸ¥ç‚¹ç®¡ç†å™¨å®ä¾‹
global_checkpoint_manager = EnhancedCheckpointManager()

def main() -> None:
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    _ = print("ğŸ”¬ æµ‹è¯•å¢å¼ºçš„æ£€æŸ¥ç‚¹ç®¡ç†å™¨...")

    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)

    # åˆ›å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨å®ä¾‹
    config = {
    'strategy': 'hybrid',
    'epoch_interval': 2,
    'time_interval_minutes': 10,
    'keep_last_n_checkpoints': 3,
    'enable_compression': True
    }
    manager = EnhancedCheckpointManager(config)

    # æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜åˆ¤æ–­
    _ = print("æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜åˆ¤æ–­...")
    decision = manager.should_save_checkpoint(5, {'val_loss': 0.5}, 'test_task')
    _ = print(f"æ£€æŸ¥ç‚¹å†³ç­–: {decision}")

    # æµ‹è¯•ä¿å­˜æ£€æŸ¥ç‚¹
    _ = print("\næµ‹è¯•ä¿å­˜æ£€æŸ¥ç‚¹...")
    state = {
    'epoch': 5,
    'metrics': {'loss': 0.45, 'accuracy': 0.82, 'val_loss': 0.5},
    'model_state': {'layer1': [0.1, 0.2, 0.3]},
    'optimizer_state': {'lr': 0.001},
    'config': {'batch_size': 32, 'epochs': 10}
    }

    checkpoint_id = manager.save_checkpoint(state, 'test_task', 'epoch')
    _ = print(f"ä¿å­˜æ£€æŸ¥ç‚¹ID: {checkpoint_id}")

    # æµ‹è¯•åŠ è½½æ£€æŸ¥ç‚¹
    _ = print("\næµ‹è¯•åŠ è½½æ£€æŸ¥ç‚¹...")
    loaded_state = manager.load_checkpoint(checkpoint_id)
    if loaded_state:

    _ = print(f"åŠ è½½çš„æ£€æŸ¥ç‚¹epoch: {loaded_state.get('epoch')}")
    _ = print(f"åŠ è½½çš„æ£€æŸ¥ç‚¹metrics: {loaded_state.get('metrics')}")

    # æ˜¾ç¤ºæ£€æŸ¥ç‚¹ä¿¡æ¯
    _ = print("\næ£€æŸ¥ç‚¹ä¿¡æ¯:")
    info = manager.get_checkpoint_info(task_id='test_task')
    print(json.dumps(info, indent=2, ensure_ascii=False))

if __name__ == "__main__":


    _ = main()