#!/usr/bin/env python3
"""
åˆ†å¸ƒå¼è®­ç»ƒå®¹é”™æœºåˆ¶ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•åœ¨å®é™…é¡¹ç›®ä¸­ä½¿ç”¨å¢å¼ºçš„åˆ†å¸ƒå¼è®­ç»ƒå®¹é”™æœºåˆ¶
"""

import asyncio
import logging
import time
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
from pathlib import Path
project_root: str = Path(__file__).parent.parent.parent
_ = sys.path.insert(0, str(project_root))

from training.enhanced_distributed_training_fault_tolerance import global_enhanced_fault_tolerance

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level: str=logging.INFO,
    format: str='%(asctime)s - %(levelname)s - %(message)s'
)
logger: Any = logging.getLogger(__name__)

class DistributedTrainingExample:
    """åˆ†å¸ƒå¼è®­ç»ƒç¤ºä¾‹"""
    
    def __init__(self) -> None:
        self.fault_tolerance = global_enhanced_fault_tolerance
        self.is_training = False
        self.training_tasks = {}
    
    async def setup_training_environment(self):
        """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
        _ = logger.info("è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ...")
        
        # åˆå§‹åŒ–å®¹é”™æœºåˆ¶
        config = {
            'enabled': True,
            'auto_recovery_enabled': True,
            'checkpoint_interval': 120,  # 2åˆ†é’Ÿ
            'health_check_interval': 30,  # 30ç§’
            'distributed_optimizer': {
                'monitoring_interval': 15
            },
            'task_migrator': {
                'max_retry_attempts': 3,
                'migration_strategy': 'load_balanced'
            }
        }
        
        # é‡æ–°åˆå§‹åŒ–å®¹é”™æœºåˆ¶ï¼ˆåœ¨å®é™…é¡¹ç›®ä¸­å¯èƒ½ä¸éœ€è¦ï¼‰
        # è¿™é‡Œåªæ˜¯ä¸ºäº†æ¼”ç¤ºç›®çš„
        self.fault_tolerance = global_enhanced_fault_tolerance
        self.fault_tolerance.config = config
        
        # åˆå§‹åŒ–ç»„ä»¶
        _ = await self.fault_tolerance.initialize_components()
        
        _ = logger.info("è®­ç»ƒç¯å¢ƒè®¾ç½®å®Œæˆ")
    
    async def register_training_nodes(self):
        """æ³¨å†Œè®­ç»ƒèŠ‚ç‚¹"""
        _ = logger.info("æ³¨å†Œè®­ç»ƒèŠ‚ç‚¹...")
        
        # æ³¨å†Œå¤šä¸ªè®­ç»ƒèŠ‚ç‚¹
        nodes = [
            _ = ('node_gpu_1', {'cpu_cores': 16, 'memory_gb': 64, 'gpu_count': 2}),
            _ = ('node_gpu_2', {'cpu_cores': 32, 'memory_gb': 128, 'gpu_count': 4}),
            _ = ('node_cpu_1', {'cpu_cores': 32, 'memory_gb': 64, 'gpu_count': 0}),
            _ = ('node_cpu_2', {'cpu_cores': 64, 'memory_gb': 128, 'gpu_count': 0})
        ]
        
        for node_id, node_info in nodes:
            success = await self.fault_tolerance.register_training_node(node_id, node_info)
            if success:
                _ = logger.info(f"âœ… æˆåŠŸæ³¨å†ŒèŠ‚ç‚¹: {node_id}")
            else:
                _ = logger.error(f"âŒ æ³¨å†ŒèŠ‚ç‚¹å¤±è´¥: {node_id}")
    
    async def simulate_training_process(self):
        """æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹"""
        _ = logger.info("å¼€å§‹æ¨¡æ‹Ÿåˆ†å¸ƒå¼è®­ç»ƒè¿‡ç¨‹...")
        self.is_training = True
        
        # å¯åŠ¨å®¹é”™ç›‘æ§
        _ = await self.fault_tolerance.start_monitoring()
        
        # åˆ›å»ºå¤šä¸ªè®­ç»ƒä»»åŠ¡
        training_tasks = [
            {'task_id': 'vision_model_training', 'model_type': 'vision', 'epochs': 20},
            {'task_id': 'nlp_model_training', 'model_type': 'nlp', 'epochs': 15},
            {'task_id': 'audio_model_training', 'model_type': 'audio', 'epochs': 10}
        ]
        
        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ†é…èŠ‚ç‚¹å¹¶å¼€å§‹è®­ç»ƒ
        for task_info in training_tasks:
            task_id = task_info['task_id']
            self.training_tasks[task_id] = task_info
            
            # æ¨¡æ‹Ÿä»»åŠ¡åˆ†é…
            _ = logger.info(f"åˆ†é…ä»»åŠ¡ {task_id} åˆ°èŠ‚ç‚¹...")
            
            # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
            _ = await self._simulate_task_training(task_info)
        
        _ = logger.info("æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹å®Œæˆ")
    
    async def _simulate_task_training(self, task_info: Dict[str, Any]):
        """æ¨¡æ‹Ÿå•ä¸ªä»»åŠ¡çš„è®­ç»ƒè¿‡ç¨‹"""
        task_id = task_info['task_id']
        epochs = task_info['epochs']
        
        _ = logger.info(f"å¼€å§‹è®­ç»ƒä»»åŠ¡: {task_id} ({epochs} epochs)")
        
        for epoch in range(1, epochs + 1):
            if not self.is_training:
                break
            
            # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´
            _ = await asyncio.sleep(1)
            
            # æ¨¡æ‹Ÿè®­ç»ƒè¿›åº¦
            progress = (epoch / epochs) * 100
            loss = max(0.01, 1.0 - (epoch / epochs) * 0.9)
            accuracy = min(0.99, 0.1 + (epoch / epochs) * 0.8)
            
            # åˆ›å»ºè®­ç»ƒçŠ¶æ€
            training_state = {
                'model_name': task_id,
                'current_epoch': epoch,
                'total_epochs': epochs,
                'metrics': {
                    _ = 'loss': round(loss, 4),
                    _ = 'accuracy': round(accuracy, 4),
                    _ = 'val_loss': round(loss * 1.1, 4),
                    _ = 'val_accuracy': round(accuracy * 0.95, 4)
                },
                'model_state': {},  # å®é™…é¡¹ç›®ä¸­ä¼šåŒ…å«æ¨¡å‹çŠ¶æ€
                'optimizer_state': {},  # å®é™…é¡¹ç›®ä¸­ä¼šåŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€
                'learning_rate': 0.001,
                'batch_size': 32,
                _ = 'progress': round(progress, 2),
                _ = 'start_time': time.time() - (epoch * 10),  # æ¨¡æ‹Ÿå¼€å§‹æ—¶é—´
                'config': {
                    'batch_size': 32,
                    'epochs': epochs,
                    'learning_rate': 0.001
                }
            }
            
            # ä¿å­˜è®­ç»ƒçŠ¶æ€
            _ = await self.fault_tolerance.save_training_state(task_id, training_state)
            
            # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
            if epoch % 5 == 0:
                checkpoint_type = 'epoch' if epoch % 10 == 0 else 'regular'
                checkpoint_id = await self.fault_tolerance.save_training_checkpoint(
                    task_id, training_state, checkpoint_type)
                if checkpoint_id:
                    _ = logger.info(f"   ğŸ’¾ ä»»åŠ¡ {task_id} çš„æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_id}")
            
            # æ¨¡æ‹ŸèŠ‚ç‚¹å¿ƒè·³æ›´æ–°
            if epoch % 3 == 0:
                # éšæœºé€‰æ‹©ä¸€ä¸ªèŠ‚ç‚¹å‘é€å¿ƒè·³
                import random
                node_id = random.choice(['node_gpu_1', 'node_gpu_2', 'node_cpu_1', 'node_cpu_2'])
                metrics = {
                    _ = 'cpu_usage': random.uniform(30, 80),
                    _ = 'memory_usage': random.uniform(40, 70),
                    'gpu_usage': random.uniform(20, 90) if 'gpu' in node_id else 0
                }
                _ = await self.fault_tolerance.handle_node_heartbeat(node_id, metrics)
            
            logger.info(f"   ğŸ§  {task_id} - Epoch {epoch}/{epochs} - "
                       f"Progress: {progress:.1f}% - "
                       f"Loss: {loss:.4f} - "
                       f"Accuracy: {accuracy:.4f}")
        
        _ = logger.info(f"âœ… ä»»åŠ¡ {task_id} è®­ç»ƒå®Œæˆ")
    
    async def simulate_node_failure_and_recovery(self):
        """æ¨¡æ‹ŸèŠ‚ç‚¹æ•…éšœå’Œæ¢å¤"""
        _ = logger.info("æ¨¡æ‹ŸèŠ‚ç‚¹æ•…éšœå’Œè‡ªåŠ¨æ¢å¤...")
        
        # æ¨¡æ‹Ÿnode_gpu_1æ•…éšœ
        _ = logger.warning("ğŸš¨ æ¨¡æ‹ŸèŠ‚ç‚¹ node_gpu_1 æ•…éšœ")
        
        # åœæ­¢å‘é€è¯¥èŠ‚ç‚¹çš„å¿ƒè·³ï¼Œæ¨¡æ‹Ÿæ•…éšœ
        # åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œæ•…éšœæ£€æµ‹å™¨ä¼šè‡ªåŠ¨æ£€æµ‹åˆ°è¿™ç§æƒ…å†µ
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©æ•…éšœæ£€æµ‹å™¨æ£€æµ‹åˆ°æ•…éšœ
        _ = await asyncio.sleep(35)  # è¶…è¿‡é»˜è®¤çš„30ç§’è¶…æ—¶æ—¶é—´
        
        # æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€
        system_status = self.fault_tolerance.get_system_status()
        failed_nodes = system_status['components']['fault_detector'].get('failed_nodes', 0)
        _ = logger.info(f"æ£€æµ‹åˆ° {failed_nodes} ä¸ªæ•…éšœèŠ‚ç‚¹")
        
        # å¦‚æœå¯ç”¨äº†è‡ªåŠ¨æ¢å¤ï¼Œä»»åŠ¡è¿ç§»å™¨ä¼šè‡ªåŠ¨è¿ç§»ä»»åŠ¡
        # åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œè¿™é‡Œä¼šçœ‹åˆ°ä»»åŠ¡è¿ç§»çš„æ—¥å¿—
        
        _ = logger.info("æ¨¡æ‹Ÿæ•…éšœæ¢å¤è¿‡ç¨‹å®Œæˆ")
    
    async def show_system_status(self):
        """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
        _ = logger.info("è·å–ç³»ç»ŸçŠ¶æ€...")
        
        status = self.fault_tolerance.get_system_status()
        
        print("\n" + "="*60)
        _ = print("ğŸ“Š åˆ†å¸ƒå¼è®­ç»ƒç³»ç»ŸçŠ¶æ€æŠ¥å‘Š")
        print("="*60)
        _ = print(f"æ—¶é—´: {status['timestamp']}")
        _ = print(f"å®¹é”™æœºåˆ¶å¯ç”¨: {status['enabled']}")
        _ = print(f"è‡ªåŠ¨æ¢å¤å¯ç”¨: {status['auto_recovery_enabled']}")
        _ = print(f"ç›‘æ§è¿è¡Œä¸­: {status['is_running']}")
        
        # æ˜¾ç¤ºç»„ä»¶çŠ¶æ€
        components = status['components']
        _ = print(f"\nğŸ”§ ç»„ä»¶çŠ¶æ€:")
        _ = print(f"   æ£€æŸ¥ç‚¹ç®¡ç†å™¨: {components['checkpoint_manager']['total_checkpoints']} ä¸ªæ£€æŸ¥ç‚¹")
        _ = print(f"   çŠ¶æ€ç®¡ç†å™¨: {components['state_manager']['total_states']} ä¸ªçŠ¶æ€")
        
        # æ˜¾ç¤ºé›†ç¾¤çŠ¶æ€
        cluster_status = components['fault_detector']
        _ = print(f"\nğŸ–¥ï¸  é›†ç¾¤çŠ¶æ€:")
        _ = print(f"   æ€»èŠ‚ç‚¹æ•°: {cluster_status['total_nodes']}")
        _ = print(f"   å¥åº·èŠ‚ç‚¹: {cluster_status['healthy_nodes']}")
        _ = print(f"   è­¦å‘ŠèŠ‚ç‚¹: {cluster_status['warning_nodes']}")
        _ = print(f"   å±é™©èŠ‚ç‚¹: {cluster_status['critical_nodes']}")
        _ = print(f"   æ•…éšœèŠ‚ç‚¹: {cluster_status['failed_nodes']}")
        
        # æ˜¾ç¤ºèŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
        if cluster_status['nodes']:
            _ = print(f"\nğŸ“‹ èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯:")
            for node in cluster_status['nodes']:
                status_icon = {
                    'healthy': 'âœ…',
                    'warning': 'âš ï¸',
                    'critical': 'ğŸ”´',
                    'failed': 'âŒ'
                _ = }.get(node['status'], 'â“')
                # ä¿®å¤ï¼šä½¿ç”¨node_idä½œä¸ºèŠ‚ç‚¹æ ‡è¯†
                node_id = node.get('node_id', 'unknown')
                _ = print(f"   {status_icon} {node_id}: {node['status']}")
        
        print("="*60)
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        _ = logger.info("æ¸…ç†èµ„æº...")
        
        # åœæ­¢ç›‘æ§
        _ = self.fault_tolerance.stop_monitoring()
        
        # æ³¨é”€èŠ‚ç‚¹
        nodes = ['node_gpu_1', 'node_gpu_2', 'node_cpu_1', 'node_cpu_2']
        for node_id in nodes:
            _ = await self.fault_tolerance.unregister_training_node(node_id)
        
        _ = logger.info("èµ„æºæ¸…ç†å®Œæˆ")

async def main() -> None:
    """ä¸»å‡½æ•°"""
    _ = print("ğŸš€ åˆ†å¸ƒå¼è®­ç»ƒå®¹é”™æœºåˆ¶ä½¿ç”¨ç¤ºä¾‹")
    print("="*50)
    
    # åˆ›å»ºç¤ºä¾‹å®ä¾‹
    example = DistributedTrainingExample()
    
    try:
        # 1. è®¾ç½®è®­ç»ƒç¯å¢ƒ
        _ = await example.setup_training_environment()
        
        # 2. æ³¨å†Œè®­ç»ƒèŠ‚ç‚¹
        _ = await example.register_training_nodes()
        
        # 3. æ˜¾ç¤ºåˆå§‹ç³»ç»ŸçŠ¶æ€
        _ = await example.show_system_status()
        
        # 4. æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        _ = print("\nğŸƒ å¼€å§‹æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹...")
        _ = await example.simulate_training_process()
        
        # 5. æ¨¡æ‹ŸèŠ‚ç‚¹æ•…éšœå’Œæ¢å¤
        _ = await example.simulate_node_failure_and_recovery()
        
        # 6. æ˜¾ç¤ºæœ€ç»ˆç³»ç»ŸçŠ¶æ€
        _ = await example.show_system_status()
        
        # 7. æ¸…ç†èµ„æº
        _ = await example.cleanup()
        
        _ = print("\nğŸ‰ ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
        
    except Exception as e:
        _ = logger.error(f"ç¤ºä¾‹è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        # ç¡®ä¿å³ä½¿å‡ºé”™ä¹Ÿæ¸…ç†èµ„æº
        _ = await example.cleanup()

if __name__ == "__main__":
    _ = asyncio.run(main())