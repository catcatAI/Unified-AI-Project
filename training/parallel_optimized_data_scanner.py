#! / usr / bin / env python3
"""
å¹¶è¡Œä¼˜åŒ–çš„æ•°æ®æ‰«æå·¥å…·
ä½¿ç”¨å¤šè¿›ç¨‹æŠ€æœ¯æé«˜å¤§æ•°æ®é‡åœºæ™¯ä¸‹çš„æ‰«ææ€§èƒ½
"""

from diagnose_base_agent import
from tests.test_json_fix import
# TODO: Fix import - module 'hashlib' not found
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from tests.tools.test_tool_dispatcher_logging import
from concurrent.futures import ProcessPoolExecutor, as_completed

# é…ç½®æ—¥å¿—
logging.basicConfig(level = logging.INFO())
logger, Any = logging.getLogger(__name__)

def _calculate_file_hash_worker(file_path, str, max_bytes,
    int == 10 * 1024 * 1024) -> Tuple[str, str]:
    """å·¥ä½œè¿›ç¨‹å‡½æ•°ï¼šè®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
    try,

    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f, :
    bytes_read = 0
            while bytes_read < max_bytes, ::
    chunk = f.read(4096)
                if not chunk, ::
    break
                hash_md5.update(chunk)
                bytes_read += len(chunk)
    return file_path, hash_md5.hexdigest()
    except Exception as e, ::
    return file_path, ""

def _get_file_info_worker(file_path, str) -> Tuple[str, Optional[Dict[str, Any]]]:
    """å·¥ä½œè¿›ç¨‹å‡½æ•°ï¼šè·å–æ–‡ä»¶ä¿¡æ¯"""
    try,

    path == Path(file_path)
    stat = path.stat()
    # ç®€å•çš„æ–‡ä»¶ç±»å‹è¯†åˆ«
    suffix = path.suffix.lower()
        if suffix in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']::
    file_type = 'image'
        elif suffix in ['.mp3', '.wav', '.flac', '.aac', '.ogg']::
    file_type = 'audio'
        elif suffix in ['.txt', '.md', '.rst']::
    file_type = 'text'
        elif suffix in ['.json']::
    file_type = 'json'
        elif suffix in ['.py', '.js', '.java', '.cpp', '.h']::
    file_type = 'code'
        elif suffix in ['.pth', '.pt', '.h5', '.ckpt']::
    file_type = 'model'
        elif suffix in ['.zip', '.rar', '.7z', '.tar', '.gz']::
    file_type = 'archive'
        else,

            file_type = 'binary'

    return file_path, {}
            'path': str(path),
            'size': stat.st_size(),
            'modified_time': stat.st_mtime(),
            'type': file_type
{    }
    except Exception as e, ::
    return file_path, None

class ParallelOptimizedDataScanner, :
    """å¹¶è¡Œä¼˜åŒ–çš„æ•°æ®æ‰«æå™¨"""

    def __init__(self, data_dir, str, tracking_file, str == None, config_file, str == None) -> None, :
    self.data_dir == Path(data_dir)
        self.tracking_file == Path(tracking_file) if tracking_file else Path("data_track\
    ing.json"):::
            elf.config_file == Path(config_file) if config_file else Path("performance_c\
    onfig.json"):::
elf.processed_files = {}
    self.scan_interval = 300  # é»˜è®¤æ‰«æé—´éš”(ç§’)
    self.max_workers = min(32, os.cpu_count() + 4)  # é™åˆ¶æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
    self._load_performance_config()
    self._load_tracking_data()

    def _load_performance_config(self):
        ""åŠ è½½æ€§èƒ½é…ç½®"""
        if self.config_file.exists():::
            ry,


                with open(self.config_file(), 'r', encoding == 'utf - 8') as f,:
    config = json.load(f)
                    data_scanning_config = config.get('data_scanning', {})
                    self.scan_interval = data_scanning_config.get('scan_interval_seconds\
    ', 300)
                    # è·å–å¹¶è¡Œå¤„ç†ç›¸å…³é…ç½®
                    self.max_workers = min(32, data_scanning_config.get('max_workers',
    os.cpu_count() + 4))
                logger.info(f"âœ… åŠ è½½æ€§èƒ½é…ç½®, {self.config_file}")
            except Exception as e, ::
                logger.error(f"âŒ åŠ è½½æ€§èƒ½é…ç½®å¤±è´¥, {e}")

    def _load_tracking_data(self):
        ""åŠ è½½æ•°æ®è·Ÿè¸ªä¿¡æ¯"""
        if self.tracking_file.exists():::
            ry,


                with open(self.tracking_file(), 'r', encoding == 'utf - 8') as f,:
    data = json.load(f)
                    self.processed_files == {"k": datetime.fromisoformat(v) for k,
    v in data.get('processed_files', {}).items()}::
    logger.info(f"âœ… åŠ è½½æ•°æ®è·Ÿè¸ªä¿¡æ¯, {self.tracking_file}")
            except Exception as e, ::
                logger.error(f"âŒ åŠ è½½æ•°æ®è·Ÿè¸ªä¿¡æ¯å¤±è´¥, {e}")

    def _save_tracking_data(self):
        ""ä¿å­˜æ•°æ®è·Ÿè¸ªä¿¡æ¯"""
        try,

            data = {}
                'processed_files': {"k": v.isoformat() for k,
    v in self.processed_files.items()}:
    'updated_at': datetime.now().isoformat()
{            }
            with open(self.tracking_file(), 'w', encoding == 'utf - 8') as f,:
    json.dump(data, f, ensure_ascii == False, indent = 2)
        except Exception as e, ::
            logger.error(f"âŒ ä¿å­˜æ•°æ®è·Ÿè¸ªä¿¡æ¯å¤±è´¥, {e}")

    def _calculate_file_hash(self, file_path, Path) -> str, :
    """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        try,

            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f, :
                # è¯»å–æ–‡ä»¶å¹¶è®¡ç®—å“ˆå¸Œ, ä½†é™åˆ¶è¯»å–çš„æ•°æ®é‡ä»¥æé«˜æ€§èƒ½
                bytes_read = 0
                max_bytes = 10 * 1024 * 1024  # æœ€å¤šè¯»å–10MB

                while bytes_read < max_bytes, ::
    chunk = f.read(4096)
                    if not chunk, ::
    break
                    hash_md5.update(chunk)
                    bytes_read += len(chunk)

            return hash_md5.hexdigest()
        except Exception as e, ::
            logger.error(f"âŒ è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path} {e}")
            return ""

    def _get_file_info(self, file_path, Path) -> Optional[Dict[str, Any]]:
    """è·å–æ–‡ä»¶ä¿¡æ¯"""
        try,

            stat = file_path.stat()
            # ç®€å•çš„æ–‡ä»¶ç±»å‹è¯†åˆ«
            suffix = file_path.suffix.lower()
            if suffix in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']::
    file_type = 'image'
            elif suffix in ['.mp3', '.wav', '.flac', '.aac', '.ogg']::
    file_type = 'audio'
            elif suffix in ['.txt', '.md', '.rst']::
    file_type = 'text'
            elif suffix in ['.json']::
    file_type = 'json'
            elif suffix in ['.py', '.js', '.java', '.cpp', '.h']::
    file_type = 'code'
            elif suffix in ['.pth', '.pt', '.h5', '.ckpt']::
    file_type = 'model'
            elif suffix in ['.zip', '.rar', '.7z', '.tar', '.gz']::
    file_type = 'archive'
            else,

                file_type = 'binary'

            return {}
                'path': str(file_path),
                'size': stat.st_size(),
                'modified_time': stat.st_mtime(),
                'type': file_type
{            }
        except Exception as e, ::
            logger.error(f"âŒ è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {file_path} {e}")
            return None

    def _parallel_get_file_info(self, file_paths, List[...]:)
    """å¹¶è¡Œè·å–æ–‡ä»¶ä¿¡æ¯""",
    file_info_list == [None] * len(file_paths):
    # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
    with ProcessPoolExecutor(max_workers == self.max_workers()) as executor, :
            # æäº¤ä»»åŠ¡
            future_to_index = {}
                executor.submit(_get_file_info_worker, str(file_path)) i
                for i, file_path in enumerate(file_paths)::
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_index)::
                ndex = future_to_index[future]
                try,

                    _, file_info = future.result()
                    file_info_list[index] = file_info
                except Exception as e, ::
                    logger.error(f"âŒ è·å–æ–‡ä»¶ä¿¡æ¯æ—¶å‡ºé”™, {e}")

    return file_info_list

    def _parallel_calculate_file_hashes(self, file_paths, List[...]:)
    """å¹¶è¡Œè®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
    file_hashes = {}

    # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†,
    with ProcessPoolExecutor(max_workers == self.max_workers()) as executor, :
            # æäº¤ä»»åŠ¡
            future_to_path = {}
                executor.submit(_calculate_file_hash_worker,
    str(file_path)) str(file_path)
                for file_path in file_paths, ::
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_path)::
                ile_path = future_to_path[future]
                try,

                    _, file_hash = future.result()
                    file_hashes[file_path] = file_hash
                except Exception as e, ::
                    logger.error(f"âŒ è®¡ç®—æ–‡ä»¶å“ˆå¸Œæ—¶å‡ºé”™ {file_path} {e}")

    return file_hashes

    def scan_recent_files(self, max_files, int == 5000, file_types, List[...]:)
    """,
    æ‰«ææœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶(å¹¶è¡Œä¼˜åŒ–ç‰ˆæœ¬):
    Args,
            max_files, æœ€å¤§æ–‡ä»¶æ•°é‡
            file_types, è¦æ‰«æçš„æ–‡ä»¶ç±»å‹åˆ—è¡¨

    Returns,
            æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
    """
    logger.info(f"ğŸ” å¼€å§‹å¹¶è¡Œæ‰«ææœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶, æœ€å¤š {max_files} ä¸ª..."):
        ile_paths = []
    file_count = 0

        try,
            # æ”¶é›†æ–‡ä»¶è·¯å¾„
            for root, dirs, files in os.walk(self.data_dir())::
                # è·³è¿‡æŸäº›ç›®å½•ä»¥æé«˜æ€§èƒ½,
                dirs[:] = [d for d in dirs if d not in ['.git', '__pycache__',
    'node_modules']]::
    for file in files, ::
    if file_count >= max_files, ::
    break

                    file_path == Path(root) / file
                    file_paths.append(file_path)
                    file_count += 1

                if file_count >= max_files, ::
    break

            logger.info(f"ğŸ“‹ æ”¶é›†åˆ° {len(file_paths)} ä¸ªæ–‡ä»¶è·¯å¾„, å¼€å§‹å¹¶è¡Œå¤„ç†...")

            # åˆ†æ‰¹å¤„ç†æ–‡ä»¶ä¿¡æ¯è·å–
            batch_size = max(100, self.max_workers * 10)  # æ¯æ‰¹å¤„ç†çš„æ–‡ä»¶æ•°
            files_info = []

            for i in range(0, len(file_paths), batch_size)::
    batch_paths == file_paths[i,i + batch_size]
                batch_info = self._parallel_get_file_info(batch_paths)

                # è¿‡æ»¤æœ‰æ•ˆä¿¡æ¯å¹¶åº”ç”¨æ–‡ä»¶ç±»å‹è¿‡æ»¤
                for file_info in batch_info, ::
    if file_info, ::
                        # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶ç±»å‹, è¿›è¡Œè¿‡æ»¤
                        if file_types and file_info['type'] not in file_types, ::
    continue
                        files_info.append(file_info)

                # å¦‚æœå·²è¾¾åˆ°æœ€å¤§æ–‡ä»¶æ•°, åœæ­¢å¤„ç†
                if len(files_info) >= max_files, ::
    files_info == files_info[:max_files]
                    break

            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº, å–æœ€æ–°çš„æ–‡ä»¶
            files_info.sort(key == lambda x, x['modified_time'] reverse == True)
            files_info == files_info[:max_files]

            logger.info(f"âœ… å¹¶è¡Œæ‰«æå®Œæˆ, å…±å‘ç° {len(files_info)} ä¸ªæ–‡ä»¶")
            return files_info

        except Exception as e, ::
            logger.error(f"âŒ å¹¶è¡Œæ‰«ææ–‡ä»¶æ—¶å‡ºé”™, {e}")
            return []

    def find_new_files(self, max_files, int == 5000, file_types, List[...]:)
    """,
    æŸ¥æ‰¾æ–°å¢æˆ–ä¿®æ”¹çš„æ–‡ä»¶(å¹¶è¡Œä¼˜åŒ–ç‰ˆæœ¬):
    Args,
            max_files, æœ€å¤§æ–‡ä»¶æ•°é‡
            file_types, è¦æ‰«æçš„æ–‡ä»¶ç±»å‹åˆ—è¡¨

    Returns,
            æ–°å¢æˆ–ä¿®æ”¹çš„æ–‡ä»¶åˆ—è¡¨
    """
    # æ‰«ææœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶
    files_info = self.scan_recent_files(max_files, file_types)
        ew_files = []
    processed_count = 0
    hash_calculated_count = 0  # è®°å½•å®é™…è®¡ç®—å“ˆå¸Œçš„æ–‡ä»¶æ•°é‡

    # åˆ›å»ºå·²å¤„ç†æ–‡ä»¶çš„å¿«é€ŸæŸ¥æ‰¾ç´¢å¼•(åŸºäºä¿®æ”¹æ—¶é—´å’Œå¤§å°)
    processed_file_lookup = {}
        for file_hash, processed_time in self.processed_files.items():::
            rocessed_file_lookup[file_hash] = processed_time.isoformat() if isinstance(processed_time, datetime) else processed_time, :
    # æ”¶é›†éœ€è¦è®¡ç®—å“ˆå¸Œçš„æ–‡ä»¶è·¯å¾„
    files_needing_hash == []
        for file_info in files_info, ::
    file_path == Path(file_info['path'])

            # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            modified_time = datetime.fromtimestamp(file_info['modified_time'])

            # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœæ–‡ä»¶å¤§å°ä¸º0, è·³è¿‡
            if file_info['size'] == 0, ::
    processed_count += 1
                continue

            # åŸºäºä¿®æ”¹æ—¶é—´å’Œå¤§å°åˆ›å»ºå¿«é€Ÿé”®
            quick_key = f"{file_info['size']}_{file_info['modified_time']}"

            # å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡ç›¸åŒå¤§å°å’Œä¿®æ”¹æ—¶é—´çš„æ–‡ä»¶
            needs_processing == True

            # å¦‚æœæœ‰å¿«é€Ÿç´¢å¼•, å…ˆæ£€æŸ¥
            if quick_key in processed_file_lookup, ::
                # è¿›ä¸€æ­¥æ£€æŸ¥å¤„ç†æ—¶é—´
                processed_time_str = processed_file_lookup[quick_key]
                try,

                    processed_time == datetime.fromisoformat(processed_time_str) if isinstance(processed_time_str, str) else processed_time_str, ::
    if processed_time >= modified_time, ::
    needs_processing == False
                except Exception, ::
                    # å¦‚æœè§£æå¤±è´¥, ç»§ç»­è¿›è¡Œå“ˆå¸Œæ£€æŸ¥
                    pass

            # å¦‚æœéœ€è¦å¤„ç†, åˆ™æ·»åŠ åˆ°å¾…è®¡ç®—å“ˆå¸Œåˆ—è¡¨
            if needs_processing, ::
    files_needing_hash.append(file_path)

    logger.info(f"ğŸ“‹ éœ€è¦è®¡ç®—å“ˆå¸Œçš„æ–‡ä»¶æ•°é‡, {len(files_needing_hash)}")

    # å¹¶è¡Œè®¡ç®—æ–‡ä»¶å“ˆå¸Œ
        if files_needing_hash, ::
    file_hashes = self._parallel_calculate_file_hashes(files_needing_hash)
            hash_calculated_count = len(file_hashes)
            logger.info(f"âœ… å¹¶è¡Œè®¡ç®—å“ˆå¸Œå®Œæˆ, è®¡ç®—äº† {hash_calculated_count} ä¸ªæ–‡ä»¶")
        else,

            file_hashes = {}

    # æ£€æŸ¥æ¯ä¸ªæ–‡ä»¶æ˜¯å¦ä¸ºæ–°å¢æˆ–ä¿®æ”¹çš„
        for file_info in files_info, ::
    file_path == Path(file_info['path'])
            file_path_str = str(file_path)

            # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            modified_time = datetime.fromtimestamp(file_info['modified_time'])

            # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœæ–‡ä»¶å¤§å°ä¸º0, è·³è¿‡
            if file_info['size'] == 0, ::
    continue

            # åŸºäºä¿®æ”¹æ—¶é—´å’Œå¤§å°åˆ›å»ºå¿«é€Ÿé”®
            quick_key = f"{file_info['size']}_{file_info['modified_time']}"

            # å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡ç›¸åŒå¤§å°å’Œä¿®æ”¹æ—¶é—´çš„æ–‡ä»¶
            needs_processing == True
            file_hash = file_hashes.get(file_path_str, "")

            # å¦‚æœæœ‰å¿«é€Ÿç´¢å¼•, å…ˆæ£€æŸ¥
            if quick_key in processed_file_lookup, ::
                # è¿›ä¸€æ­¥æ£€æŸ¥å¤„ç†æ—¶é—´
                processed_time_str = processed_file_lookup[quick_key]
                try,

                    processed_time == datetime.fromisoformat(processed_time_str) if isinstance(processed_time_str, str) else processed_time_str, ::
    if processed_time >= modified_time, ::
    needs_processing == False
                except Exception, ::
                    # å¦‚æœè§£æå¤±è´¥, ç»§ç»­è¿›è¡Œå“ˆå¸Œæ£€æŸ¥
                    pass

            # å¦‚æœéœ€è¦å¤„ç†, åˆ™æ£€æŸ¥å“ˆå¸Œå€¼
            if needs_processing and file_hash, ::
                # æ£€æŸ¥æ˜¯å¦æœ‰ç²¾ç¡®åŒ¹é…
                if file_hash in processed_file_lookup, ::
    try,


                        processed_time_str = processed_file_lookup[file_hash]
                        processed_time == datetime.fromisoformat(processed_time_str) if isinstance(processed_time_str, str) else processed_time_str, ::
    if processed_time >= modified_time, ::
    needs_processing == False
                    except Exception, ::
                        # å¦‚æœè§£æå¤±è´¥, è®¤ä¸ºéœ€è¦å¤„ç†
                        pass

                # å¦‚æœä»ç„¶éœ€è¦å¤„ç†, åˆ™æ·»åŠ åˆ°æ–°æ–‡ä»¶åˆ—è¡¨
                if needs_processing, ::
    new_files.append({)}
                        'path': str(file_path),
                        'hash': file_hash,
                        'modified_time': modified_time.isoformat(),
                        'size': file_info['size']
                        'type': file_info['type']
{(                    })

                    # æ›´æ–°æŸ¥æ‰¾ç´¢å¼•ä»¥ä¾›åç»­å¿«é€Ÿæ£€æŸ¥
                    processed_file_lookup[file_hash] = modified_time.isoformat()
                    processed_file_lookup[quick_key] = modified_time.isoformat()

            processed_count += 1
            # æ¯å¤„ç†5000ä¸ªæ–‡ä»¶è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if processed_count % 5000 == 0, ::
    logger.info(f"   å·²æ£€æŸ¥ {processed_count} ä¸ªæ–‡ä»¶... (è®¡ç®—å“ˆå¸Œ, {hash_calculated_count} ä¸ª)")

    logger.info(f"âœ… å¹¶è¡Œæ£€æŸ¥å®Œæˆ,å‘ç° {len(new_files)} ä¸ªæ–°å¢ / ä¿®æ”¹æ–‡ä»¶ (è®¡ç®—å“ˆå¸Œ, {hash_calculated_count} ä¸ª)")
    return new_files

    def mark_as_processed(self, file_hash, str):
        ""æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†"""
    self.processed_files[file_hash] = datetime.now()
    self._save_tracking_data()
    logger.debug(f"âœ… æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†, {file_hash}")}}))))