#!/usr/bin/env python3
"""
ËµÑÊ∫êÁÆ°ÁêÜÂô®
Ë¥üË¥£ÁÆ°ÁêÜËÆ°ÁÆóËµÑÊ∫êÔºàCPU„ÄÅGPU„ÄÅÂÜÖÂ≠òÔºâÂπ∂Âä®ÊÄÅÂàÜÈÖçÁªô‰∏çÂêåÊ®°Âûã
"""

import os
import logging
import psutil
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
import json
from datetime import datetime

# Ê∑ªÂä†È°πÁõÆË∑ØÂæÑ
import sys
project_root = Path(__file__).parent.parent
backend_path = project_root / "apps" / "backend"
sys.path.insert(0, str(backend_path))
sys.path.insert(0, str(backend_path / "src"))

# ÂØºÂÖ•Ë∑ØÂæÑÈÖçÁΩÆÊ®°Âùó
try:
    from apps.backend.src.path_config import (
        PROJECT_ROOT, 
        DATA_DIR, 
        TRAINING_DIR, 
        get_data_path, 
        resolve_path
    )
except ImportError:
    # Â¶ÇÊûúË∑ØÂæÑÈÖçÁΩÆÊ®°Âùó‰∏çÂèØÁî®Ôºå‰ΩøÁî®ÈªòËÆ§Ë∑ØÂæÑÂ§ÑÁêÜ
    PROJECT_ROOT = project_root
    DATA_DIR = PROJECT_ROOT / "data"
    TRAINING_DIR = PROJECT_ROOT / "training"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ResourceManager:
    """ËµÑÊ∫êÁÆ°ÁêÜÂô®ÔºåË¥üË¥£ÁÆ°ÁêÜËÆ°ÁÆóËµÑÊ∫êÂπ∂Âä®ÊÄÅÂàÜÈÖçÁªô‰∏çÂêåÊ®°Âûã"""
    
    def __init__(self):
        self.cpu_count = psutil.cpu_count()
        self.physical_cpu_count = psutil.cpu_count(logical=False)
        self.total_memory = psutil.virtual_memory().total
        self.available_memory = psutil.virtual_memory().available
        self.gpu_info = self._detect_gpus()
        self.resource_allocation = {}  # ËÆ∞ÂΩïËµÑÊ∫êÂàÜÈÖçÊÉÖÂÜµ
        self.resource_usage_history = []  # ËµÑÊ∫ê‰ΩøÁî®ÂéÜÂè≤
        
        logger.info(f"üñ•Ô∏è  Á≥ªÁªüËµÑÊ∫ê‰ø°ÊÅØ:")
        logger.info(f"   CPUÊ†∏ÂøÉÊï∞: {self.cpu_count} (Áâ©ÁêÜÊ†∏ÂøÉ: {self.physical_cpu_count})")
        logger.info(f"   ÊÄªÂÜÖÂ≠ò: {self.total_memory / (1024**3):.2f} GB")
        logger.info(f"   GPU‰ø°ÊÅØ: {self.gpu_info}")
    
    def _detect_gpus(self) -> List[Dict[str, Any]]:
        """Ê£ÄÊµãÂèØÁî®GPU"""
        gpus = []
        
        # È¶ñÂÖàÂ∞ùËØïÊ£ÄÊµãNVIDIA GPU
        try:
            import pynvml
            pynvml.nvmlInit()
            device_count = pynvml.nvmlDeviceGetCount()
            
            for i in range(device_count):
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                name = pynvml.nvmlDeviceGetName(handle)
                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                
                gpu_info = {
                    'id': i,
                    'name': name.decode('utf-8') if isinstance(name, bytes) else name,
                    'total_memory': memory_info.total,
                    'free_memory': memory_info.free,
                    'used_memory': memory_info.used
                }
                gpus.append(gpu_info)
                
            logger.info(f"‚úÖ Ê£ÄÊµãÂà∞ {len(gpus)} ‰∏™NVIDIA GPU")
        except ImportError:
            logger.warning("‚ö†Ô∏è  Êú™ÂÆâË£ÖpynvmlÂ∫ìÔºåÊó†Ê≥ïÊ£ÄÊµãNVIDIA GPU")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Ê£ÄÊµãNVIDIA GPUÊó∂Âá∫Èîô: {e}")
        
        # Â¶ÇÊûúÊ≤°ÊúâÊ£ÄÊµãÂà∞NVIDIA GPUÔºåÂ∞ùËØïÊ£ÄÊµãÂÖ∂‰ªñGPU
        if not gpus:
            try:
                # Â∞ùËØï‰ΩøÁî®torchÊ£ÄÊµãGPU
                import torch
                if torch.cuda.is_available():
                    for i in range(torch.cuda.device_count()):
                        gpu_info = {
                            'id': i,
                            'name': torch.cuda.get_device_name(i),
                            'total_memory': torch.cuda.get_device_properties(i).total_memory,
                            'free_memory': torch.cuda.get_device_properties(i).total_memory,  # ÁÆÄÂåñÂ§ÑÁêÜ
                            'used_memory': 0
                        }
                        gpus.append(gpu_info)
                    logger.info(f"‚úÖ ÈÄöËøáPyTorchÊ£ÄÊµãÂà∞ {len(gpus)} ‰∏™GPU")
            except ImportError:
                logger.warning("‚ö†Ô∏è  Êú™ÂÆâË£ÖtorchÂ∫ìÔºåÊó†Ê≥ïÊ£ÄÊµãGPU")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  ÈÄöËøáPyTorchÊ£ÄÊµãGPUÊó∂Âá∫Èîô: {e}")
        
        return gpus
    
    def get_system_resources(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÂΩìÂâçÁ≥ªÁªüËµÑÊ∫êÁä∂ÊÄÅ"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory_info = psutil.virtual_memory()
        
        # Êõ¥Êñ∞GPU‰ø°ÊÅØ
        gpu_info = self._update_gpu_info()
        
        resources = {
            'cpu': {
                'count': self.cpu_count,
                'physical_count': self.physical_cpu_count,
                'usage_percent': cpu_percent,
                'available_cores': self.cpu_count * (100 - cpu_percent) / 100
            },
            'memory': {
                'total': memory_info.total,
                'available': memory_info.available,
                'used': memory_info.used,
                'usage_percent': memory_info.percent
            },
            'gpu': gpu_info,
            'timestamp': datetime.now().isoformat()
        }
        
        # ËÆ∞ÂΩïËµÑÊ∫ê‰ΩøÁî®ÂéÜÂè≤
        self.resource_usage_history.append(resources)
        if len(self.resource_usage_history) > 100:  # ÈôêÂà∂ÂéÜÂè≤ËÆ∞ÂΩïÊï∞Èáè
            self.resource_usage_history.pop(0)
        
        return resources
    
    def _update_gpu_info(self) -> List[Dict[str, Any]]:
        """Êõ¥Êñ∞GPU‰ø°ÊÅØ"""
        updated_gpus = []
        
        try:
            import pynvml
            for gpu in self.gpu_info:
                handle = pynvml.nvmlDeviceGetHandleByIndex(gpu['id'])
                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                
                updated_gpu = gpu.copy()
                updated_gpu['free_memory'] = memory_info.free
                updated_gpu['used_memory'] = memory_info.used
                updated_gpus.append(updated_gpu)
        except Exception:
            # Â¶ÇÊûúÊó†Ê≥ïÊõ¥Êñ∞ÔºåËøîÂõûÂéüÊúâ‰ø°ÊÅØ
            updated_gpus = self.gpu_info
        
        return updated_gpus
    
    def get_model_resource_requirements(self, model_type: str) -> Dict[str, Any]:
        """Ëé∑ÂèñÊ®°ÂûãÁöÑËµÑÊ∫êÈúÄÊ±Ç"""
        # ÂÆö‰πâ‰∏çÂêåÊ®°ÂûãÁöÑËµÑÊ∫êÈúÄÊ±Ç
        requirements = {
            'vision_service': {
                'cpu_cores': 1,  # Èôç‰ΩéCPUÈúÄÊ±Ç
                'memory_gb': 1,  # Èôç‰ΩéÂÜÖÂ≠òÈúÄÊ±Ç
                'gpu_memory_gb': 1,
                'priority': 2
            },
            'audio_service': {
                'cpu_cores': 1,
                'memory_gb': 1,
                'gpu_memory_gb': 0,  # Èü≥È¢ëÂ§ÑÁêÜÈÄöÂ∏∏‰∏çÈúÄË¶ÅGPU
                'priority': 1
            },
            'causal_reasoning_engine': {
                'cpu_cores': 1,  # Èôç‰ΩéCPUÈúÄÊ±Ç
                'memory_gb': 1,  # Èôç‰ΩéÂÜÖÂ≠òÈúÄÊ±Ç
                'gpu_memory_gb': 0,  # ÈÄªËæëÊé®ÁêÜ‰∏ªË¶Å‰ΩøÁî®CPU
                'priority': 3
            },
            'multimodal_service': {
                'cpu_cores': 1,  # Èôç‰ΩéCPUÈúÄÊ±Ç
                'memory_gb': 1,  # Èôç‰ΩéÂÜÖÂ≠òÈúÄÊ±Ç
                'gpu_memory_gb': 1,
                'priority': 4
            },
            'math_model': {
                'cpu_cores': 1,
                'memory_gb': 1,
                'gpu_memory_gb': 0,
                'priority': 1
            },
            'logic_model': {
                'cpu_cores': 1,
                'memory_gb': 1,
                'gpu_memory_gb': 0,
                'priority': 2
            },
            'concept_models': {
                'cpu_cores': 1,
                'memory_gb': 1,
                'gpu_memory_gb': 0,
                'priority': 3
            },
            'environment_simulator': {
                'cpu_cores': 1,
                'memory_gb': 1,
                'gpu_memory_gb': 0,
                'priority': 2
            },
            'adaptive_learning_controller': {
                'cpu_cores': 1,
                'memory_gb': 1,
                'gpu_memory_gb': 0,
                'priority': 3
            },
            'alpha_deep_model': {
                'cpu_cores': 1,
                'memory_gb': 1,
                'gpu_memory_gb': 0,
                'priority': 4
            }
        }
        
        return requirements.get(model_type, {
            'cpu_cores': 1,
            'memory_gb': 1,
            'gpu_memory_gb': 0,
            'priority': 1
        })
    
    def allocate_resources(self, requirements: Dict[str, Any], model_name: str = None) -> Optional[Dict[str, Any]]:
        """‰∏∫Ê®°ÂûãÂàÜÈÖçËµÑÊ∫ê"""
        if not requirements:
            return None
        
        # Ëé∑ÂèñÂΩìÂâçÁ≥ªÁªüËµÑÊ∫ê
        system_resources = self.get_system_resources()
        cpu_info = system_resources['cpu']
        memory_info = system_resources['memory']
        
        # Ê£ÄÊü•CPUËµÑÊ∫ê
        required_cpu = requirements.get('cpu_cores', 1)
        available_cpu = cpu_info['available_cores']
        
        if required_cpu > available_cpu:
            logger.warning(f"‚ö†Ô∏è  CPUËµÑÊ∫ê‰∏çË∂≥: ÈúÄË¶Å {required_cpu} Ê†∏ÂøÉÔºåÂèØÁî® {available_cpu:.2f} Ê†∏ÂøÉ")
            return None
        
        # Ê£ÄÊü•ÂÜÖÂ≠òËµÑÊ∫ê
        required_memory_gb = requirements.get('memory_gb', 1)
        available_memory_gb = memory_info['available'] / (1024**3)
        
        if required_memory_gb > available_memory_gb:
            logger.warning(f"‚ö†Ô∏è  ÂÜÖÂ≠òËµÑÊ∫ê‰∏çË∂≥: ÈúÄË¶Å {required_memory_gb:.2f} GBÔºåÂèØÁî® {available_memory_gb:.2f} GB")
            return None
        
        # Ê£ÄÊü•GPUËµÑÊ∫êÔºàÂ¶ÇÊûúÈúÄË¶ÅÔºâ
        required_gpu_memory_gb = requirements.get('gpu_memory_gb', 0)
        if required_gpu_memory_gb > 0 and self.gpu_info:
            total_gpu_memory_gb = sum(gpu['free_memory'] for gpu in self.gpu_info) / (1024**3)
            if required_gpu_memory_gb > total_gpu_memory_gb:
                logger.warning(f"‚ö†Ô∏è  GPUÂÜÖÂ≠òËµÑÊ∫ê‰∏çË∂≥: ÈúÄË¶Å {required_gpu_memory_gb:.2f} GBÔºåÂèØÁî® {total_gpu_memory_gb:.2f} GB")
                # Â¶ÇÊûúGPUÂÜÖÂ≠ò‰∏çË∂≥Ôºå‰ΩÜÊ®°ÂûãÂèØ‰ª•‰ΩøÁî®CPUËøêË°åÔºåÂàôÁªßÁª≠ÂàÜÈÖç
                required_gpu_memory_gb = 0
        
        # ÂàÜÈÖçËµÑÊ∫ê
        allocation = {
            'cpu_cores': required_cpu,
            'memory_gb': required_memory_gb,
            'gpu_memory_gb': required_gpu_memory_gb,
            'allocated_at': datetime.now().isoformat()
        }
        
        # ËÆ∞ÂΩïËµÑÊ∫êÂàÜÈÖç
        if model_name:
            self.resource_allocation[model_name] = allocation
        
        logger.info(f"‚úÖ ËµÑÊ∫êÂàÜÈÖçÊàêÂäü: CPU {required_cpu} Ê†∏ÂøÉ, ÂÜÖÂ≠ò {required_memory_gb} GB, GPU {required_gpu_memory_gb} GB")
        return allocation
    
    def release_resources(self, model_name: str):
        """ÈáäÊîæÊ®°ÂûãÂç†Áî®ÁöÑËµÑÊ∫ê"""
        if model_name in self.resource_allocation:
            del self.resource_allocation[model_name]
            logger.info(f"üîÑ ÈáäÊîæÊ®°Âûã {model_name} ÁöÑËµÑÊ∫ê")
    
    def get_resource_utilization(self) -> Dict[str, Any]:
        """Ëé∑ÂèñËµÑÊ∫êÂà©Áî®ÁéáÊä•Âëä"""
        system_resources = self.get_system_resources()
        cpu_info = system_resources['cpu']
        memory_info = system_resources['memory']
        
        utilization = {
            'cpu_utilization': {
                'used_cores': self.cpu_count - cpu_info['available_cores'],
                'total_cores': self.cpu_count,
                'utilization_percent': (self.cpu_count - cpu_info['available_cores']) / self.cpu_count * 100
            },
            'memory_utilization': {
                'used_gb': (memory_info['total'] - memory_info['available']) / (1024**3),
                'total_gb': memory_info['total'] / (1024**3),
                'utilization_percent': memory_info['percent']
            },
            'allocated_models': list(self.resource_allocation.keys())
        }
        
        return utilization