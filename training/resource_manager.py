#!/usr/bin/env python3
"""
èµ„æºç®¡ç†å™¨
è´Ÿè´£ç®¡ç†è®¡ç®—èµ„æºï¼ˆCPUã€GPUã€å†…å­˜ï¼‰å¹¶åŠ¨æ€åˆ†é…ç»™ä¸åŒæ¨¡å‹
"""

import os
import logging
import psutil
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
project_root = Path(__file__).parent.parent
backend_path = project_root / "apps" / "backend"
sys.path.insert(0, str(backend_path))
sys.path.insert(0, str(backend_path / "src"))

# å¯¼å…¥è·¯å¾„é…ç½®æ¨¡å—
try:
    from src.path_config import (
        PROJECT_ROOT, 
        DATA_DIR, 
        TRAINING_DIR, 
        get_data_path, 
        resolve_path
    )
except ImportError:
    # å¦‚æœè·¯å¾„é…ç½®æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„å¤„ç†
    PROJECT_ROOT = project_root
    DATA_DIR = PROJECT_ROOT / "data"
    TRAINING_DIR = PROJECT_ROOT / "training"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ResourceManager:
    """èµ„æºç®¡ç†å™¨ï¼Œè´Ÿè´£ç®¡ç†è®¡ç®—èµ„æºå¹¶åŠ¨æ€åˆ†é…ç»™ä¸åŒæ¨¡å‹"""
    
    def __init__(self):
        self.cpu_count = psutil.cpu_count()
        self.physical_cpu_count = psutil.cpu_count(logical=False)
        self.total_memory = psutil.virtual_memory().total
        self.available_memory = psutil.virtual_memory().available
        self.gpu_info = self._detect_gpus()
        self.resource_allocation = {}  # è®°å½•èµ„æºåˆ†é…æƒ…å†µ
        self.resource_usage_history = []  # èµ„æºä½¿ç”¨å†å²
        
        logger.info(f"ğŸ–¥ï¸  ç³»ç»Ÿèµ„æºä¿¡æ¯:")
        logger.info(f"   CPUæ ¸å¿ƒæ•°: {self.cpu_count} (ç‰©ç†æ ¸å¿ƒ: {self.physical_cpu_count})")
        logger.info(f"   æ€»å†…å­˜: {self.total_memory / (1024**3):.2f} GB")
        logger.info(f"   GPUä¿¡æ¯: {self.gpu_info}")
    
    def _detect_gpus(self) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¯ç”¨GPU"""
        gpus = []
        
        # é¦–å…ˆå°è¯•æ£€æµ‹NVIDIA GPU
        try:
            import pynvml
            pynvml.nvmlInit()
            device_count = pynvml.nvmlDeviceGetCount()
            
            for i in range(device_count):
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                name = pynvml.nvmlDeviceGetName(handle)
                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                
                gpu_info = {
                    'id': i,
                    'name': name.decode('utf-8') if isinstance(name, bytes) else name,
                    'total_memory': memory_info.total,
                    'free_memory': memory_info.free,
                    'used_memory': memory_info.used
                }
                gpus.append(gpu_info)
                
            logger.info(f"âœ… æ£€æµ‹åˆ° {len(gpus)} ä¸ªNVIDIA GPU")
        except ImportError:
            logger.warning("âš ï¸  æœªå®‰è£…pynvmlåº“ï¼Œæ— æ³•æ£€æµ‹NVIDIA GPU")
        except Exception as e:
            logger.warning(f"âš ï¸  æ£€æµ‹NVIDIA GPUæ—¶å‡ºé”™: {e}")
        
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°NVIDIA GPUï¼Œå°è¯•æ£€æµ‹å…¶ä»–GPU
        if not gpus:
            try:
                # å°è¯•ä½¿ç”¨torchæ£€æµ‹GPU
                import torch
                if torch.cuda.is_available():
                    for i in range(torch.cuda.device_count()):
                        gpu_info = {
                            'id': i,
                            'name': torch.cuda.get_device_name(i),
                            'total_memory': torch.cuda.get_device_properties(i).total_memory,
                            'free_memory': torch.cuda.get_device_properties(i).total_memory,  # ç®€åŒ–å¤„ç†
                            'used_memory': 0
                        }
                        gpus.append(gpu_info)
                    logger.info(f"âœ… é€šè¿‡PyTorchæ£€æµ‹åˆ° {len(gpus)} ä¸ªGPU")
            except ImportError:
                logger.warning("âš ï¸  æœªå®‰è£…torchåº“ï¼Œæ— æ³•æ£€æµ‹GPU")
            except Exception as e:
                logger.warning(f"âš ï¸  é€šè¿‡PyTorchæ£€æµ‹GPUæ—¶å‡ºé”™: {e}")
        
        return gpus
    
    def get_system_resources(self) -> Dict[str, Any]:
        """è·å–å½“å‰ç³»ç»Ÿèµ„æºçŠ¶æ€"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory_info = psutil.virtual_memory()
        
        # æ›´æ–°GPUä¿¡æ¯
        gpu_info = self._update_gpu_info()
        
        resources = {
            'cpu': {
                'count': self.cpu_count,
                'physical_count': self.physical_cpu_count,
                'usage_percent': cpu_percent,
                'available_cores': self.cpu_count * (100 - cpu_percent) / 100
            },
            'memory': {
                'total': memory_info.total,
                'available': memory_info.available,
                'used': memory_info.used,
                'usage_percent': memory_info.percent
            },
            'gpu': gpu_info,
            'timestamp': datetime.now().isoformat()
        }
        
        # è®°å½•èµ„æºä½¿ç”¨å†å²
        self.resource_usage_history.append(resources)
        if len(self.resource_usage_history) > 100:  # é™åˆ¶å†å²è®°å½•æ•°é‡
            self.resource_usage_history.pop(0)
        
        return resources
    
    def _update_gpu_info(self) -> List[Dict[str, Any]]:
        """æ›´æ–°GPUä¿¡æ¯"""
        updated_gpus = []
        
        try:
            import pynvml
            for gpu in self.gpu_info:
                handle = pynvml.nvmlDeviceGetHandleByIndex(gpu['id'])
                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                
                updated_gpu = gpu.copy()
                updated_gpu['free_memory'] = memory_info.free
                updated_gpu['used_memory'] = memory_info.used
                updated_gpus.append(updated_gpu)
        except Exception:
            # å¦‚æœæ— æ³•æ›´æ–°ï¼Œè¿”å›åŸæœ‰ä¿¡æ¯
            updated_gpus = self.gpu_info
        
        return updated_gpus
    
    def get_model_resource_requirements(self, model_type: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹çš„èµ„æºéœ€æ±‚"""
        # å®šä¹‰ä¸åŒæ¨¡å‹çš„èµ„æºéœ€æ±‚
        requirements = {
            'vision_service': {
                'cpu_cores': 2,
                'memory_gb': 4,
                'gpu_memory_gb': 2,
                'priority': 2
            },
            'audio_service': {
                'cpu_cores': 1,
                'memory_gb': 2,
                'gpu_memory_gb': 0,  # éŸ³é¢‘å¤„ç†é€šå¸¸ä¸éœ€è¦GPU
                'priority': 1
            },
            'causal_reasoning_engine': {
                'cpu_cores': 4,
                'memory_gb': 8,
                'gpu_memory_gb': 0,  # é€»è¾‘æ¨ç†ä¸»è¦ä½¿ç”¨CPU
                'priority': 3
            },
            'multimodal_service': {
                'cpu_cores': 4,
                'memory_gb': 8,
                'gpu_memory_gb': 4,
                'priority': 4
            },
            'math_model': {
                'cpu_cores': 1,
                'memory_gb': 1,
                'gpu_memory_gb': 0,
                'priority': 1
            },
            'logic_model': {
                'cpu_cores': 2,
                'memory_gb': 2,
                'gpu_memory_gb': 0,
                'priority': 2
            },
            'concept_models': {
                'cpu_cores': 3,
                'memory_gb': 4,
                'gpu_memory_gb': 1,
                'priority': 3
            },
            'environment_simulator': {
                'cpu_cores': 2,
                'memory_gb': 3,
                'gpu_memory_gb': 0,
                'priority': 2
            },
            'causal_reasoning_engine': {
                'cpu_cores': 3,
                'memory_gb': 4,
                'gpu_memory_gb': 0,
                'priority': 3
            },
            'adaptive_learning_controller': {
                'cpu_cores': 1,
                'memory_gb': 2,
                'gpu_memory_gb': 0,
                'priority': 1
            },
            'alpha_deep_model': {
                'cpu_cores': 2,
                'memory_gb': 3,
                'gpu_memory_gb': 1,
                'priority': 2
            }
        }
        
        return requirements.get(model_type, {
            'cpu_cores': 1,
            'memory_gb': 1,
            'gpu_memory_gb': 0,
            'priority': 1
        })
    
    def allocate_resources(self, model_requirements: Dict[str, Any], model_name: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®æ¨¡å‹éœ€æ±‚åˆ†é…èµ„æº"""
        current_resources = self.get_system_resources()
        
        required_cpu = model_requirements.get('cpu_cores', 1)
        required_memory = model_requirements.get('memory_gb', 1) * (1024**3)  # è½¬æ¢ä¸ºå­—èŠ‚
        required_gpu_memory = model_requirements.get('gpu_memory_gb', 0) * (1024**3)
        
        # æ£€æŸ¥CPUèµ„æº
        available_cpu = current_resources['cpu']['available_cores']
        if available_cpu < required_cpu:
            logger.warning(f"âš ï¸  CPUèµ„æºä¸è¶³: éœ€è¦ {required_cpu} æ ¸å¿ƒï¼Œå¯ç”¨ {available_cpu:.2f} æ ¸å¿ƒ")
            return None
        
        # æ£€æŸ¥å†…å­˜èµ„æº
        available_memory = current_resources['memory']['available']
        if available_memory < required_memory:
            logger.warning(f"âš ï¸  å†…å­˜èµ„æºä¸è¶³: éœ€è¦ {required_memory / (1024**3):.2f} GBï¼Œå¯ç”¨ {available_memory / (1024**3):.2f} GB")
            return None
        
        # æ£€æŸ¥GPUèµ„æº
        allocated_gpu = None
        if required_gpu_memory > 0 and current_resources['gpu']:
            for gpu in current_resources['gpu']:
                if gpu['free_memory'] >= required_gpu_memory:
                    allocated_gpu = gpu
                    break
        
        if required_gpu_memory > 0 and not allocated_gpu:
            logger.warning(f"âš ï¸  GPUå†…å­˜èµ„æºä¸è¶³: éœ€è¦ {required_gpu_memory / (1024**3):.2f} GB")
            # å¦‚æœæ¨¡å‹éœ€è¦GPUä½†æ²¡æœ‰å¯ç”¨GPUï¼Œå¯ä»¥è€ƒè™‘æ˜¯å¦ç»§ç»­ä½¿ç”¨CPU
        
        # åˆ†é…èµ„æº
        allocation = {
            'model_name': model_name,
            'cpu_cores': required_cpu,
            'memory_bytes': required_memory,
            'gpu': allocated_gpu,
            'allocated_at': datetime.now().isoformat()
        }
        
        # è®°å½•èµ„æºåˆ†é…
        self.resource_allocation[model_name] = allocation
        
        logger.info(f"âœ… ä¸ºæ¨¡å‹ {model_name} åˆ†é…èµ„æºæˆåŠŸ:")
        logger.info(f"   CPUæ ¸å¿ƒ: {required_cpu}")
        logger.info(f"   å†…å­˜: {required_memory / (1024**3):.2f} GB")
        if allocated_gpu:
            logger.info(f"   GPU: {allocated_gpu['name']} ({required_gpu_memory / (1024**3):.2f} GB)")
        
        return allocation
    
    def release_resources(self, model_name: str):
        """é‡Šæ”¾æ¨¡å‹å ç”¨çš„èµ„æº"""
        if model_name in self.resource_allocation:
            allocation = self.resource_allocation.pop(model_name)
            logger.info(f"ğŸ”„ é‡Šæ”¾æ¨¡å‹ {model_name} çš„èµ„æº: CPU {allocation['cpu_cores']} æ ¸å¿ƒ, å†…å­˜ {allocation['memory_bytes'] / (1024**3):.2f} GB")
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹ {model_name} çš„èµ„æºåˆ†é…è®°å½•")
    
    def get_resource_allocation_status(self) -> Dict[str, Any]:
        """è·å–èµ„æºåˆ†é…çŠ¶æ€"""
        total_allocated_cpu = sum(allocation['cpu_cores'] for allocation in self.resource_allocation.values())
        total_allocated_memory = sum(allocation['memory_bytes'] for allocation in self.resource_allocation.values())
        
        status = {
            'total_system_cpu': self.cpu_count,
            'allocated_cpu': total_allocated_cpu,
            'available_cpu': self.cpu_count - total_allocated_cpu,
            'total_system_memory_gb': self.total_memory / (1024**3),
            'allocated_memory_gb': total_allocated_memory / (1024**3),
            'available_memory_gb': (self.total_memory - total_allocated_memory) / (1024**3),
            'active_allocations': self.resource_allocation,
            'gpu_count': len(self.gpu_info)
        }
        
        return status
    
    def optimize_resource_allocation(self) -> Dict[str, Any]:
        """ä¼˜åŒ–èµ„æºåˆ†é…"""
        # è·å–å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ
        current_resources = self.get_system_resources()
        
        # æ ¹æ®ä¼˜å…ˆçº§é‡æ–°åˆ†é…èµ„æº
        sorted_allocations = sorted(
            self.resource_allocation.items(),
            key=lambda x: self.get_model_resource_requirements(x[0]).get('priority', 1),
            reverse=True
        )
        
        optimization_result = {
            'reallocations': [],
            'resource_status': self.get_resource_allocation_status(),
            'timestamp': datetime.now().isoformat()
        }
        
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„èµ„æºä¼˜åŒ–é€»è¾‘
        # ä¾‹å¦‚ï¼šæ ¹æ®æ¨¡å‹ä¼˜å…ˆçº§å’Œèµ„æºä½¿ç”¨æƒ…å†µåŠ¨æ€è°ƒæ•´èµ„æºåˆ†é…
        
        return optimization_result
    
    def monitor_resources(self) -> Dict[str, Any]:
        """ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ"""
        current_resources = self.get_system_resources()
        allocation_status = self.get_resource_allocation_status()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰èµ„æºç“¶é¢ˆ
        issues = []
        if current_resources['cpu']['usage_percent'] > 90:
            issues.append("CPUä½¿ç”¨ç‡è¿‡é«˜")
        
        if current_resources['memory']['usage_percent'] > 90:
            issues.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜")
        
        if self.gpu_info:
            for gpu in current_resources['gpu']:
                if gpu['used_memory'] / gpu['total_memory'] > 0.9:
                    issues.append(f"GPU {gpu['id']} å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜")
        
        monitoring_result = {
            'current_resources': current_resources,
            'allocation_status': allocation_status,
            'issues': issues,
            'timestamp': datetime.now().isoformat()
        }
        
        if issues:
            logger.warning(f"âš ï¸  èµ„æºç›‘æ§å‘ç°é—®é¢˜: {', '.join(issues)}")
        
        return monitoring_result
    
    def save_resource_status(self, status_path: str = None):
        """ä¿å­˜èµ„æºçŠ¶æ€åˆ°æ–‡ä»¶"""
        if not status_path:
            status_path = TRAINING_DIR / "resource_status.json"
        
        status_data = {
            'resource_allocation': self.resource_allocation,
            'resource_usage_history': self.resource_usage_history[-20:],  # åªä¿å­˜æœ€è¿‘20æ¡è®°å½•
            'generated_at': datetime.now().isoformat()
        }
        
        try:
            with open(status_path, 'w', encoding='utf-8') as f:
                json.dump(status_data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ èµ„æºçŠ¶æ€å·²ä¿å­˜åˆ°: {status_path}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜èµ„æºçŠ¶æ€å¤±è´¥: {e}")
    
    def load_resource_status(self, status_path: str = None):
        """ä»æ–‡ä»¶åŠ è½½èµ„æºçŠ¶æ€"""
        if not status_path:
            status_path = TRAINING_DIR / "resource_status.json"
        
        if not Path(status_path).exists():
            logger.warning(f"âš ï¸ èµ„æºçŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: {status_path}")
            return False
        
        try:
            with open(status_path, 'r', encoding='utf-8') as f:
                status_data = json.load(f)
            
            self.resource_allocation = status_data.get('resource_allocation', {})
            self.resource_usage_history = status_data.get('resource_usage_history', [])
            logger.info(f"âœ… èµ„æºçŠ¶æ€å·²ä» {status_path} åŠ è½½")
            return True
        except Exception as e:
            logger.error(f"âŒ åŠ è½½èµ„æºçŠ¶æ€å¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•ResourceManager"""
    print("ğŸ–¥ï¸  æµ‹è¯•èµ„æºç®¡ç†å™¨...")
    
    # åˆå§‹åŒ–èµ„æºç®¡ç†å™¨
    resource_manager = ResourceManager()
    
    # æ˜¾ç¤ºç³»ç»Ÿèµ„æº
    resources = resource_manager.get_system_resources()
    print(f"ğŸ“Š ç³»ç»Ÿèµ„æºçŠ¶æ€:")
    print(f"  CPUä½¿ç”¨ç‡: {resources['cpu']['usage_percent']:.1f}%")
    print(f"  å¯ç”¨CPUæ ¸å¿ƒ: {resources['cpu']['available_cores']:.1f}")
    print(f"  å†…å­˜ä½¿ç”¨ç‡: {resources['memory']['usage_percent']:.1f}%")
    print(f"  å¯ç”¨å†…å­˜: {resources['memory']['available'] / (1024**3):.2f} GB")
    print(f"  GPUæ•°é‡: {len(resources['gpu'])}")
    
    # æµ‹è¯•æ¨¡å‹èµ„æºåˆ†é…
    print(f"\nğŸ”§ æ¨¡å‹èµ„æºåˆ†é…æµ‹è¯•:")
    for model_type in ['vision_service', 'causal_reasoning_engine', 'multimodal_service']:
        requirements = resource_manager.get_model_resource_requirements(model_type)
        allocation = resource_manager.allocate_resources(requirements, model_type)
        if allocation:
            print(f"  {model_type}: åˆ†é…æˆåŠŸ")
        else:
            print(f"  {model_type}: åˆ†é…å¤±è´¥")
    
    # æ˜¾ç¤ºèµ„æºåˆ†é…çŠ¶æ€
    status = resource_manager.get_resource_allocation_status()
    print(f"\nğŸ“ˆ èµ„æºåˆ†é…çŠ¶æ€:")
    print(f"  å·²åˆ†é…CPU: {status['allocated_cpu']} æ ¸å¿ƒ")
    print(f"  å¯ç”¨CPU: {status['available_cpu']:.1f} æ ¸å¿ƒ")
    print(f"  å·²åˆ†é…å†…å­˜: {status['allocated_memory_gb']:.2f} GB")
    print(f"  å¯ç”¨å†…å­˜: {status['available_memory_gb']:.2f} GB")
    
    # ç›‘æ§èµ„æº
    monitoring_result = resource_manager.monitor_resources()
    if monitoring_result['issues']:
        print(f"\nâš ï¸  èµ„æºç›‘æ§å‘ç°é—®é¢˜:")
        for issue in monitoring_result['issues']:
            print(f"  - {issue}")
    else:
        print(f"\nâœ… èµ„æºç›‘æ§æ­£å¸¸")
    
    # ä¿å­˜èµ„æºçŠ¶æ€
    resource_manager.save_resource_status()
    print(f"\nâœ… èµ„æºç®¡ç†å™¨æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    main()