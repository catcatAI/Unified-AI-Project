#!/usr/bin/env python3
"""
èµ„æºç®¡ç†å™¨
è´Ÿè´£ç®¡ç†è®¡ç®—èµ„æºï¼ˆCPUã€GPUã€å†…å­˜ï¼‰å¹¶åŠ¨æ€åˆ†é…ç»™ä¸åŒæ¨¡å‹
"""

import os
import logging
import psutil
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
import json
from datetime import datetime
import heapq

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
project_root = Path(__file__).parent.parent
backend_path = project_root / "apps" / "backend"
sys.path.insert(0, str(backend_path))
sys.path.insert(0, str(backend_path / "src"))

# å¯¼å…¥è·¯å¾„é…ç½®æ¨¡å—
try:
    from apps.backend.src.path_config import (
        PROJECT_ROOT, 
        DATA_DIR, 
        TRAINING_DIR, 
        get_data_path, 
        resolve_path
    )
except ImportError:
    # å¦‚æœè·¯å¾„é…ç½®æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„å¤„ç†
    PROJECT_ROOT = project_root
    DATA_DIR = PROJECT_ROOT / "data"
    TRAINING_DIR = PROJECT_ROOT / "training"

# å¯¼å…¥æ™ºèƒ½èµ„æºåˆ†é…å™¨
from .smart_resource_allocator import SmartResourceAllocator

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ResourceManager:
    """èµ„æºç®¡ç†å™¨ï¼Œè´Ÿè´£ç®¡ç†è®¡ç®—èµ„æºå¹¶åŠ¨æ€åˆ†é…ç»™ä¸åŒæ¨¡å‹"""
    
    def __init__(self):
        self.cpu_count = psutil.cpu_count()
        self.physical_cpu_count = psutil.cpu_count(logical=False)
        self.total_memory = psutil.virtual_memory().total
        self.available_memory = psutil.virtual_memory().available
        self.gpu_info = self._detect_gpus()
        self.resource_allocation = {}  # è®°å½•èµ„æºåˆ†é…æƒ…å†µ
        self.resource_usage_history = []  # èµ„æºä½¿ç”¨å†å²
        self.task_queue = []  # ä»»åŠ¡é˜Ÿåˆ—ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        
        # æ™ºèƒ½èµ„æºåˆ†é…å™¨
        self.smart_allocator = SmartResourceAllocator()
        self.running_tasks = {}  # æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
        
        logger.info(f"ğŸ–¥ï¸  ç³»ç»Ÿèµ„æºä¿¡æ¯:")
        logger.info(f"   CPUæ ¸å¿ƒæ•°: {self.cpu_count} (ç‰©ç†æ ¸å¿ƒ: {self.physical_cpu_count})")
        logger.info(f"   æ€»å†…å­˜: {self.total_memory / (1024**3):.2f} GB")
        logger.info(f"   GPUä¿¡æ¯: {self.gpu_info}")
    
    def _detect_gpus(self) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¯ç”¨GPU"""
        gpus = []
        
        # é¦–å…ˆå°è¯•æ£€æµ‹NVIDIA GPU
        try:
            import pynvml
            pynvml.nvmlInit()
            device_count = pynvml.nvmlDeviceGetCount()
            
            for i in range(device_count):
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                name = pynvml.nvmlDeviceGetName(handle)
                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                
                gpu_info = {
                    'id': i,
                    'name': name.decode('utf-8') if isinstance(name, bytes) else name,
                    'total_memory': memory_info.total,
                    'free_memory': memory_info.free,
                    'used_memory': memory_info.used
                }
                gpus.append(gpu_info)
                
            logger.info(f"âœ… æ£€æµ‹åˆ° {len(gpus)} ä¸ªNVIDIA GPU")
        except ImportError:
            logger.warning("âš ï¸  æœªå®‰è£…pynvmlåº“ï¼Œæ— æ³•æ£€æµ‹NVIDIA GPU")
        except Exception as e:
            logger.warning(f"âš ï¸  æ£€æµ‹NVIDIA GPUæ—¶å‡ºé”™: {e}")
        
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°NVIDIA GPUï¼Œå°è¯•æ£€æµ‹å…¶ä»–GPU
        if not gpus:
            try:
                # å°è¯•ä½¿ç”¨torchæ£€æµ‹GPU
                import torch
                if torch.cuda.is_available():
                    for i in range(torch.cuda.device_count()):
                        props = torch.cuda.get_device_properties(i)
                        gpu_info = {
                            'id': i,
                            'name': torch.cuda.get_device_name(i),
                            'total_memory': props.total_memory,
                            'free_memory': props.total_memory,  # ç®€åŒ–å¤„ç†
                            'used_memory': 0
                        }
                        gpus.append(gpu_info)
                    logger.info(f"âœ… é€šè¿‡PyTorchæ£€æµ‹åˆ° {len(gpus)} ä¸ªGPU")
            except ImportError:
                logger.warning("âš ï¸  æœªå®‰è£…torchåº“ï¼Œæ— æ³•æ£€æµ‹GPU")
            except Exception as e:
                logger.warning(f"âš ï¸  é€šè¿‡PyTorchæ£€æµ‹GPUæ—¶å‡ºé”™: {e}")
        
        return gpus
    
    def get_system_resources(self) -> Dict[str, Any]:
        """è·å–å½“å‰ç³»ç»Ÿèµ„æºçŠ¶æ€"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory_info = psutil.virtual_memory()
            
            # æ›´æ–°GPUä¿¡æ¯
            gpu_info = self._update_gpu_info()
            
            resources = {
                'cpu': {
                    'count': self.cpu_count,
                    'physical_count': self.physical_cpu_count,
                    'usage_percent': cpu_percent,
                    'available_cores': self.cpu_count * (100 - cpu_percent) / 100
                },
                'memory': {
                    'total': memory_info.total,
                    'available': memory_info.available,
                    'used': memory_info.used,
                    'usage_percent': memory_info.percent
                },
                'gpu': gpu_info,
                'timestamp': datetime.now().isoformat()
            }
            
            # è®°å½•èµ„æºä½¿ç”¨å†å²
            self.resource_usage_history.append(resources)
            if len(self.resource_usage_history) > 100:  # é™åˆ¶å†å²è®°å½•æ•°é‡
                self.resource_usage_history.pop(0)
            
            return resources
        except Exception as e:
            logger.error(f"âŒ è·å–ç³»ç»Ÿèµ„æºä¿¡æ¯å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤èµ„æºä¿¡æ¯
            return {
                'cpu': {
                    'count': self.cpu_count,
                    'physical_count': self.physical_cpu_count,
                    'usage_percent': 0,
                    'available_cores': self.cpu_count
                },
                'memory': {
                    'total': self.total_memory,
                    'available': self.available_memory,
                    'used': 0,
                    'usage_percent': 0
                },
                'gpu': self.gpu_info,
                'timestamp': datetime.now().isoformat()
            }
    
    def _update_gpu_info(self) -> List[Dict[str, Any]]:
        """æ›´æ–°GPUä¿¡æ¯"""
        updated_gpus = []
        
        try:
            import pynvml
            for gpu in self.gpu_info:
                handle = pynvml.nvmlDeviceGetHandleByIndex(gpu['id'])
                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                
                updated_gpu = gpu.copy()
                updated_gpu['free_memory'] = memory_info.free
                updated_gpu['used_memory'] = memory_info.used
                updated_gpus.append(updated_gpu)
        except Exception:
            # å¦‚æœæ— æ³•æ›´æ–°ï¼Œè¿”å›åŸæœ‰ä¿¡æ¯
            updated_gpus = self.gpu_info
        
        return updated_gpus
    
    def get_model_resource_requirements(self, model_type: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹çš„èµ„æºéœ€æ±‚"""
        # å®šä¹‰ä¸åŒæ¨¡å‹çš„èµ„æºéœ€æ±‚
        requirements = {
            'vision_service': {
                'cpu_cores': 2,
                'memory_gb': 2,
                'gpu_memory_gb': 2,
                'priority': 2,
                'estimated_time_hours': 2
            },
            'audio_service': {
                'cpu_cores': 1,
                'memory_gb': 1,
                'gpu_memory_gb': 0,  # éŸ³é¢‘å¤„ç†é€šå¸¸ä¸éœ€è¦GPU
                'priority': 1,
                'estimated_time_hours': 1
            },
            'causal_reasoning_engine': {
                'cpu_cores': 2,
                'memory_gb': 2,
                'gpu_memory_gb': 0,  # é€»è¾‘æ¨ç†ä¸»è¦ä½¿ç”¨CPU
                'priority': 3,
                'estimated_time_hours': 3
            },
            'multimodal_service': {
                'cpu_cores': 3,
                'memory_gb': 3,
                'gpu_memory_gb': 3,
                'priority': 4,
                'estimated_time_hours': 4
            },
            'math_model': {
                'cpu_cores': 1,
                'memory_gb': 1,
                'gpu_memory_gb': 0,
                'priority': 1,
                'estimated_time_hours': 1
            },
            'logic_model': {
                'cpu_cores': 2,
                'memory_gb': 1,
                'gpu_memory_gb': 0,
                'priority': 2,
                'estimated_time_hours': 2
            },
            'concept_models': {
                'cpu_cores': 2,
                'memory_gb': 2,
                'gpu_memory_gb': 1,
                'priority': 3,
                'estimated_time_hours': 3
            },
            'environment_simulator': {
                'cpu_cores': 2,
                'memory_gb': 1,
                'gpu_memory_gb': 0,
                'priority': 2,
                'estimated_time_hours': 2
            },
            'adaptive_learning_controller': {
                'cpu_cores': 1,
                'memory_gb': 1,
                'gpu_memory_gb': 0,
                'priority': 3,
                'estimated_time_hours': 1
            },
            'alpha_deep_model': {
                'cpu_cores': 3,
                'memory_gb': 3,
                'gpu_memory_gb': 2,
                'priority': 4,
                'estimated_time_hours': 4
            }
        }
        
        return requirements.get(model_type, {
            'cpu_cores': 1,
            'memory_gb': 1,
            'gpu_memory_gb': 0,
            'priority': 1,
            'estimated_time_hours': 1
        })
    
    def add_task_to_queue(self, task_info: Dict[str, Any]):
        """å°†ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—ä¸­"""
        # ä½¿ç”¨ä¼˜å…ˆçº§å’Œé¢„è®¡æ—¶é—´ä½œä¸ºæ’åºä¾æ®
        priority = task_info.get('requirements', {}).get('priority', 1)
        estimated_time = task_info.get('requirements', {}).get('estimated_time_hours', 1)
        
        # åˆ›å»ºä»»åŠ¡å…ƒç»„ï¼š(ä¼˜å…ˆçº§è´Ÿå€¼, é¢„è®¡æ—¶é—´, ä»»åŠ¡ä¿¡æ¯)
        # ä½¿ç”¨è´Ÿå€¼æ˜¯å› ä¸ºheapqæ˜¯æœ€å°å †ï¼Œæˆ‘ä»¬éœ€è¦æœ€å¤§ä¼˜å…ˆçº§å…ˆæ‰§è¡Œ
        task_tuple = (-priority, estimated_time, task_info)
        heapq.heappush(self.task_queue, task_tuple)
        logger.info(f"ğŸ“¥ ä»»åŠ¡å·²æ·»åŠ åˆ°é˜Ÿåˆ—: {task_info.get('model_name', 'Unknown')}")
    
    def get_next_task(self) -> Optional[Dict[str, Any]]:
        """è·å–ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„ä»»åŠ¡"""
        if not self.task_queue:
            return None
        
        # å¼¹å‡ºä¼˜å…ˆçº§æœ€é«˜çš„ä»»åŠ¡
        priority, estimated_time, task_info = heapq.heappop(self.task_queue)
        return task_info
    
    def allocate_resources(self, requirements: Dict[str, Any], model_name: str = None) -> Optional[Dict[str, Any]]:
        """ä¸ºæ¨¡å‹åˆ†é…èµ„æº"""
        if not requirements:
            return None
        
        # ä½¿ç”¨æ™ºèƒ½èµ„æºåˆ†é…å™¨è¿›è¡Œèµ„æºåˆ†é…
        from .smart_resource_allocator import ResourceRequest
        
        # åˆ›å»ºèµ„æºè¯·æ±‚
        resource_request = ResourceRequest(
            task_id=model_name or f"task_{int(datetime.now().timestamp())}",
            cpu_cores=requirements.get('cpu_cores', 1),
            memory_gb=requirements.get('memory_gb', 1),
            gpu_memory_gb=requirements.get('gpu_memory_gb', 0),
            priority=requirements.get('priority', 1),
            estimated_time_hours=requirements.get('estimated_time_hours', 1),
            resource_type="gpu" if requirements.get('gpu_memory_gb', 0) > 0 else "cpu"
        )
        
        # è¯·æ±‚èµ„æº
        self.smart_allocator.request_resources(resource_request)
        
        # åˆ†é…èµ„æº
        allocations = self.smart_allocator.allocate_resources()
        
        if not allocations:
            logger.warning(f"âš ï¸  èµ„æºåˆ†é…å¤±è´¥: {model_name}")
            return None
        
        # è·å–åˆ†é…ç»“æœ
        allocation_result = allocations[0]  # å‡è®¾ç¬¬ä¸€ä¸ªåˆ†é…å°±æ˜¯æˆ‘ä»¬éœ€è¦çš„
        
        # è½¬æ¢ä¸ºåŸæœ‰æ ¼å¼
        allocation = {
            'cpu_cores': allocation_result.allocated_cpu_cores,
            'memory_gb': allocation_result.allocated_memory_gb,
            'gpu_memory_gb': allocation_result.allocated_gpu_memory_gb,
            'allocated_at': datetime.now().isoformat()
        }
        
        # è®°å½•èµ„æºåˆ†é…
        if model_name:
            self.resource_allocation[model_name] = allocation
        
        logger.info(f"âœ… èµ„æºåˆ†é…æˆåŠŸ: CPU {allocation_result.allocated_cpu_cores} æ ¸å¿ƒ, å†…å­˜ {allocation_result.allocated_memory_gb} GB, GPU {allocation_result.allocated_gpu_memory_gb} GB")
        return allocation
    
    def release_resources(self, model_name: str):
        """é‡Šæ”¾æ¨¡å‹å ç”¨çš„èµ„æº"""
        if model_name in self.resource_allocation:
            del self.resource_allocation[model_name]
            logger.info(f"ğŸ”„ é‡Šæ”¾æ¨¡å‹ {model_name} çš„èµ„æº")
    
    def get_resource_utilization(self) -> Dict[str, Any]:
        """è·å–èµ„æºåˆ©ç”¨ç‡æŠ¥å‘Š"""
        system_resources = self.get_system_resources()
        cpu_info = system_resources['cpu']
        memory_info = system_resources['memory']
        
        utilization = {
            'cpu_utilization': {
                'used_cores': self.cpu_count - cpu_info['available_cores'],
                'total_cores': self.cpu_count,
                'utilization_percent': (self.cpu_count - cpu_info['available_cores']) / self.cpu_count * 100
            },
            'memory_utilization': {
                'used_gb': (memory_info['total'] - memory_info['available']) / (1024**3),
                'total_gb': memory_info['total'] / (1024**3),
                'utilization_percent': memory_info['percent']
            },
            'allocated_models': list(self.resource_allocation.keys())
        }
        
        return utilization
    
    def get_resource_allocation_status(self) -> Dict[str, Any]:
        """è·å–èµ„æºåˆ†é…çŠ¶æ€"""
        system_resources = self.get_system_resources()
        cpu_info = system_resources['cpu']
        memory_info = system_resources['memory']
        
        allocated_cpu = sum(allocation.get('cpu_cores', 0) for allocation in self.resource_allocation.values())
        allocated_memory = sum(allocation.get('memory_gb', 0) for allocation in self.resource_allocation.values())
        
        status = {
            'total_cpu': self.cpu_count,
            'allocated_cpu': allocated_cpu,
            'available_cpu': self.cpu_count - allocated_cpu,
            'total_memory_gb': self.total_memory / (1024**3),
            'allocated_memory_gb': allocated_memory,
            'available_memory_gb': (self.total_memory / (1024**3)) - allocated_memory,
            'gpu_info': self.gpu_info,
            'allocated_models': list(self.resource_allocation.keys()),
            'pending_tasks': len(self.task_queue)
        }
        
        return status
    
    def optimize_resource_allocation(self) -> Dict[str, Any]:
        """ä¼˜åŒ–èµ„æºåˆ†é…"""
        logger.info("âš™ï¸  å¼€å§‹ä¼˜åŒ–èµ„æºåˆ†é…...")
        
        # è·å–å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ
        status = self.get_resource_allocation_status()
        
        optimization_result = {
            'timestamp': datetime.now().isoformat(),
            'actions_taken': [],
            'current_status': status
        }
        
        # å¦‚æœæœ‰å¤§é‡ç©ºé—²èµ„æºï¼Œå¯ä»¥è€ƒè™‘å¢åŠ å¹¶è¡Œä»»åŠ¡
        if status['available_cpu'] > status['total_cpu'] * 0.5:
            optimization_result['actions_taken'].append("ç³»ç»Ÿæœ‰å¤§é‡ç©ºé—²CPUèµ„æºï¼Œå¯ä»¥å¢åŠ å¹¶è¡Œä»»åŠ¡")
        
        if status['available_memory_gb'] > status['total_memory_gb'] * 0.5:
            optimization_result['actions_taken'].append("ç³»ç»Ÿæœ‰å¤§é‡ç©ºé—²å†…å­˜èµ„æºï¼Œå¯ä»¥å¢åŠ å¹¶è¡Œä»»åŠ¡")
        
        # å¦‚æœèµ„æºç´§å¼ ï¼Œè€ƒè™‘æš‚åœä½ä¼˜å…ˆçº§ä»»åŠ¡
        if status['available_cpu'] < 1 or status['available_memory_gb'] < 1:
            optimization_result['actions_taken'].append("ç³»ç»Ÿèµ„æºç´§å¼ ï¼Œå»ºè®®æš‚åœä½ä¼˜å…ˆçº§ä»»åŠ¡")
        
        logger.info("âœ… èµ„æºåˆ†é…ä¼˜åŒ–å®Œæˆ")
        return optimization_result
    
    def dynamic_resource_scaling(self, model_name: str, current_performance: Dict[str, Any]) -> bool:
        """åŠ¨æ€è°ƒæ•´æ¨¡å‹èµ„æºåˆ†é…"""
        logger.info(f"ğŸ“ˆ åŠ¨æ€è°ƒæ•´æ¨¡å‹ {model_name} çš„èµ„æºåˆ†é…")
        
        if model_name not in self.resource_allocation:
            logger.warning(f"âš ï¸  æ¨¡å‹ {model_name} æœªåˆ†é…èµ„æº")
            return False
        
        # è·å–å½“å‰èµ„æºåˆ†é…
        current_allocation = self.resource_allocation[model_name]
        cpu_cores = current_allocation['cpu_cores']
        memory_gb = current_allocation['memory_gb']
        
        # æ ¹æ®æ€§èƒ½æŒ‡æ ‡è°ƒæ•´èµ„æº
        accuracy = current_performance.get('accuracy', 0.0)
        loss = current_performance.get('loss', 1.0)
        processing_time = current_performance.get('processing_time', 1.0)
        
        # å¦‚æœå‡†ç¡®ç‡ä½ä¸”æŸå¤±é«˜ï¼Œå¢åŠ èµ„æº
        if accuracy < 0.8 and loss > 0.5:
            # å¢åŠ CPUæ ¸å¿ƒ
            if cpu_cores < self.cpu_count:
                current_allocation['cpu_cores'] = min(cpu_cores + 1, self.cpu_count)
                logger.info(f"   å¢åŠ CPUæ ¸å¿ƒ: {cpu_cores} -> {current_allocation['cpu_cores']}")
            
            # å¢åŠ å†…å­˜
            current_allocation['memory_gb'] = memory_gb * 1.2
            logger.info(f"   å¢åŠ å†…å­˜: {memory_gb:.2f}GB -> {current_allocation['memory_gb']:.2f}GB")
        
        # å¦‚æœå¤„ç†æ—¶é—´è¿‡é•¿ï¼Œå¢åŠ èµ„æº
        elif processing_time > 10.0:  # è¶…è¿‡10ç§’
            if cpu_cores < self.cpu_count:
                current_allocation['cpu_cores'] = min(cpu_cores + 1, self.cpu_count)
                logger.info(f"   å¢åŠ CPUæ ¸å¿ƒ: {cpu_cores} -> {current_allocation['cpu_cores']}")
        
        # å¦‚æœå‡†ç¡®ç‡é«˜ä¸”æŸå¤±ä½ï¼Œå¯ä»¥å‡å°‘èµ„æºä»¥èŠ‚çœèµ„æº
        elif accuracy > 0.95 and loss < 0.1:
            # å‡å°‘CPUæ ¸å¿ƒ
            if cpu_cores > 1:
                current_allocation['cpu_cores'] = max(cpu_cores - 1, 1)
                logger.info(f"   å‡å°‘CPUæ ¸å¿ƒ: {cpu_cores} -> {current_allocation['cpu_cores']}")
            
            # å‡å°‘å†…å­˜
            current_allocation['memory_gb'] = max(memory_gb * 0.8, 1.0)
            logger.info(f"   å‡å°‘å†…å­˜: {memory_gb:.2f}GB -> {current_allocation['memory_gb']:.2f}GB")
        
        # æ›´æ–°èµ„æºåˆ†é…è®°å½•
        self.resource_allocation[model_name] = current_allocation
        logger.info(f"âœ… æ¨¡å‹ {model_name} èµ„æºè°ƒæ•´å®Œæˆ")
        return True