#!/usr/bin/env python3
"""
æ•°æ®ç®¡ç†å™¨
è´Ÿè´£è‡ªåŠ¨æ£€æµ‹ã€åˆ†ç±»å’Œå¤„ç†è®­ç»ƒæ•°æ®
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Tuple
import mimetypes
import hashlib
from datetime import datetime
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
project_root = Path(__file__).parent.parent
backend_path = project_root / "apps" / "backend"
sys.path.insert(0, str(backend_path))
sys.path.insert(0, str(backend_path / "src"))

# å¯¼å…¥è·¯å¾„é…ç½®æ¨¡å—
try:
    from apps.backend.src.path_config import (
        PROJECT_ROOT, 
        DATA_DIR, 
        TRAINING_DIR, 
        get_data_path, 
        resolve_path
    )
except ImportError:
    # å¦‚æœè·¯å¾„é…ç½®æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„å¤„ç†
    PROJECT_ROOT = project_root
    DATA_DIR = PROJECT_ROOT / "data"
    TRAINING_DIR = PROJECT_ROOT / "training"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataManager:
    """æ•°æ®ç®¡ç†å™¨ï¼Œè´Ÿè´£è‡ªåŠ¨æ£€æµ‹ã€åˆ†ç±»å’Œå¤„ç†è®­ç»ƒæ•°æ®"""
    
    def __init__(self, data_dir: str = None):
        self.data_dir = Path(data_dir) if data_dir else DATA_DIR
        self.data_catalog = {}
        self.data_quality_scores = {}
        self.supported_formats = {
            'image': ['.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff'],
            'audio': ['.wav', '.mp3', '.flac', '.aac', '.ogg'],
            'text': ['.txt', '.md', '.json', '.csv', '.xml'],
            'video': ['.mp4', '.avi', '.mov', '.mkv', '.flv'],
            'document': ['.pdf', '.doc', '.docx', '.ppt', '.pptx']
        }
        self.model_data_mapping = {
            'vision_service': ['image', 'document'],
            'audio_service': ['audio'],
            'causal_reasoning_engine': ['text'],
            'multimodal_service': ['image', 'audio', 'text', 'video'],
            'math_model': ['text'],
            'logic_model': ['text'],
            'concept_models': ['text', 'json'],
            'environment_simulator': ['text', 'json'],
            'causal_reasoning_engine': ['text', 'json'],  # æ·»åŠ å¯¹å› æœæ¨ç†å¼•æ“çš„JSONæ•°æ®æ”¯æŒ
            'adaptive_learning_controller': ['text', 'json'],
            'alpha_deep_model': ['text', 'json']
        }
    
    def scan_data(self) -> Dict[str, Any]:
        """æ‰«æå¹¶åˆ†ç±»æ‰€æœ‰æ•°æ®"""
        logger.info(f"ğŸ” å¼€å§‹æ‰«ææ•°æ®ç›®å½•: {self.data_dir}")
        
        # æ¸…ç©ºä¹‹å‰çš„æ•°æ®ç›®å½•
        self.data_catalog = {}
        
        # éå†æ•°æ®ç›®å½•
        for root, dirs, files in os.walk(self.data_dir):
            # è·³è¿‡éšè—ç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.')]
            
            for file in files:
                # è·³è¿‡éšè—æ–‡ä»¶
                if file.startswith('.'):
                    continue
                    
                file_path = Path(root) / file
                relative_path = file_path.relative_to(self.data_dir)
                
                # è·å–æ–‡ä»¶ä¿¡æ¯
                try:
                    stat = file_path.stat()
                    file_info = {
                        'path': str(file_path),
                        'relative_path': str(relative_path),
                        'size': stat.st_size,
                        'modified_time': stat.st_mtime,
                        'extension': file_path.suffix.lower(),
                        'type': self._classify_file(file_path)
                    }
                    
                    # æ·»åŠ åˆ°æ•°æ®ç›®å½•
                    self.data_catalog[str(relative_path)] = file_info
                except Exception as e:
                    logger.warning(f"âš ï¸ æ— æ³•è·å–æ–‡ä»¶ä¿¡æ¯ {file_path}: {e}")
        
        logger.info(f"âœ… æ•°æ®æ‰«æå®Œæˆï¼Œå…±å‘ç° {len(self.data_catalog)} ä¸ªæ–‡ä»¶")
        return self.data_catalog
    
    def _classify_file(self, file_path: Path) -> str:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ†ç±»æ–‡ä»¶"""
        extension = file_path.suffix.lower()
        
        for data_type, extensions in self.supported_formats.items():
            if extension in extensions:
                return data_type
        
        # å°è¯•ä½¿ç”¨mimetypesåˆ†ç±»
        mime_type, _ = mimetypes.guess_type(str(file_path))
        if mime_type:
            if mime_type.startswith('image/'):
                return 'image'
            elif mime_type.startswith('audio/'):
                return 'audio'
            elif mime_type.startswith('video/'):
                return 'video'
            elif mime_type.startswith('text/'):
                return 'text'
            elif mime_type == 'application/pdf':
                return 'document'
        
        # é»˜è®¤åˆ†ç±»ä¸ºæ–‡æœ¬
        return 'text'
    
    def assess_data_quality(self, file_path: str) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ–‡ä»¶çš„æ•°æ®è´¨é‡"""
        path = Path(file_path)
        if not path.exists():
            return {'quality_score': 0, 'issues': ['æ–‡ä»¶ä¸å­˜åœ¨']}
        
        quality_info = {
            'quality_score': 0,
            'file_size': path.stat().st_size,
            'modified_time': path.stat().st_mtime,
            'issues': []
        }
        
        try:
            # æ–‡ä»¶å¤§å°è¯„ä¼°
            if quality_info['file_size'] < 10:  # å°äº10å­—èŠ‚
                quality_info['issues'].append('æ–‡ä»¶è¿‡å°')
                quality_info['quality_score'] -= 20
            elif quality_info['file_size'] > 100 * 1024 * 1024:  # å¤§äº100MB
                quality_info['issues'].append('æ–‡ä»¶è¿‡å¤§')
                quality_info['quality_score'] -= 10
            else:
                quality_info['quality_score'] += 20
            
            # æ–‡ä»¶ç±»å‹ç‰¹å®šæ£€æŸ¥
            file_type = self._classify_file(path)
            if file_type == 'image':
                quality_info = self._assess_image_quality(path, quality_info)
            elif file_type == 'audio':
                quality_info = self._assess_audio_quality(path, quality_info)
            elif file_type == 'text':
                quality_info = self._assess_text_quality(path, quality_info)
            
            # æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥
            if self._is_file_corrupted(path):
                quality_info['issues'].append('æ–‡ä»¶å¯èƒ½å·²æŸå')
                quality_info['quality_score'] -= 30
            else:
                quality_info['quality_score'] += 10
                
        except Exception as e:
            quality_info['issues'].append(f'è¯„ä¼°é”™è¯¯: {str(e)}')
            quality_info['quality_score'] = 0
        
        # ç¡®ä¿åˆ†æ•°åœ¨0-100èŒƒå›´å†…
        quality_info['quality_score'] = max(0, min(100, quality_info['quality_score']))
        
        self.data_quality_scores[str(path)] = quality_info
        return quality_info
    
    def _assess_image_quality(self, file_path: Path, quality_info: Dict) -> Dict:
        """è¯„ä¼°å›¾åƒæ–‡ä»¶è´¨é‡"""
        try:
            # å°è¯•å¯¼å…¥PILæ¥æ£€æŸ¥å›¾åƒæ–‡ä»¶
            from PIL import Image
            with Image.open(file_path) as img:
                width, height = img.size
                # æ£€æŸ¥å›¾åƒå°ºå¯¸
                if width < 10 or height < 10:
                    quality_info['issues'].append('å›¾åƒå°ºå¯¸è¿‡å°')
                    quality_info['quality_score'] -= 15
                elif width > 10000 or height > 10000:
                    quality_info['issues'].append('å›¾åƒå°ºå¯¸è¿‡å¤§')
                    quality_info['quality_score'] -= 10
                else:
                    quality_info['quality_score'] += 15
                    
                # æ£€æŸ¥å›¾åƒæ¨¡å¼
                if img.mode not in ['RGB', 'RGBA', 'L']:
                    quality_info['issues'].append(f'å›¾åƒæ¨¡å¼ä¸å¸¸è§: {img.mode}')
        except ImportError:
            # å¦‚æœæ²¡æœ‰PILï¼Œè·³è¿‡å›¾åƒç‰¹å®šæ£€æŸ¥
            pass
        except Exception as e:
            quality_info['issues'].append(f'å›¾åƒè¯»å–é”™è¯¯: {str(e)}')
            quality_info['quality_score'] -= 20
        
        return quality_info
    
    def _assess_audio_quality(self, file_path: Path, quality_info: Dict) -> Dict:
        """è¯„ä¼°éŸ³é¢‘æ–‡ä»¶è´¨é‡"""
        # ç®€å•çš„éŸ³é¢‘æ–‡ä»¶è´¨é‡æ£€æŸ¥
        try:
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åæ˜¯å¦ä¸ºæ”¯æŒçš„éŸ³é¢‘æ ¼å¼
            extension = file_path.suffix.lower()
            if extension in ['.wav', '.mp3', '.flac']:
                quality_info['quality_score'] += 10
            else:
                quality_info['issues'].append(f'éŸ³é¢‘æ ¼å¼å¯èƒ½ä¸æ”¯æŒ: {extension}')
        except Exception as e:
            quality_info['issues'].append(f'éŸ³é¢‘æ£€æŸ¥é”™è¯¯: {str(e)}')
            quality_info['quality_score'] -= 10
        
        return quality_info
    
    def _assess_text_quality(self, file_path: Path, quality_info: Dict) -> Dict:
        """è¯„ä¼°æ–‡æœ¬æ–‡ä»¶è´¨é‡"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # æ£€æŸ¥æ–‡ä»¶å†…å®¹
            if len(content.strip()) == 0:
                quality_info['issues'].append('æ–‡ä»¶å†…å®¹ä¸ºç©º')
                quality_info['quality_score'] -= 30
            elif len(content) < 10:
                quality_info['issues'].append('æ–‡ä»¶å†…å®¹è¿‡å°‘')
                quality_info['quality_score'] -= 15
            else:
                quality_info['quality_score'] += 20
                
            # æ£€æŸ¥ç¼–ç é—®é¢˜
            try:
                content.encode('utf-8')
                quality_info['quality_score'] += 5
            except UnicodeError:
                quality_info['issues'].append('ç¼–ç é—®é¢˜')
                quality_info['quality_score'] -= 10
                
        except UnicodeDecodeError:
            quality_info['issues'].append('æ–‡ä»¶ç¼–ç ä¸æ”¯æŒ')
            quality_info['quality_score'] -= 25
        except Exception as e:
            quality_info['issues'].append(f'æ–‡æœ¬è¯»å–é”™è¯¯: {str(e)}')
            quality_info['quality_score'] -= 20
        
        return quality_info
    
    def _is_file_corrupted(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¯èƒ½å·²æŸå"""
        try:
            # è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            
            # ç®€å•æ£€æŸ¥ï¼šå¦‚æœæ–‡ä»¶å¤§å°ä¸º0ï¼Œåˆ™è®¤ä¸ºå·²æŸå
            if file_path.stat().st_size == 0:
                return True
                
            return False
        except Exception:
            return True
    
    def get_data_by_type(self, data_type: str) -> List[Dict[str, Any]]:
        """æ ¹æ®æ•°æ®ç±»å‹è·å–æ–‡ä»¶åˆ—è¡¨"""
        result = []
        for file_info in self.data_catalog.values():
            if file_info['type'] == data_type:
                result.append(file_info)
        return result
    
    def get_high_quality_data(self, min_quality_score: int = 70) -> Dict[str, List[Dict[str, Any]]]:
        """è·å–é«˜è´¨é‡æ•°æ®ï¼ˆæŒ‰ç±»å‹åˆ†ç»„ï¼‰"""
        high_quality_data = {}
        
        # å…ˆè¯„ä¼°æ‰€æœ‰æ•°æ®çš„è´¨é‡
        for file_path in self.data_catalog.keys():
            self.assess_data_quality(file_path)
        
        # æŒ‰ç±»å‹åˆ†ç»„é«˜è´¨é‡æ•°æ®
        for file_path, quality_info in self.data_quality_scores.items():
            if quality_info['quality_score'] >= min_quality_score:
                file_info = self.data_catalog.get(file_path)
                if file_info:
                    data_type = file_info['type']
                    if data_type not in high_quality_data:
                        high_quality_data[data_type] = []
                    high_quality_data[data_type].append(file_info)
        
        return high_quality_data
    
    def prepare_training_data(self, model_type: str) -> List[Dict[str, Any]]:
        """ä¸ºç‰¹å®šæ¨¡å‹ç±»å‹å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info(f"ğŸ“¦ ä¸ºæ¨¡å‹ {model_type} å‡†å¤‡è®­ç»ƒæ•°æ®")
        
        # è·å–è¯¥æ¨¡å‹æ”¯æŒçš„æ•°æ®ç±»å‹
        supported_types = self.model_data_mapping.get(model_type, [])
        if not supported_types:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹ {model_type} çš„æ•°æ®æ˜ å°„")
            return []
        
        # æ”¶é›†æ”¯æŒçš„æ•°æ®
        training_data = []
        for data_type in supported_types:
            data_files = self.get_data_by_type(data_type)
            training_data.extend(data_files)
        
        # å¯¹äºæ¦‚å¿µæ¨¡å‹ï¼Œç›´æ¥æ·»åŠ æ¦‚å¿µæ¨¡å‹è®­ç»ƒæ•°æ®
        if model_type in ['concept_models', 'environment_simulator', 'causal_reasoning_engine', 
                         'adaptive_learning_controller', 'alpha_deep_model']:
            # æ·»åŠ æ¦‚å¿µæ¨¡å‹ä¸“ç”¨è®­ç»ƒæ•°æ®
            concept_data_dir = self.data_dir / "concept_models_training_data"
            if concept_data_dir.exists():
                for json_file in concept_data_dir.glob("*.json"):
                    # æ ¹æ®æ¨¡å‹ç±»å‹è¿‡æ»¤æ•°æ®
                    if self._is_data_relevant_for_model(json_file.name, model_type):
                        file_info = {
                            'path': str(json_file),
                            'relative_path': str(json_file.relative_to(self.data_dir)),
                            'size': json_file.stat().st_size,
                            'modified_time': json_file.stat().st_mtime,
                            'extension': '.json',
                            'type': 'json'
                        }
                        training_data.append(file_info)
        
        # è¿‡æ»¤é«˜è´¨é‡æ•°æ®
        high_quality_data = self.get_high_quality_data()
        filtered_data = []
        for data_item in training_data:
            # æ£€æŸ¥æ•°æ®æ˜¯å¦åœ¨é«˜è´¨é‡æ•°æ®ä¸­
            data_type = data_item['type']
            if data_type in high_quality_data:
                high_quality_files = [f['path'] for f in high_quality_data[data_type]]
                if data_item['path'] in high_quality_files:
                    filtered_data.append(data_item)
            else:
                # å¦‚æœæ²¡æœ‰é«˜è´¨é‡æ•°æ®æ£€æŸ¥ï¼Œç›´æ¥æ·»åŠ 
                filtered_data.append(data_item)
        
        logger.info(f"âœ… ä¸ºæ¨¡å‹ {model_type} å‡†å¤‡äº† {len(filtered_data)} ä¸ªè®­ç»ƒæ•°æ®æ–‡ä»¶")
        return filtered_data
    
    def _is_data_relevant_for_model(self, filename: str, model_type: str) -> bool:
        """æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦ä¸ç‰¹å®šæ¨¡å‹ç›¸å…³"""
        # æ ¹æ®æ–‡ä»¶åå’Œæ¨¡å‹ç±»å‹åˆ¤æ–­ç›¸å…³æ€§
        if model_type == 'environment_simulator' and 'environment' in filename:
            return True
        elif model_type == 'causal_reasoning_engine' and 'causal' in filename:
            return True
        elif model_type == 'adaptive_learning_controller' and 'adaptive' in filename:
            return True
        elif model_type == 'alpha_deep_model' and 'alpha' in filename:
            return True
        elif model_type == 'concept_models':
            # æ¦‚å¿µæ¨¡å‹å¯ä»¥ä½¿ç”¨æ‰€æœ‰æ¦‚å¿µæ•°æ®
            return any(keyword in filename for keyword in ['environment', 'causal', 'adaptive', 'alpha'])
        
        return False

    def get_data_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        if not self.data_catalog:
            self.scan_data()
        
        stats = {
            'total_files': len(self.data_catalog),
            'file_types': {},
            'total_size': 0,
            'last_scan_time': datetime.now().isoformat()
        }
        
        # ç»Ÿè®¡å„ç±»æ–‡ä»¶æ•°é‡å’Œå¤§å°
        for file_info in self.data_catalog.values():
            file_type = file_info['type']
            if file_type not in stats['file_types']:
                stats['file_types'][file_type] = {'count': 0, 'size': 0}
            
            stats['file_types'][file_type]['count'] += 1
            stats['file_types'][file_type]['size'] += file_info['size']
            stats['total_size'] += file_info['size']
        
        return stats
    
    def save_data_catalog(self, catalog_path: str = None):
        """ä¿å­˜æ•°æ®ç›®å½•åˆ°æ–‡ä»¶"""
        if not catalog_path:
            catalog_path = TRAINING_DIR / "data_catalog.json"
        
        catalog_data = {
            'catalog': self.data_catalog,
            'quality_scores': self.data_quality_scores,
            'statistics': self.get_data_statistics(),
            'generated_at': datetime.now().isoformat()
        }
        
        try:
            with open(catalog_path, 'w', encoding='utf-8') as f:
                json.dump(catalog_data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ æ•°æ®ç›®å½•å·²ä¿å­˜åˆ°: {catalog_path}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ•°æ®ç›®å½•å¤±è´¥: {e}")
    
    def load_data_catalog(self, catalog_path: str = None):
        """ä»æ–‡ä»¶åŠ è½½æ•°æ®ç›®å½•"""
        if not catalog_path:
            catalog_path = TRAINING_DIR / "data_catalog.json"
        
        if not Path(catalog_path).exists():
            logger.warning(f"âš ï¸ æ•°æ®ç›®å½•æ–‡ä»¶ä¸å­˜åœ¨: {catalog_path}")
            return False
        
        try:
            with open(catalog_path, 'r', encoding='utf-8') as f:
                catalog_data = json.load(f)
            
            self.data_catalog = catalog_data.get('catalog', {})
            self.data_quality_scores = catalog_data.get('quality_scores', {})
            logger.info(f"âœ… æ•°æ®ç›®å½•å·²ä» {catalog_path} åŠ è½½")
            return True
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ•°æ®ç›®å½•å¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•DataManager"""
    print("ğŸ” æµ‹è¯•æ•°æ®ç®¡ç†å™¨...")
    
    # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
    data_manager = DataManager()
    
    # æ‰«ææ•°æ®
    catalog = data_manager.scan_data()
    print(f"ğŸ“Š æ‰«æåˆ° {len(catalog)} ä¸ªæ–‡ä»¶")
    
    # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
    stats = data_manager.get_data_statistics()
    print(f"ğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"  æ€»å¤§å°: {stats['total_size'] / (1024*1024):.2f} MB")
    print(f"  æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
    for file_type, info in stats['file_types'].items():
        print(f"    {file_type}: {info['count']} ä¸ªæ–‡ä»¶, {info['size'] / (1024*1024):.2f} MB")
    
    # è¯„ä¼°å‡ ä¸ªæ–‡ä»¶çš„è´¨é‡
    print(f"\nğŸ” æ•°æ®è´¨é‡è¯„ä¼°:")
    sample_files = list(catalog.keys())[:3]  # å–å‰3ä¸ªæ–‡ä»¶è¿›è¡Œè¯„ä¼°
    for file_path in sample_files:
        quality = data_manager.assess_data_quality(file_path)
        print(f"  {file_path}: è´¨é‡è¯„åˆ† {quality['quality_score']}/100")
        if quality['issues']:
            print(f"    é—®é¢˜: {', '.join(quality['issues'])}")
    
    # ä¸ºä¸åŒæ¨¡å‹å‡†å¤‡æ•°æ®
    print(f"\nğŸ“¦ è®­ç»ƒæ•°æ®å‡†å¤‡:")
    for model_type in ['vision_service', 'audio_service', 'causal_reasoning_engine']:
        training_data = data_manager.prepare_training_data(model_type)
        print(f"  {model_type}: {len(training_data)} ä¸ªè®­ç»ƒæ–‡ä»¶")
    
    # ä¿å­˜æ•°æ®ç›®å½•
    data_manager.save_data_catalog()
    print(f"\nâœ… æ•°æ®ç®¡ç†å™¨æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    main()