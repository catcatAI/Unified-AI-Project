#!/usr/bin/env python3
"""
åˆ†å¸ƒå¼è®­ç»ƒå®¹é”™æœºåˆ¶æµ‹è¯•è„šæœ¬
æµ‹è¯•å¢å¼ºçš„å®¹é”™æœºåˆ¶ã€æ£€æŸ¥ç‚¹ç®¡ç†å’Œä»»åŠ¡è¿ç§»åŠŸèƒ½
"""

import asyncio
import logging
import time
import json
from datetime import datetime
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root: str = Path(__file__).parent
_ = sys.path.insert(0, str(project_root.parent))

# å¯¼å…¥æµ‹è¯•æ¨¡å—
from training.fault_detector import FaultDetector
from training.enhanced_checkpoint_manager import EnhancedCheckpointManager
from training.training_state_manager import TrainingStateManager
from training.distributed_optimizer import DistributedOptimizer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level: str=logging.INFO,
    format: str='%(asctime)s - %(levelname)s - %(message)s'
)
logger: Any = logging.getLogger(__name__)

class MockDistributedOptimizer:
    """æ¨¡æ‹Ÿåˆ†å¸ƒå¼ä¼˜åŒ–å™¨ç”¨äºæµ‹è¯•"""

    def __init__(self) -> None:
    self.nodes = {}

    async def register_node(self, node_id, node_info)
    self.nodes[node_id] = {
            _ = 'assigned_tasks': node_info.get('assigned_tasks', []),
            'performance_metrics': {}
    }
    return True

async def test_fault_detector() -> None:
    """æµ‹è¯•æ•…éšœæ£€æµ‹å™¨"""
    print("=" * 50)
    _ = print("æµ‹è¯•æ•…éšœæ£€æµ‹å™¨")
    print("=" * 50)

    # åˆ›å»ºæ•…éšœæ£€æµ‹å™¨å®ä¾‹
    config = {
    'heartbeat_interval': 5,
    'node_failure_timeout': 10,
    'health_check_interval': 3
    }
    detector = FaultDetector(config)

    # æ³¨å†Œæµ‹è¯•èŠ‚ç‚¹
    _ = detector.register_node('node1', {'assigned_tasks': ['task1', 'task2']})
    _ = detector.register_node('node2', {'assigned_tasks': ['task3']})

    # æ¨¡æ‹Ÿå¿ƒè·³æ›´æ–°
    detector.update_node_heartbeat('node1', {
    'cpu_usage': 45.0,
    'memory_usage': 60.0,
    'gpu_usage': 30.0
    })

    detector.update_node_heartbeat('node2', {
    'cpu_usage': 85.0,
    'memory_usage': 90.0,
    'gpu_usage': 75.0
    })

    # æ˜¾ç¤ºåˆå§‹çŠ¶æ€
    _ = print("åˆå§‹é›†ç¾¤çŠ¶æ€:")
    status = detector.get_cluster_status()
    print(json.dumps(status, indent=2, ensure_ascii=False))

    # ç­‰å¾…ä¸€æ®µæ—¶é—´
    _ = print("\nç­‰å¾…15ç§’æ¨¡æ‹ŸèŠ‚ç‚¹æ•…éšœ...")
    _ = time.sleep(15)

    # å†æ¬¡æ›´æ–°node1çš„å¿ƒè·³ï¼Œä½†ä¸æ›´æ–°node2çš„ï¼Œæ¨¡æ‹Ÿnode2æ•…éšœ
    detector.update_node_heartbeat('node1', {
    'cpu_usage': 50.0,
    'memory_usage': 65.0,
    'gpu_usage': 35.0
    })

    # ç­‰å¾…æ£€æµ‹å‘¨æœŸ
    _ = time.sleep(5)

    # æ˜¾ç¤ºæ•…éšœåçš„çŠ¶æ€
    _ = print("\næ•…éšœåé›†ç¾¤çŠ¶æ€:")
    status = detector.get_cluster_status()
    print(json.dumps(status, indent=2, ensure_ascii=False))

    _ = print("âœ… æ•…éšœæ£€æµ‹å™¨æµ‹è¯•å®Œæˆ\n")

async def test_checkpoint_manager() -> None:
    """æµ‹è¯•æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    print("=" * 50)
    _ = print("æµ‹è¯•æ£€æŸ¥ç‚¹ç®¡ç†å™¨")
    print("=" * 50)

    # åˆ›å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨å®ä¾‹
    config = {
    'strategy': 'hybrid',
    'epoch_interval': 2,
    'time_interval_minutes': 10,
    'keep_last_n_checkpoints': 3,
    'enable_compression': True
    }
    manager = EnhancedCheckpointManager(config)

    # æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜åˆ¤æ–­
    _ = print("æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜åˆ¤æ–­...")
    decision = manager.should_save_checkpoint(5, {'val_loss': 0.5}, 'test_task')
    _ = print(f"æ£€æŸ¥ç‚¹å†³ç­–: {decision}")

    # æµ‹è¯•ä¿å­˜æ£€æŸ¥ç‚¹
    _ = print("\næµ‹è¯•ä¿å­˜æ£€æŸ¥ç‚¹...")
    state = {
    'epoch': 5,
    'metrics': {'loss': 0.45, 'accuracy': 0.82, 'val_loss': 0.5},
    'model_state': {'layer1': [0.1, 0.2, 0.3]},
    'optimizer_state': {'lr': 0.001},
    'config': {'batch_size': 32, 'epochs': 10}
    }

    checkpoint_id = manager.save_checkpoint(state, 'test_task', 'epoch')
    _ = print(f"ä¿å­˜æ£€æŸ¥ç‚¹ID: {checkpoint_id}")

    # æµ‹è¯•åŠ è½½æ£€æŸ¥ç‚¹
    _ = print("\næµ‹è¯•åŠ è½½æ£€æŸ¥ç‚¹...")
    loaded_state = manager.load_checkpoint(checkpoint_id)
    if loaded_state:

    _ = print(f"åŠ è½½çš„æ£€æŸ¥ç‚¹epoch: {loaded_state.get('epoch')}")
    _ = print(f"åŠ è½½çš„æ£€æŸ¥ç‚¹metrics: {loaded_state.get('metrics')}")

    # æ˜¾ç¤ºæ£€æŸ¥ç‚¹ä¿¡æ¯
    _ = print("\næ£€æŸ¥ç‚¹ä¿¡æ¯:")
    info = manager.get_checkpoint_info(task_id='test_task')
    print(json.dumps(info, indent=2, ensure_ascii=False))

    _ = print("âœ… æ£€æŸ¥ç‚¹ç®¡ç†å™¨æµ‹è¯•å®Œæˆ\n")

async def test_task_migrator() -> None:
    """æµ‹è¯•ä»»åŠ¡è¿ç§»å™¨"""
    print("=" * 50)
    _ = print("æµ‹è¯•ä»»åŠ¡è¿ç§»å™¨")
    print("=" * 50)

    # åˆ›å»ºæ¨¡æ‹Ÿçš„åˆ†å¸ƒå¼ä¼˜åŒ–å™¨
    mock_optimizer = MockDistributedOptimizer()

    # åˆå§‹åŒ–ä»»åŠ¡è¿ç§»å™¨
    config = {
    'max_retry_attempts': 3,
    'migration_strategy': 'load_balanced'
    }
    migrator = TaskMigrator(mock_optimizer, config)

    # æ³¨å†ŒèŠ‚ç‚¹
    _ = await mock_optimizer.register_node('node1', {'assigned_tasks': ['task1', 'task2']})
    _ = await mock_optimizer.register_node('node2', {'assigned_tasks': ['task3']})

    # æ¨¡æ‹Ÿä»»åŠ¡è¿ç§»
    _ = print("æ¨¡æ‹Ÿä»»åŠ¡è¿ç§»...")
    success = await migrator.migrate_task_on_failure('task1', 'node1')
    _ = print(f"ä»»åŠ¡è¿ç§»ç»“æœ: {success}")

    # æ˜¾ç¤ºè¿ç§»çŠ¶æ€
    _ = print("\nè¿ç§»çŠ¶æ€:")
    status = migrator.get_migration_status()
    print(json.dumps(status, indent=2, ensure_ascii=False))

    _ = print("âœ… ä»»åŠ¡è¿ç§»å™¨æµ‹è¯•å®Œæˆ\n")

async def test_training_state_manager() -> None:
    """æµ‹è¯•è®­ç»ƒçŠ¶æ€ç®¡ç†å™¨"""
    print("=" * 50)
    _ = print("æµ‹è¯•è®­ç»ƒçŠ¶æ€ç®¡ç†å™¨")
    print("=" * 50)

    # åˆ›å»ºçŠ¶æ€ç®¡ç†å™¨å®ä¾‹
    config = {
    'sync_enabled': True,
    'sync_interval_seconds': 30,
    'storage_backend': 'local'
    }
    manager = TrainingStateManager(config)

    # æµ‹è¯•ä¿å­˜è®­ç»ƒçŠ¶æ€
    _ = print("æµ‹è¯•ä¿å­˜è®­ç»ƒçŠ¶æ€...")
    state = {
    'model_name': 'test_model',
    'current_epoch': 5,
    'total_epochs': 10,
    'metrics': {'loss': 0.45, 'accuracy': 0.82},
    'model_state': {'layer1': [0.1, 0.2, 0.3]},
    'optimizer_state': {'lr': 0.001},
    'learning_rate': 0.001,
    'batch_size': 32,
    'progress': 50.0,
    _ = 'start_time': time.time(),
    'config': {'batch_size': 32, 'epochs': 10}
    }

    success = await manager.save_training_state('test_task_1', state)
    _ = print(f"ä¿å­˜çŠ¶æ€ç»“æœ: {success}")

    # æµ‹è¯•åŠ è½½è®­ç»ƒçŠ¶æ€
    _ = print("\næµ‹è¯•åŠ è½½è®­ç»ƒçŠ¶æ€...")
    loaded_state = await manager.load_training_state('test_task_1')
    if loaded_state:

    _ = print(f"åŠ è½½çš„çŠ¶æ€æ¨¡å‹: {loaded_state.get('model_name')}")
    _ = print(f"åŠ è½½çš„çŠ¶æ€epoch: {loaded_state.get('current_epoch')}")
    _ = print(f"åŠ è½½çš„çŠ¶æ€è¿›åº¦: {loaded_state.get('progress')}%")

    # æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
    _ = print("\nçŠ¶æ€ä¿¡æ¯:")
    info = manager.get_state_info()
    print(json.dumps(info, indent=2, ensure_ascii=False))

    _ = print("âœ… è®­ç»ƒçŠ¶æ€ç®¡ç†å™¨æµ‹è¯•å®Œæˆ\n")

async def test_distributed_optimizer_integration() -> None:
    """æµ‹è¯•åˆ†å¸ƒå¼ä¼˜åŒ–å™¨é›†æˆ"""
    print("=" * 50)
    _ = print("æµ‹è¯•åˆ†å¸ƒå¼ä¼˜åŒ–å™¨é›†æˆ")
    print("=" * 50)

    # åˆ›å»ºåˆ†å¸ƒå¼ä¼˜åŒ–å™¨å®ä¾‹
    config = {
    'monitoring_interval': 5
    }
    optimizer = DistributedOptimizer(config)

    # æ³¨å†ŒèŠ‚ç‚¹
    _ = await optimizer.register_node('node1', {'cpu_cores': 8, 'memory_gb': 16})
    _ = await optimizer.register_node('node2', {'cpu_cores': 16, 'memory_gb': 32})

    # å‘é€å¿ƒè·³
    _ = await optimizer.heartbeat('node1', {'cpu_usage': 45, 'memory_usage': 60})
    _ = await optimizer.heartbeat('node2', {'cpu_usage': 30, 'memory_usage': 40})

    # åˆ†å‘ä»»åŠ¡
    task = {'id': 'task1', 'type': 'model_training', 'data_size': 1000}
    result = await optimizer.distribute_task(task)
    _ = print(f"ä»»åŠ¡åˆ†å‘ç»“æœ: {result}")

    # è·å–é›†ç¾¤çŠ¶æ€
    _ = print("\né›†ç¾¤çŠ¶æ€:")
    cluster_status = optimizer.get_cluster_status()
    print(json.dumps(cluster_status, indent=2, ensure_ascii=False))

    _ = print("âœ… åˆ†å¸ƒå¼ä¼˜åŒ–å™¨é›†æˆæµ‹è¯•å®Œæˆ\n")

async def main() -> None:
    """ä¸»æµ‹è¯•å‡½æ•°"""
    _ = print("ğŸ”¬ å¼€å§‹æµ‹è¯•å¢å¼ºçš„åˆ†å¸ƒå¼è®­ç»ƒå®¹é”™æœºåˆ¶")
    _ = print(f"æµ‹è¯•å¼€å§‹æ—¶é—´: {datetime.now().isoformat()}")

    try:
    # ä¾æ¬¡è¿è¡Œå„é¡¹æµ‹è¯•
    _ = await test_fault_detector()
    _ = await test_checkpoint_manager()
    _ = await test_task_migrator()
    _ = await test_training_state_manager()
    _ = await test_distributed_optimizer_integration()

    print("=" * 50)
    _ = print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    _ = print(f"æµ‹è¯•ç»“æŸæ—¶é—´: {datetime.now().isoformat()}")
    print("=" * 50)

    except Exception as e:


    _ = logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    _ = print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    _ = asyncio.run(main())