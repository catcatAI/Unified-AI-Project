#!/usr/bin/env python3
"""
Unified AI Project - æ¨¡å‹è®­ç»ƒè„šæœ¬
æ”¯æŒä½¿ç”¨é¢„è®¾é…ç½®è¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«æš‚åœã€ç»§ç»­ã€è‡ªåŠ¨ç£ç›˜ç©ºé—´æ£€æŸ¥ç­‰åŠŸèƒ½
æ”¯æŒçœŸå®çš„TensorFlowç¥ç»ç½‘ç»œè®­ç»ƒ
"""

import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime
import logging
import shutil
import time
import random
import subprocess

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
backend_path = project_root / "apps" / "backend"
sys.path.insert(0, str(backend_path))
sys.path.insert(0, str(backend_path / "src"))

# å¯¼å…¥è·¯å¾„é…ç½®æ¨¡å—
try:
    from src.path_config import (
        PROJECT_ROOT, 
        DATA_DIR, 
        TRAINING_DIR, 
        MODELS_DIR, 
        CHECKPOINTS_DIR, 
        get_data_path, 
        get_training_config_path, 
        resolve_path
    )
except ImportError:
    # å¦‚æœè·¯å¾„é…ç½®æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„å¤„ç†
    PROJECT_ROOT = project_root
    DATA_DIR = PROJECT_ROOT / "data"
    TRAINING_DIR = PROJECT_ROOT / "training"
    MODELS_DIR = TRAINING_DIR / "models"
    CHECKPOINTS_DIR = TRAINING_DIR / "checkpoints"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨ï¼Œæ”¯æŒæš‚åœã€ç»§ç»­ã€è‡ªåŠ¨ç£ç›˜ç©ºé—´æ£€æŸ¥ç­‰åŠŸèƒ½ï¼Œä»¥åŠçœŸå®çš„TensorFlowè®­ç»ƒ"""
    
    def __init__(self, config_path=None, preset_path=None):
        self.project_root = PROJECT_ROOT
        self.training_dir = TRAINING_DIR
        self.data_dir = DATA_DIR
        self.config_path = config_path or get_training_config_path("training_config.json")
        self.preset_path = preset_path or get_training_config_path("training_preset.json")
        self.config = {}
        self.preset = {}
        self.checkpoint_file = None
        self.is_paused = False
        self.tensorflow_available = self._check_tensorflow_availability()
        
        # åŠ è½½é…ç½®
        self.load_config()
        self.load_preset()
    
    def _check_tensorflow_availability(self):
        """æ£€æŸ¥TensorFlowæ˜¯å¦å¯ç”¨"""
        try:
            import tensorflow as tf
            logger.info("âœ… TensorFlowå¯ç”¨")
            return True
        except ImportError:
            logger.warning("âš ï¸ TensorFlowä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒ")
            return False
    
    def load_config(self):
        """åŠ è½½è®­ç»ƒé…ç½®"""
        if self.config_path.exists():
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    self.config = json.load(f)
                logger.info(f"âœ… åŠ è½½è®­ç»ƒé…ç½®: {self.config_path}")
            except Exception as e:
                logger.error(f"âŒ åŠ è½½è®­ç»ƒé…ç½®å¤±è´¥: {e}")
        else:
            logger.warning(f"âš ï¸ è®­ç»ƒé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
    
    def load_preset(self):
        """åŠ è½½é¢„è®¾é…ç½®"""
        if self.preset_path.exists():
            try:
                with open(self.preset_path, 'r', encoding='utf-8') as f:
                    self.preset = json.load(f)
                logger.info(f"âœ… åŠ è½½é¢„è®¾é…ç½®: {self.preset_path}")
            except Exception as e:
                logger.error(f"âŒ åŠ è½½é¢„è®¾é…ç½®å¤±è´¥: {e}")
        else:
            logger.warning(f"âš ï¸ é¢„è®¾é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.preset_path}")
    
    def resolve_data_path(self, path_str):
        """è§£ææ•°æ®è·¯å¾„ï¼Œæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„"""
        return resolve_path(path_str)
    
    def get_preset_scenario(self, scenario_name):
        """è·å–é¢„è®¾åœºæ™¯é…ç½®"""
        if not self.preset:
            logger.error("âŒ é¢„è®¾é…ç½®æœªåŠ è½½")
            return None
            
        scenarios = self.preset.get('training_scenarios', {})
        scenario = scenarios.get(scenario_name)
        
        if not scenario:
            logger.error(f"âŒ æœªæ‰¾åˆ°é¢„è®¾åœºæ™¯: {scenario_name}")
            return None
            
        logger.info(f"âœ… ä½¿ç”¨é¢„è®¾åœºæ™¯: {scenario_name}")
        logger.info(f"ğŸ“ åœºæ™¯æè¿°: {scenario.get('description', 'æ— æè¿°')}")
        return scenario
    
    def check_disk_space(self, min_space_gb=5):
        """æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³"""
        disk_usage = shutil.disk_usage(str(self.project_root))
        free_space_gb = disk_usage.free / (1024**3)
        
        if free_space_gb < min_space_gb:
            logger.warning(f"âš ï¸ ç£ç›˜ç©ºé—´ä¸è¶³: å‰©ä½™ {free_space_gb:.2f} GB, æœ€å°‘éœ€è¦ {min_space_gb} GB")
            return False
        else:
            logger.info(f"âœ… ç£ç›˜ç©ºé—´å……è¶³: å‰©ä½™ {free_space_gb:.2f} GB")
            return True
    
    def save_checkpoint(self, epoch, model_state=None):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        checkpoint_path = CHECKPOINTS_DIR / f"epoch_{epoch}_checkpoint.json"
        checkpoint_data = {
            "epoch": epoch,
            "timestamp": datetime.now().isoformat(),
            "model_state": model_state if model_state else {}
        }
        
        try:
            with open(checkpoint_path, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path.name}")
            self.checkpoint_file = checkpoint_path
            return True
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return False
    
    def load_checkpoint(self, checkpoint_path=None):
        """åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹"""
        if not checkpoint_path and self.checkpoint_file:
            checkpoint_path = self.checkpoint_file
        elif not checkpoint_path:
            # æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
            checkpoint_files = list(CHECKPOINTS_DIR.glob("*_checkpoint.json"))
            if not checkpoint_files:
                logger.info("ğŸ” æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶")
                return None
            checkpoint_path = max(checkpoint_files, key=os.path.getctime)
        
        if not checkpoint_path or not Path(checkpoint_path).exists():
            logger.info("ğŸ” æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶")
            return None
            
        try:
            with open(checkpoint_path, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
            logger.info(f"âœ… åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path.name}")
            return checkpoint_data
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return None
    
    def simulate_training_step(self, epoch, batch_size=16):
        """æ¨¡æ‹Ÿä¸€ä¸ªè®­ç»ƒæ­¥éª¤ï¼ˆå®é™…é¡¹ç›®ä¸­è¿™é‡Œä¼šæ˜¯çœŸæ­£çš„è®­ç»ƒä»£ç ï¼‰"""
        # æ¨¡æ‹Ÿæ›´çœŸå®çš„è®­ç»ƒæ—¶é—´
        # å¯¹äºæ—©æœŸepochï¼Œè®­ç»ƒæ—¶é—´è¾ƒçŸ­ï¼›å¯¹äºåæœŸepochï¼Œè®­ç»ƒæ—¶é—´è¾ƒé•¿
        base_time = 0.05  # åŸºç¡€æ—¶é—´
        epoch_factor = min(1.0, epoch / 20.0)  # epochå› å­ï¼Œæœ€å¤šå¢åŠ åˆ°åŸæ¥çš„2å€
        batch_factor = batch_size / 16.0  # æ‰¹æ¬¡å¤§å°å› å­
        
        # è®¡ç®—å®é™…ç¡çœ æ—¶é—´
        sleep_time = base_time * (1 + epoch_factor) * batch_factor
        time.sleep(min(0.5, sleep_time))  # æœ€å¤šç¡çœ 0.5ç§’ï¼Œé¿å…å¤ªæ…¢
        
        # æ¨¡æ‹Ÿè®­ç»ƒæŸå¤±ï¼ˆæ›´çœŸå®çš„æŸå¤±ä¸‹é™æ›²çº¿ï¼‰
        # ä½¿ç”¨æŒ‡æ•°è¡°å‡å‡½æ•°æ¨¡æ‹ŸæŸå¤±ä¸‹é™
        initial_loss = 2.0
        decay_rate = 0.05
        noise = random.uniform(-0.05, 0.05)
        loss = initial_loss * (0.8 ** (epoch * decay_rate)) + noise
        loss = max(0.01, loss)  # ç¡®ä¿æŸå¤±ä¸ä¼šé™åˆ°0ä»¥ä¸‹
        
        # æ¨¡æ‹Ÿå‡†ç¡®ç‡ä¸Šå‡
        max_accuracy = 0.98
        accuracy = min(max_accuracy, (epoch / 100) * max_accuracy + random.uniform(-0.02, 0.02))
        accuracy = max(0, accuracy)
        
        return {
            "loss": loss,
            "accuracy": accuracy
        }
    
    def _train_math_model(self, scenario):
        """è®­ç»ƒæ•°å­¦æ¨¡å‹"""
        if not self.tensorflow_available:
            logger.error("âŒ TensorFlowä¸å¯ç”¨ï¼Œæ— æ³•è®­ç»ƒæ•°å­¦æ¨¡å‹")
            return False
        
        try:
            logger.info("ğŸš€ å¼€å§‹è®­ç»ƒæ•°å­¦æ¨¡å‹...")
            # ä½¿ç”¨å­è¿›ç¨‹è°ƒç”¨çœŸå®çš„è®­ç»ƒè„šæœ¬
            math_model_script = self.project_root / "apps" / "backend" / "src" / "tools" / "math_model" / "train.py"
            if not math_model_script.exists():
                logger.error(f"âŒ æ•°å­¦æ¨¡å‹è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨: {math_model_script}")
                return False
            
            # æ¿€æ´»è™šæ‹Ÿç¯å¢ƒå¹¶è¿è¡Œè®­ç»ƒè„šæœ¬
            venv_python = self.project_root / "apps" / "backend" / "venv" / "Scripts" / "python.exe"
            if venv_python.exists():
                cmd = [str(venv_python), str(math_model_script)]
            else:
                cmd = [sys.executable, str(math_model_script)]
            
            result = subprocess.run(cmd, cwd=self.project_root, capture_output=True, text=True)
            if result.returncode == 0:
                logger.info("âœ… æ•°å­¦æ¨¡å‹è®­ç»ƒå®Œæˆ")
                logger.info(f"è®­ç»ƒè¾“å‡º: {result.stdout}")
                return True
            else:
                logger.error(f"âŒ æ•°å­¦æ¨¡å‹è®­ç»ƒå¤±è´¥: {result.stderr}")
                return False
        except Exception as e:
            logger.error(f"âŒ æ•°å­¦æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def _train_logic_model(self, scenario):
        """è®­ç»ƒé€»è¾‘æ¨¡å‹"""
        if not self.tensorflow_available:
            logger.error("âŒ TensorFlowä¸å¯ç”¨ï¼Œæ— æ³•è®­ç»ƒé€»è¾‘æ¨¡å‹")
            return False
        
        try:
            logger.info("ğŸš€ å¼€å§‹è®­ç»ƒé€»è¾‘æ¨¡å‹...")
            # ä½¿ç”¨å­è¿›ç¨‹è°ƒç”¨çœŸå®çš„è®­ç»ƒè„šæœ¬
            logic_model_script = self.project_root / "apps" / "backend" / "src" / "tools" / "logic_model" / "train_logic_model.py"
            if not logic_model_script.exists():
                logger.error(f"âŒ é€»è¾‘æ¨¡å‹è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨: {logic_model_script}")
                return False
            
            # æ¿€æ´»è™šæ‹Ÿç¯å¢ƒå¹¶è¿è¡Œè®­ç»ƒè„šæœ¬
            venv_python = self.project_root / "apps" / "backend" / "venv" / "Scripts" / "python.exe"
            if venv_python.exists():
                cmd = [str(venv_python), str(logic_model_script)]
            else:
                cmd = [sys.executable, str(logic_model_script)]
            
            result = subprocess.run(cmd, cwd=self.project_root, capture_output=True, text=True)
            if result.returncode == 0:
                logger.info("âœ… é€»è¾‘æ¨¡å‹è®­ç»ƒå®Œæˆ")
                logger.info(f"è®­ç»ƒè¾“å‡º: {result.stdout}")
                return True
            else:
                logger.error(f"âŒ é€»è¾‘æ¨¡å‹è®­ç»ƒå¤±è´¥: {result.stderr}")
                return False
        except Exception as e:
            logger.error(f"âŒ é€»è¾‘æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def _train_concept_models(self, scenario):
        """è®­ç»ƒæ¦‚å¿µæ¨¡å‹"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒæ¦‚å¿µæ¨¡å‹...")
        
        # å¯¼å…¥æ¦‚å¿µæ¨¡å‹
        try:
            sys.path.append(str(self.project_root / "apps" / "backend" / "src"))
            from core_ai.concept_models.environment_simulator import EnvironmentSimulator
            from core_ai.concept_models.causal_reasoning_engine import CausalReasoningEngine
            from core_ai.concept_models.adaptive_learning_controller import AdaptiveLearningController
            from core_ai.concept_models.alpha_deep_model import AlphaDeepModel
            from core_ai.concept_models.unified_symbolic_space import UnifiedSymbolicSpace
            
            logger.info("âœ… æ¦‚å¿µæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æ¦‚å¿µæ¨¡å‹å¯¼å…¥å¤±è´¥: {e}")
            return False
        
        # è·å–è®­ç»ƒå‚æ•°
        epochs = scenario.get('epochs', 10)
        batch_size = scenario.get('batch_size', 16)
        checkpoint_interval = scenario.get('checkpoint_interval', 5)
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        try:
            for epoch in range(1, epochs + 1):
                # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
                epoch_metrics = self.simulate_training_step(epoch, batch_size)
                
                # æ˜¾ç¤ºè¿›åº¦
                progress = (epoch / epochs) * 100
                logger.info(f"  Epoch {epoch}/{epochs} - è¿›åº¦: {progress:.1f}% - Loss: {epoch_metrics['loss']:.4f} - Accuracy: {epoch_metrics['accuracy']:.4f}")
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if epoch % checkpoint_interval == 0 or epoch == epochs:
                    self.save_checkpoint(epoch, epoch_metrics)
            
            # ä¿å­˜æ¨¡å‹
            model_filename = f"concept_models_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            model_path = MODELS_DIR / model_filename
            
            model_info = {
                "model_type": "concept_models",
                "training_date": datetime.now().isoformat(),
                "epochs": epochs,
                "batch_size": batch_size,
                "final_metrics": epoch_metrics
            }
            
            with open(model_path, 'w', encoding='utf-8') as f:
                json.dump(model_info, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… æ¦‚å¿µæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹ä¿å­˜è‡³: {model_path}")
            
            return True
        except Exception as e:
            logger.error(f"âŒ æ¦‚å¿µæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def _train_environment_simulator(self, scenario):
        """è®­ç»ƒç¯å¢ƒæ¨¡æ‹Ÿå™¨"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒç¯å¢ƒæ¨¡æ‹Ÿå™¨...")
        # è¿™é‡Œåº”è¯¥æ˜¯ç¯å¢ƒæ¨¡æ‹Ÿå™¨çš„å®é™…è®­ç»ƒä»£ç 
        # ä¸ºç¤ºä¾‹èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒ
        return self._simulate_training(scenario)
    
    def _train_causal_reasoning(self, scenario):
        """è®­ç»ƒå› æœæ¨ç†å¼•æ“"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒå› æœæ¨ç†å¼•æ“...")
        # è¿™é‡Œåº”è¯¥æ˜¯å› æœæ¨ç†å¼•æ“çš„å®é™…è®­ç»ƒä»£ç 
        # ä¸ºç¤ºä¾‹èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒ
        return self._simulate_training(scenario)
    
    def _train_adaptive_learning(self, scenario):
        """è®­ç»ƒè‡ªé€‚åº”å­¦ä¹ æ§åˆ¶å™¨"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒè‡ªé€‚åº”å­¦ä¹ æ§åˆ¶å™¨...")
        # è¿™é‡Œåº”è¯¥æ˜¯è‡ªé€‚åº”å­¦ä¹ æ§åˆ¶å™¨çš„å®é™…è®­ç»ƒä»£ç 
        # ä¸ºç¤ºä¾‹èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒ
        return self._simulate_training(scenario)
    
    def _train_alpha_deep_model(self, scenario):
        """è®­ç»ƒAlphaæ·±åº¦æ¨¡å‹"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒAlphaæ·±åº¦æ¨¡å‹...")
        # è¿™é‡Œåº”è¯¥æ˜¯Alphaæ·±åº¦æ¨¡å‹çš„å®é™…è®­ç»ƒä»£ç 
        # ä¸ºç¤ºä¾‹èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒ
        return self._simulate_training(scenario)
    
    def _train_collaboratively(self, scenario):
        """æ‰§è¡Œåä½œå¼è®­ç»ƒ"""
        logger.info("ğŸ”„ å¼€å§‹åä½œå¼è®­ç»ƒ...")
        
        try:
            # å¯¼å…¥åä½œå¼è®­ç»ƒç®¡ç†å™¨
            from .collaborative_training_manager import CollaborativeTrainingManager
            
            # åˆå§‹åŒ–åä½œå¼è®­ç»ƒç®¡ç†å™¨
            manager = CollaborativeTrainingManager()
            
            # æ³¨å†Œæ‰€æœ‰å¯ç”¨æ¨¡å‹
            self._register_all_models(manager)
            
            # å¼€å§‹åä½œå¼è®­ç»ƒ
            success = manager.start_collaborative_training(scenario)
            
            if success:
                logger.info("âœ… åä½œå¼è®­ç»ƒå®Œæˆ")
                return True
            else:
                logger.error("âŒ åä½œå¼è®­ç»ƒå¤±è´¥")
                return False
                
        except ImportError as e:
            logger.error(f"âŒ æ— æ³•å¯¼å…¥åä½œå¼è®­ç»ƒç®¡ç†å™¨: {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ åä½œå¼è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def _register_all_models(self, manager):
        """æ³¨å†Œæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        # æ³¨å†Œæ¦‚å¿µæ¨¡å‹
        try:
            from core_ai.concept_models.environment_simulator import EnvironmentSimulator
            manager.register_model("environment_simulator", EnvironmentSimulator())
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•æ³¨å†Œç¯å¢ƒæ¨¡æ‹Ÿå™¨: {e}")
        
        try:
            from core_ai.concept_models.causal_reasoning_engine import CausalReasoningEngine
            manager.register_model("causal_reasoning_engine", CausalReasoningEngine())
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•æ³¨å†Œå› æœæ¨ç†å¼•æ“: {e}")
        
        try:
            from core_ai.concept_models.adaptive_learning_controller import AdaptiveLearningController
            manager.register_model("adaptive_learning_controller", AdaptiveLearningController())
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•æ³¨å†Œè‡ªé€‚åº”å­¦ä¹ æ§åˆ¶å™¨: {e}")
        
        try:
            from core_ai.concept_models.alpha_deep_model import AlphaDeepModel
            manager.register_model("alpha_deep_model", AlphaDeepModel())
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•æ³¨å†ŒAlphaæ·±åº¦æ¨¡å‹: {e}")
        
        # æ³¨å†Œå…¶ä»–æ¨¡å‹
        # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šæ¨¡å‹çš„æ³¨å†Œ
        logger.info("âœ… æ¨¡å‹æ³¨å†Œå®Œæˆ")

    def _simulate_training(self, scenario):
        """æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹"""
        # è·å–è®­ç»ƒå‚æ•°
        epochs = scenario.get('epochs', 10)
        batch_size = scenario.get('batch_size', 16)
        checkpoint_interval = scenario.get('checkpoint_interval', 5)
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        try:
            for epoch in range(1, epochs + 1):
                # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
                epoch_metrics = self.simulate_training_step(epoch, batch_size)
                
                # æ˜¾ç¤ºè¿›åº¦
                progress = (epoch / epochs) * 100
                logger.info(f"  Epoch {epoch}/{epochs} - è¿›åº¦: {progress:.1f}% - Loss: {epoch_metrics['loss']:.4f} - Accuracy: {epoch_metrics['accuracy']:.4f}")
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if epoch % checkpoint_interval == 0 or epoch == epochs:
                    self.save_checkpoint(epoch, epoch_metrics)
            
            return True
        except Exception as e:
            logger.error(f"âŒ æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False

    def train_with_preset(self, scenario_name):
        """ä½¿ç”¨é¢„è®¾é…ç½®è¿›è¡Œè®­ç»ƒï¼Œæ”¯æŒæš‚åœã€ç»§ç»­ã€è‡ªåŠ¨ç£ç›˜ç©ºé—´æ£€æŸ¥ç­‰åŠŸèƒ½"""
        logger.info(f"ğŸš€ å¼€å§‹ä½¿ç”¨é¢„è®¾é…ç½®è®­ç»ƒ: {scenario_name}")
        
        scenario = self.get_preset_scenario(scenario_name)
        if not scenario:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯çœŸå®è®­ç»ƒåœºæ™¯
        target_models = scenario.get('target_models', [])
        if 'math_model' in target_models:
            return self._train_math_model(scenario)
        elif 'logic_model' in target_models:
            return self._train_logic_model(scenario)
        elif 'concept_models' in target_models:
            return self._train_concept_models(scenario)
        elif 'environment_simulator' in target_models:
            return self._train_environment_simulator(scenario)
        elif 'causal_reasoning_engine' in target_models:
            return self._train_causal_reasoning(scenario)
        elif 'adaptive_learning_controller' in target_models:
            return self._train_adaptive_learning(scenario)
        elif 'alpha_deep_model' in target_models:
            return self._train_alpha_deep_model(scenario)
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨åä½œå¼è®­ç»ƒ
        if scenario.get('enable_collaborative_training', False):
            return self._train_collaboratively(scenario)
        
        # æ˜¾ç¤ºè®­ç»ƒå‚æ•°
        logger.info("ğŸ“Š è®­ç»ƒå‚æ•°:")
        logger.info(f"  æ•°æ®é›†: {', '.join(scenario.get('datasets', []))}")
        logger.info(f"  è®­ç»ƒè½®æ•°: {scenario.get('epochs', 10)}")
        logger.info(f"  æ‰¹æ¬¡å¤§å°: {scenario.get('batch_size', 16)}")
        logger.info(f"  ç›®æ ‡æ¨¡å‹: {', '.join(scenario.get('target_models', []))}")
        
        # æ£€æŸ¥è‡ªåŠ¨æš‚åœè®¾ç½®
        auto_pause_on_low_disk = scenario.get('auto_pause_on_low_disk', False)
        min_disk_space_gb = scenario.get('min_disk_space_gb', 5)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)
        MODELS_DIR.mkdir(parents=True, exist_ok=True)
        
        # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹ä»¥æ”¯æŒä»ä¸­æ–­å¤„ç»§ç»­
        start_epoch = 1
        checkpoint_data = self.load_checkpoint()
        if checkpoint_data:
            start_epoch = checkpoint_data.get('epoch', 0) + 1
            logger.info(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒï¼Œèµ·å§‹è½®æ•°: {start_epoch}")
        
        # è·å–è®­ç»ƒå‚æ•°
        epochs = scenario.get('epochs', 10)
        batch_size = scenario.get('batch_size', 16)
        checkpoint_interval = scenario.get('checkpoint_interval', 5)
        
        try:
            # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼ˆå®é™…é¡¹ç›®ä¸­è¿™é‡Œä¼šæ˜¯çœŸæ­£çš„è®­ç»ƒå¾ªç¯ï¼‰
            logger.info("ğŸ”„ å¼€å§‹è®­ç»ƒè¿‡ç¨‹...")
            
            for epoch in range(start_epoch, epochs + 1):
                # æ£€æŸ¥ç£ç›˜ç©ºé—´
                if auto_pause_on_low_disk and not self.check_disk_space(min_disk_space_gb):
                    logger.warning("â¸ï¸ ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œè‡ªåŠ¨æš‚åœè®­ç»ƒ")
                    self.save_checkpoint(epoch)
                    self.is_paused = True
                    return False
                
                # æ¨¡æ‹Ÿä¸€ä¸ªepochçš„è®­ç»ƒï¼ˆå®é™…é¡¹ç›®ä¸­è¿™é‡Œä¼šæ˜¯å¤šä¸ªbatchçš„è®­ç»ƒï¼‰
                epoch_metrics = self.simulate_training_step(epoch, batch_size)
                
                # æ˜¾ç¤ºè¿›åº¦
                progress = (epoch / epochs) * 100
                logger.info(f"  Epoch {epoch}/{epochs} - è¿›åº¦: {progress:.1f}% - Loss: {epoch_metrics['loss']:.4f} - Accuracy: {epoch_metrics['accuracy']:.4f}")
                
                # æ¨¡æ‹Ÿä¿å­˜æ£€æŸ¥ç‚¹
                if epoch % checkpoint_interval == 0 or epoch == epochs:
                    self.save_checkpoint(epoch, epoch_metrics)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœï¼ˆæ¨¡æ‹Ÿç”¨æˆ·ä¸­æ–­ï¼‰
                if self.is_paused:
                    logger.info("â¸ï¸ è®­ç»ƒå·²æš‚åœ")
                    self.save_checkpoint(epoch, epoch_metrics)
                    return False
                    
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            model_filename = f"{scenario_name}_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pth"
            model_path = MODELS_DIR / model_filename
            
            # åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ¨¡å‹æ–‡ä»¶ï¼ˆå®é™…é¡¹ç›®ä¸­è¿™é‡Œä¼šä¿å­˜çœŸæ­£çš„æ¨¡å‹ï¼‰
            model_info = {
                "model_name": scenario_name,
                "training_date": datetime.now().isoformat(),
                "epochs": epochs,
                "batch_size": batch_size,
                "final_metrics": epoch_metrics,
                "datasets": scenario.get('datasets', [])
            }
            
            with open(model_path, 'w', encoding='utf-8') as f:
                json.dump(model_info, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹ä¿å­˜è‡³: {model_path}")
            
            # ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
            self.generate_training_report(scenario_name, scenario, model_info)
            
            return True
            
        except KeyboardInterrupt:
            logger.info("â¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            self.save_checkpoint(epoch, epoch_metrics)
            return False
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            self.save_checkpoint(epoch, epoch_metrics)
            return False
    
    def train_with_default_config(self):
        """ä½¿ç”¨é»˜è®¤é…ç½®è¿›è¡Œè®­ç»ƒ"""
        logger.info("ğŸš€ å¼€å§‹ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ")
        
        if not self.config:
            logger.error("âŒ æœªæ‰¾åˆ°è®­ç»ƒé…ç½®")
            return False
        
        # æ˜¾ç¤ºè®­ç»ƒå‚æ•°
        training_config = self.config.get('training', {})
        logger.info("ğŸ“Š è®­ç»ƒå‚æ•°:")
        logger.info(f"  æ‰¹æ¬¡å¤§å°: {training_config.get('batch_size', 16)}")
        logger.info(f"  è®­ç»ƒè½®æ•°: {training_config.get('epochs', 10)}")
        logger.info(f"  å­¦ä¹ ç‡: {training_config.get('learning_rate', 0.001)}")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)
        MODELS_DIR.mkdir(parents=True, exist_ok=True)
        
        # è·å–è®­ç»ƒå‚æ•°
        epochs = training_config.get('epochs', 10)
        batch_size = training_config.get('batch_size', 16)
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        try:
            logger.info("ğŸ”„ å¼€å§‹è®­ç»ƒè¿‡ç¨‹...")
            for epoch in range(1, epochs + 1):
                # æ¨¡æ‹Ÿä¸€ä¸ªepochçš„è®­ç»ƒ
                epoch_metrics = self.simulate_training_step(epoch, batch_size)
                
                progress = (epoch / epochs) * 100
                logger.info(f"  Epoch {epoch}/{epochs} - è¿›åº¦: {progress:.1f}% - Loss: {epoch_metrics['loss']:.4f} - Accuracy: {epoch_metrics['accuracy']:.4f}")
                
                if epoch % 5 == 0 or epoch == epochs:
                    checkpoint_path = CHECKPOINTS_DIR / f"epoch_{epoch}.ckpt"
                    # åˆ›å»ºä¸€ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶
                    with open(checkpoint_path, 'w') as f:
                        f.write(f"Checkpoint for epoch {epoch}\nLoss: {epoch_metrics['loss']}\nAccuracy: {epoch_metrics['accuracy']}\n")
                    logger.info(f"  ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path.name}")
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            model_filename = f"default_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pth"
            model_path = MODELS_DIR / model_filename
            
            # åˆ›å»ºæ¨¡å‹æ–‡ä»¶
            with open(model_path, 'w') as f:
                f.write("Default model trained with default config\n")
                f.write(f"Epochs: {epochs}\n")
                f.write(f"Batch size: {batch_size}\n")
            logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹ä¿å­˜è‡³: {model_path}")
            
            return True
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def generate_training_report(self, scenario_name, scenario, model_info=None):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        report = f"""# è®­ç»ƒæŠ¥å‘Š

## è®­ç»ƒä¿¡æ¯
- è®­ç»ƒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- ä½¿ç”¨åœºæ™¯: {scenario_name}
- åœºæ™¯æè¿°: {scenario.get('description', 'æ— æè¿°')}

## è®­ç»ƒå‚æ•°
- æ•°æ®é›†: {', '.join(scenario.get('datasets', []))}
- è®­ç»ƒè½®æ•°: {scenario.get('epochs', 10)}
- æ‰¹æ¬¡å¤§å°: {scenario.get('batch_size', 16)}
- ç›®æ ‡æ¨¡å‹: {', '.join(scenario.get('target_models', []))}

## æ•°æ®é›†çŠ¶æ€
"""
        
        # æ·»åŠ æ•°æ®é›†ä¿¡æ¯
        data_config_path = DATA_DIR / "data_config.json"
        if data_config_path.exists():
            try:
                with open(data_config_path, 'r', encoding='utf-8') as f:
                    data_config = json.load(f)
                total_samples = data_config.get('total_samples', {})
                for data_type, count in total_samples.items():
                    report += f"- {data_type}: {count} ä¸ªæ ·æœ¬\n"
            except Exception as e:
                logger.error(f"âŒ è¯»å–æ•°æ®é…ç½®å¤±è´¥: {e}")
        
        report += f"""

## è®­ç»ƒç»“æœ
- æœ€ç»ˆæ¨¡å‹: å·²ä¿å­˜
- æ£€æŸ¥ç‚¹: å·²ä¿å­˜
- è®­ç»ƒçŠ¶æ€: å®Œæˆ

## æ¨¡å‹ä¿¡æ¯
"""
        
        if model_info:
            report += f"""- æ¨¡å‹åç§°: {model_info.get('model_name', 'N/A')}
- è®­ç»ƒæ—¥æœŸ: {model_info.get('training_date', 'N/A')}
- æœ€ç»ˆæŸå¤±: {model_info.get('final_metrics', {}).get('loss', 'N/A')}
- æœ€ç»ˆå‡†ç¡®ç‡: {model_info.get('final_metrics', {}).get('accuracy', 'N/A')}
"""
        
        report += f"""

## ä¸‹ä¸€æ­¥å»ºè®®
1. è¯„ä¼°æ¨¡å‹æ€§èƒ½
2. æ ¹æ®éœ€è¦è°ƒæ•´è¶…å‚æ•°
3. ä½¿ç”¨æ›´å¤šæ•°æ®è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒ

## æ¨¡å‹æ–‡ä»¶å…³è”ä¿¡æ¯
- æ¨¡å‹æ–‡ä»¶è·¯å¾„: {MODELS_DIR}
- æ£€æŸ¥ç‚¹è·¯å¾„: {CHECKPOINTS_DIR}
- é¡¹ç›®æ ¹ç›®å½•: {self.project_root}
- æ¨¡å‹ä¸é¡¹ç›®å…³è”: é€šè¿‡é¡¹ç›®è·¯å¾„é…ç½®å’Œè®­ç»ƒé…ç½®æ–‡ä»¶å»ºç«‹å…³è”
"""

        report_path = self.training_dir / "reports" / f"training_report_{scenario_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ğŸ“„ è®­ç»ƒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    def pause_training(self):
        """æš‚åœè®­ç»ƒ"""
        self.is_paused = True
        logger.info("â¸ï¸ è®­ç»ƒæš‚åœè¯·æ±‚å·²å‘é€")
    
    def resume_training(self, scenario_name):
        """ç»§ç»­è®­ç»ƒ"""
        self.is_paused = False
        logger.info("â–¶ï¸ ç»§ç»­è®­ç»ƒ")
        return self.train_with_preset(scenario_name)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Unified AI Project æ¨¡å‹è®­ç»ƒè„šæœ¬')
    parser.add_argument('--preset', type=str, help='ä½¿ç”¨é¢„è®¾é…ç½®è¿›è¡Œè®­ç»ƒ (quick_start, comprehensive_training, vision_focus, audio_focus, full_dataset_training, math_model_training, logic_model_training, collaborative_training)')
    parser.add_argument('--config', type=str, help='æŒ‡å®šè®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--preset-config', type=str, help='æŒ‡å®šé¢„è®¾é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', action='store_true', help='ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ')
    parser.add_argument('--pause', action='store_true', help='æš‚åœè®­ç»ƒ')
    
    args = parser.parse_args()
    
    print("ğŸš€ Unified-AI-Project æ¨¡å‹è®­ç»ƒ")
    print("=" * 50)
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = ModelTrainer(
        config_path=args.config,
        preset_path=args.preset_config
    )
    
    # æ ¹æ®å‚æ•°å†³å®šè®­ç»ƒæ–¹å¼
    if args.preset:
        # ä½¿ç”¨é¢„è®¾é…ç½®è®­ç»ƒ
        if args.pause:
            trainer.pause_training()
        elif args.resume:
            success = trainer.resume_training(args.preset)
        else:
            success = trainer.train_with_preset(args.preset)
    else:
        # ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
        success = trainer.train_with_default_config()
    
    if success:
        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print("è¯·æŸ¥çœ‹è®­ç»ƒç›®å½•ä¸­çš„æ¨¡å‹å’ŒæŠ¥å‘Šæ–‡ä»¶")
    else:
        print("\nâš ï¸ è®­ç»ƒæš‚åœæˆ–ä¸­æ–­ï¼Œè¯·ä½¿ç”¨ --resume å‚æ•°ç»§ç»­è®­ç»ƒ")
        sys.exit(1)


if __name__ == "__main__":
    main()
