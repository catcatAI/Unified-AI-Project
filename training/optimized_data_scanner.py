#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„æ•°æ®æ‰«æå·¥å…·
"""

import os
import json
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger: Any = logging.getLogger(__name__)

class OptimizedDataScanner:
    """ä¼˜åŒ–çš„æ•°æ®æ‰«æå™¨"""
    
    def __init__(self, data_dir: str, tracking_file: str = None, config_file: str = None) -> None:
        self.data_dir = Path(data_dir)
        self.tracking_file = Path(tracking_file) if tracking_file else Path("data_tracking.json")
        self.config_file = Path(config_file) if config_file else Path("performance_config.json")
        self.processed_files = {}
        self.scan_interval = 300  # é»˜è®¤æ‰«æé—´éš”ï¼ˆç§’ï¼‰
        _ = self._load_performance_config()
        _ = self._load_tracking_data()
    
    def _load_performance_config(self):
        """åŠ è½½æ€§èƒ½é…ç½®"""
        if self.config_file.exists():
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    data_scanning_config = config.get('data_scanning', {})
                    self.scan_interval = data_scanning_config.get('scan_interval_seconds', 300)
                _ = logger.info(f"âœ… åŠ è½½æ€§èƒ½é…ç½®: {self.config_file}")
            except Exception as e:
                _ = logger.error(f"âŒ åŠ è½½æ€§èƒ½é…ç½®å¤±è´¥: {e}")
    
    def _load_tracking_data(self):
        """åŠ è½½æ•°æ®è·Ÿè¸ªä¿¡æ¯"""
        if self.tracking_file.exists():
            try:
                with open(self.tracking_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.processed_files = {k: datetime.fromisoformat(v) for k, v in data.get('processed_files', {}).items()}
                _ = logger.info(f"âœ… åŠ è½½æ•°æ®è·Ÿè¸ªä¿¡æ¯: {self.tracking_file}")
            except Exception as e:
                _ = logger.error(f"âŒ åŠ è½½æ•°æ®è·Ÿè¸ªä¿¡æ¯å¤±è´¥: {e}")
    
    def _save_tracking_data(self):
        """ä¿å­˜æ•°æ®è·Ÿè¸ªä¿¡æ¯"""
        try:
            data = {
                'processed_files': {k: v.isoformat() for k, v in self.processed_files.items()},
                _ = 'updated_at': datetime.now().isoformat()
            }
            with open(self.tracking_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            _ = logger.error(f"âŒ ä¿å­˜æ•°æ®è·Ÿè¸ªä¿¡æ¯å¤±è´¥: {e}")
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                # è¯»å–æ–‡ä»¶å¹¶è®¡ç®—å“ˆå¸Œï¼Œä½†é™åˆ¶è¯»å–çš„æ•°æ®é‡ä»¥æé«˜æ€§èƒ½
                bytes_read = 0
                max_bytes = 10 * 1024 * 1024  # æœ€å¤šè¯»å–10MB
                
                while bytes_read < max_bytes:
                    chunk = f.read(4096)
                    if not chunk:
                        break
                    _ = hash_md5.update(chunk)
                    bytes_read += len(chunk)
            
            return hash_md5.hexdigest()
        except Exception as e:
            _ = logger.error(f"âŒ è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
            return ""
    
    def _get_file_info(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        try:
            stat = file_path.stat()
            # ç®€å•çš„æ–‡ä»¶ç±»å‹è¯†åˆ«
            suffix = file_path.suffix.lower()
            if suffix in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']:
                file_type = 'image'
            elif suffix in ['.mp3', '.wav', '.flac', '.aac', '.ogg']:
                file_type = 'audio'
            elif suffix in ['.txt', '.md', '.rst']:
                file_type = 'text'
            elif suffix in ['.json']:
                file_type = 'json'
            elif suffix in ['.py', '.js', '.java', '.cpp', '.h']:
                file_type = 'code'
            elif suffix in ['.pth', '.pt', '.h5', '.ckpt']:
                file_type = 'model'
            elif suffix in ['.zip', '.rar', '.7z', '.tar', '.gz']:
                file_type = 'archive'
            else:
                file_type = 'binary'
            
            return {
                _ = 'path': str(file_path),
                'size': stat.st_size,
                'modified_time': stat.st_mtime,
                'type': file_type
            }
        except Exception as e:
            _ = logger.error(f"âŒ è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {file_path}: {e}")
            return None
    
    def scan_recent_files(self, max_files: int = 5000, file_types: List[str] = None) -> List[Dict[str, Any]]:
        """
        æ‰«ææœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶
        
        Args:
            max_files: æœ€å¤§æ–‡ä»¶æ•°é‡
            file_types: è¦æ‰«æçš„æ–‡ä»¶ç±»å‹åˆ—è¡¨
            
        Returns:
            æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        """
        _ = logger.info(f"ğŸ” å¼€å§‹æ‰«ææœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶ï¼Œæœ€å¤š {max_files} ä¸ª...")
        
        files_info = []
        file_count = 0
        
        try:
            # éå†æ•°æ®ç›®å½•
            for root, dirs, files in os.walk(self.data_dir):
                # è·³è¿‡æŸäº›ç›®å½•ä»¥æé«˜æ€§èƒ½
                dirs[:] = [d for d in dirs if d not in ['.git', '__pycache__', 'node_modules']]
                
                for file in files:
                    if file_count >= max_files:
                        break
                    
                    file_path = Path(root) / file
                    file_info = self._get_file_info(file_path)
                    
                    if file_info:
                        # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶ç±»å‹ï¼Œè¿›è¡Œè¿‡æ»¤
                        if file_types and file_info['type'] not in file_types:
                            continue
                        
                        _ = files_info.append(file_info)
                        file_count += 1
                        
                        # æ¯å¤„ç†5000ä¸ªæ–‡ä»¶è¾“å‡ºä¸€æ¬¡è¿›åº¦
                        if file_count % 5000 == 0:
                            _ = logger.info(f"   å·²å¤„ç† {file_count} ä¸ªæ–‡ä»¶...")
                
                if file_count >= max_files:
                    break
            
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„æ–‡ä»¶
            files_info.sort(key=lambda x: x['modified_time'], reverse=True)
            files_info = files_info[:max_files]
            
            _ = logger.info(f"âœ… æ‰«æå®Œæˆï¼Œå…±å‘ç° {len(files_info)} ä¸ªæ–‡ä»¶")
            return files_info
            
        except Exception as e:
            _ = logger.error(f"âŒ æ‰«ææ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return []
    
    def find_new_files(self, max_files: int = 5000, file_types: List[str] = None) -> List[Dict[str, Any]]:
        """
        æŸ¥æ‰¾æ–°å¢æˆ–ä¿®æ”¹çš„æ–‡ä»¶
        
        Args:
            max_files: æœ€å¤§æ–‡ä»¶æ•°é‡
            file_types: è¦æ‰«æçš„æ–‡ä»¶ç±»å‹åˆ—è¡¨
            
        Returns:
            æ–°å¢æˆ–ä¿®æ”¹çš„æ–‡ä»¶åˆ—è¡¨
        """
        # æ‰«ææœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶
        files_info = self.scan_recent_files(max_files, file_types)
        
        new_files = []
        processed_count = 0
        hash_calculated_count = 0  # è®°å½•å®é™…è®¡ç®—å“ˆå¸Œçš„æ–‡ä»¶æ•°é‡
        
        # åˆ›å»ºå·²å¤„ç†æ–‡ä»¶çš„å¿«é€ŸæŸ¥æ‰¾ç´¢å¼•ï¼ˆåŸºäºä¿®æ”¹æ—¶é—´å’Œå¤§å°ï¼‰
        processed_file_lookup = {}
        for file_hash, processed_time in self.processed_files.items():
            processed_file_lookup[file_hash] = processed_time.isoformat() if isinstance(processed_time, datetime) else processed_time
        
        for file_info in files_info:
            file_path = Path(file_info['path'])
            
            # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            modified_time = datetime.fromtimestamp(file_info['modified_time'])
            
            # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœæ–‡ä»¶å¤§å°ä¸º0ï¼Œè·³è¿‡
            if file_info['size'] == 0:
                processed_count += 1
                continue
            
            # åŸºäºä¿®æ”¹æ—¶é—´å’Œå¤§å°åˆ›å»ºå¿«é€Ÿé”®
            quick_key = f"{file_info['size']}_{file_info['modified_time']}"
            
            # å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡ç›¸åŒå¤§å°å’Œä¿®æ”¹æ—¶é—´çš„æ–‡ä»¶
            needs_processing = True
            file_hash = None
            
            # å¦‚æœæœ‰å¿«é€Ÿç´¢å¼•ï¼Œå…ˆæ£€æŸ¥
            if quick_key in processed_file_lookup:
                # è¿›ä¸€æ­¥æ£€æŸ¥å¤„ç†æ—¶é—´
                processed_time_str = processed_file_lookup[quick_key]
                try:
                    processed_time = datetime.fromisoformat(processed_time_str) if isinstance(processed_time_str, str) else processed_time_str
                    if processed_time >= modified_time:
                        needs_processing = False
                except Exception:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œç»§ç»­è¿›è¡Œå“ˆå¸Œæ£€æŸ¥
                    pass
            
            # å¦‚æœéœ€è¦å¤„ç†ï¼Œåˆ™æ·»åŠ åˆ°æ–°æ–‡ä»¶åˆ—è¡¨
            if needs_processing:
                # åªæœ‰åœ¨å¿…è¦æ—¶æ‰è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼
                file_hash = self._calculate_file_hash(file_path)
                hash_calculated_count += 1
                
                if not file_hash:
                    processed_count += 1
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ç²¾ç¡®åŒ¹é…
                if file_hash in processed_file_lookup:
                    try:
                        processed_time_str = processed_file_lookup[file_hash]
                        processed_time = datetime.fromisoformat(processed_time_str) if isinstance(processed_time_str, str) else processed_time_str
                        if processed_time >= modified_time:
                            needs_processing = False
                    except Exception:
                        # å¦‚æœè§£æå¤±è´¥ï¼Œè®¤ä¸ºéœ€è¦å¤„ç†
                        pass
                
                # å¦‚æœä»ç„¶éœ€è¦å¤„ç†ï¼Œåˆ™æ·»åŠ åˆ°æ–°æ–‡ä»¶åˆ—è¡¨
                if needs_processing:
                    new_files.append({
                        _ = 'path': str(file_path),
                        'hash': file_hash,
                        _ = 'modified_time': modified_time.isoformat(),
                        'size': file_info['size'],
                        'type': file_info['type']
                    })
                    
                    # æ›´æ–°æŸ¥æ‰¾ç´¢å¼•ä»¥ä¾›åç»­å¿«é€Ÿæ£€æŸ¥
                    processed_file_lookup[file_hash] = modified_time.isoformat()
                    processed_file_lookup[quick_key] = modified_time.isoformat()
            
            processed_count += 1
            # æ¯å¤„ç†5000ä¸ªæ–‡ä»¶è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if processed_count % 5000 == 0:
                _ = logger.info(f"   å·²æ£€æŸ¥ {processed_count} ä¸ªæ–‡ä»¶... (è®¡ç®—å“ˆå¸Œ: {hash_calculated_count} ä¸ª)")
        
        _ = logger.info(f"âœ… æ£€æŸ¥å®Œæˆï¼Œå‘ç° {len(new_files)} ä¸ªæ–°å¢/ä¿®æ”¹æ–‡ä»¶ (è®¡ç®—å“ˆå¸Œ: {hash_calculated_count} ä¸ª)")
        return new_files
    
    def mark_as_processed(self, file_hash: str):
        """æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†"""
        self.processed_files[file_hash] = datetime.now()
        _ = self._save_tracking_data()
        _ = logger.debug(f"âœ… æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†: {file_hash}")