#!/usr/bin/env python3
"""
Angela AI ç³»çµ±è¶…ç´šè©³ç´°æª¢æŸ¥å·¥å…·
ç¢ºä¿ç³»çµ±å®Œå…¨æ²’æœ‰å•é¡Œï¼Œç”Ÿç”¢ç’°å¢ƒå°±ç·’
"""

import asyncio
import sys
import os
import json
import time
import traceback
from datetime import datetime
from pathlib import Path

# æ·»åŠ è·¯å¾‘
sys.path.append('apps/backend')
sys.path.append('apps/backend/src')

class ComprehensiveSystemChecker:
    def __init__(self):
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "overall_status": "UNKNOWN",
            "components": {},
            "performance": {},
            "integration": {},
            "errors": [],
            "recommendations": []
        }
        self.total_tests = 0
        self.passed_tests = 0
        
    def log_test(self, test_name: str, status: str, details: str = "", performance_ms: float = 0):
        """è¨˜éŒ„æ¸¬è©¦çµæœ"""
        self.total_tests += 1
        if status == "PASS":
            self.passed_tests += 1
            
        result = {
            "status": status,
            "details": details,
            "performance_ms": performance_ms,
            "timestamp": datetime.now().isoformat()
        }
        
        print(f"[{status}] {test_name}")
        if details:
            print(f"    {details}")
        if performance_ms > 0:
            print(f"    Performance: {performance_ms:.2f}ms")
            
        return result
        
    async def check_core_components(self):
        """æª¢æŸ¥æ ¸å¿ƒçµ„ä»¶"""
        print("\nğŸ” æª¢æŸ¥æ ¸å¿ƒçµ„ä»¶...")
        
        # æ¸¬è©¦å°å…¥
        start_time = time.time()
        try:
            from src.core.orchestrator import CognitiveOrchestrator
            from src.game.angela import Angela
            import_time = (time.time() - start_time) * 1000
            self.results["components"]["core_import"] = self.log_test(
                "Core Components Import", "PASS", 
                "All core components imported successfully", 
                import_time
            )
        except Exception as e:
            self.results["components"]["core_import"] = self.log_test(
                "Core Components Import", "FAIL", str(e)
            )
            self.results["errors"].append(f"Core import failed: {e}")
            return False
            
        # æ¸¬è©¦çµ„ä»¶åˆå§‹åŒ–
        start_time = time.time()
        try:
            orchestrator = CognitiveOrchestrator()
            angela = Angela()
            init_time = (time.time() - start_time) * 1000
            self.results["components"]["core_init"] = self.log_test(
                "Core Components Initialization", "PASS",
                "All core components initialized successfully",
                init_time
            )
            return True
        except Exception as e:
            self.results["components"]["core_init"] = self.log_test(
                "Core Components Initialization", "FAIL", str(e)
            )
            self.results["errors"].append(f"Core init failed: {e}")
            return False
            
    async def check_dialogue_system(self):
        """æª¢æŸ¥å°è©±ç³»çµ±"""
        print("\nğŸ—£ï¸ æª¢æŸ¥å°è©±ç³»çµ±...")
        
        try:
            from src.core.orchestrator import CognitiveOrchestrator
            orchestrator = CognitiveOrchestrator()
            
            # æ¸¬è©¦åŸºæœ¬å°è©±
            test_inputs = [
                "Hello",
                "What's your name?",
                "How are you?",
                "Can you help me?",
                "Thank you"
            ]
            
            total_time = 0
            for i, user_input in enumerate(test_inputs):
                start_time = time.time()
                try:
                    response = await orchestrator.process_user_input(user_input)
                    response_time = (time.time() - start_time) * 1000
                    total_time += response_time
                    
                    if response and "response" in response and len(response["response"]) > 0:
                        self.results["components"][f"dialogue_test_{i+1}"] = self.log_test(
                            f"Dialogue Test {i+1}: '{user_input}'", "PASS",
                            f"Response: {response['response'][:50]}...",
                            response_time
                        )
                    else:
                        self.results["components"][f"dialogue_test_{i+1}"] = self.log_test(
                            f"Dialogue Test {i+1}: '{user_input}'", "FAIL",
                            "Empty or invalid response"
                        )
                        
                except Exception as e:
                    self.results["components"][f"dialogue_test_{i+1}"] = self.log_test(
                        f"Dialogue Test {i+1}: '{user_input}'", "FAIL", str(e)
                    )
                    
            avg_time = total_time / len(test_inputs)
            self.results["performance"]["dialogue_avg_response_time"] = avg_time
            self.results["components"]["dialogue_overall"] = self.log_test(
                "Dialogue System Overall", "PASS" if avg_time < 5000 else "WARNING",
                f"Average response time: {avg_time:.2f}ms"
            )
            
        except Exception as e:
            self.results["components"]["dialogue_system"] = self.log_test(
                "Dialogue System", "FAIL", str(e)
            )
            self.results["errors"].append(f"Dialogue system failed: {e}")
            
    async def check_memory_system(self):
        """æª¢æŸ¥è¨˜æ†¶ç³»çµ±"""
        print("\nğŸ§  æª¢æŸ¥è¨˜æ†¶ç³»çµ±...")
        
        try:
            from src.core.orchestrator import CognitiveOrchestrator
            orchestrator = CognitiveOrchestrator()
            
            # æ¸¬è©¦å°è©±æ­·å²
            initial_count = len(orchestrator.conversation_history)
            
            # æ·»åŠ ä¸€äº›å°è©±
            await orchestrator.process_user_input("My name is Alice")
            await orchestrator.process_user_input("What's my name?")
            await orchestrator.process_user_input("Remember I like coffee")
            
            final_count = len(orchestrator.conversation_history)
            
            if final_count > initial_count:
                self.results["components"]["memory_conversation"] = self.log_test(
                    "Conversation Memory", "PASS",
                    f"History grew from {initial_count} to {final_count} messages"
                )
            else:
                self.results["components"]["memory_conversation"] = self.log_test(
                    "Conversation Memory", "FAIL", "Conversation history not growing"
                )
                
            # æ¸¬è©¦å¯¦é«”æå–
            entities = orchestrator._extract_entities_from_history()
            if entities:
                self.results["components"]["memory_entities"] = self.log_test(
                    "Entity Extraction", "PASS", f"Extracted entities: {entities}"
                )
            else:
                self.results["components"]["memory_entities"] = self.log_test(
                    "Entity Extraction", "WARNING", "No entities extracted"
                )
                
        except Exception as e:
            self.results["components"]["memory_system"] = self.log_test(
                "Memory System", "FAIL", str(e)
            )
            self.results["errors"].append(f"Memory system failed: {e}")
            
    async def check_llm_integration(self):
        """æª¢æŸ¥LLMé›†æˆ"""
        print("\nğŸ¤– æª¢æŸ¥LLMé›†æˆ...")
        
        try:
            from src.core.orchestrator import CognitiveOrchestrator
            orchestrator = CognitiveOrchestrator()
            
            # æª¢æŸ¥LLMå¯ç”¨æ€§
            if hasattr(orchestrator, 'llm_available'):
                if orchestrator.llm_available:
                    self.results["components"]["llm_availability"] = self.log_test(
                        "LLM Availability", "PASS", 
                        f"Available models: {getattr(orchestrator, 'available_models', [])}"
                    )
                else:
                    self.results["components"]["llm_availability"] = self.log_test(
                        "LLM Availability", "WARNING", "No LLM available, using rule-based responses"
                    )
            else:
                self.results["components"]["llm_availability"] = self.log_test(
                    "LLM Availability", "WARNING", "LLM availability check not implemented"
                )
                
            # æ¸¬è©¦å¯¦éš›éŸ¿æ‡‰ç”Ÿæˆ
            start_time = time.time()
            response = await orchestrator.process_user_input("Tell me something interesting")
            response_time = (time.time() - start_time) * 1000
            
            if response and "response" in response:
                self.results["components"]["llm_response"] = self.log_test(
                    "LLM Response Generation", "PASS",
                    f"Generated response: {response['response'][:50]}...",
                    response_time
                )
            else:
                self.results["components"]["llm_response"] = self.log_test(
                    "LLM Response Generation", "FAIL", "No response generated"
                )
                
        except Exception as e:
            self.results["components"]["llm_integration"] = self.log_test(
                "LLM Integration", "FAIL", str(e)
            )
            self.results["errors"].append(f"LLM integration failed: {e}")
            
    async def check_angela_character(self):
        """æª¢æŸ¥Angelaè§’è‰²ç³»çµ±"""
        print("\nğŸ‘¤ æª¢æŸ¥Angelaè§’è‰²ç³»çµ±...")
        
        try:
            from src.game.angela import Angela
            angela = Angela()
            
            # æ¸¬è©¦ç‹€æ…‹ç³»çµ±
            initial_favorability = angela.favorability
            initial_mood = angela.mood
            
            # æ¸¬è©¦å¢åŠ å¥½æ„Ÿåº¦
            angela.increase_favorability(10.0)
            if angela.favorability > initial_favorability:
                self.results["components"]["angela_favorability"] = self.log_test(
                    "Angela Favorability System", "PASS",
                    f"Favorability increased from {initial_favorability} to {angela.favorability}"
                )
            else:
                self.results["components"]["angela_favorability"] = self.log_test(
                    "Angela Favorability System", "FAIL", "Favorability not increasing"
                )
                
            # æ¸¬è©¦å°è©±åŠŸèƒ½
            try:
                response = await angela.get_dialogue("Hello", {"test": True})
                if response and len(response) > 0:
                    self.results["components"]["angela_dialogue"] = self.log_test(
                        "Angela Dialogue System", "PASS",
                        f"Dialogue response: {response[:50]}..."
                    )
                else:
                    self.results["components"]["angela_dialogue"] = self.log_test(
                        "Angela Dialogue System", "FAIL", "No dialogue response"
                    )
            except Exception as e:
                # æª¢æŸ¥æ˜¯å¦æ˜¯å› ç‚ºDialogueManagerä¸å­˜åœ¨
                if "DialogueManager" in str(e) or "placeholder" in str(e):
                    self.results["components"]["angela_dialogue"] = self.log_test(
                        "Angela Dialogue System", "WARNING", "Using placeholder DialogueManager"
                    )
                else:
                    self.results["components"]["angela_dialogue"] = self.log_test(
                        "Angela Dialogue System", "FAIL", str(e)
                    )
                    
            # æ¸¬è©¦ç¦®ç‰©ç³»çµ±
            gift_response = await angela.give_gift({"name": "rose", "value": 15, "type": "favorite"})
            if gift_response and len(gift_response) > 0:
                self.results["components"]["angela_gift_system"] = self.log_test(
                    "Angela Gift System", "PASS",
                    f"Gift response: {gift_response[:50]}..."
                )
            else:
                self.results["components"]["angela_gift_system"] = self.log_test(
                    "Angela Gift System", "FAIL", "No gift response"
                )
                
        except Exception as e:
            self.results["components"]["angela_character"] = self.log_test(
                "Angela Character System", "FAIL", str(e)
            )
            self.results["errors"].append(f"Angela character failed: {e}")
            
    async def check_performance_and_stability(self):
        """æª¢æŸ¥æ€§èƒ½å’Œç©©å®šæ€§"""
        print("\nâš¡ æª¢æŸ¥æ€§èƒ½å’Œç©©å®šæ€§...")
        
        try:
            from src.core.orchestrator import CognitiveOrchestrator
            orchestrator = CognitiveOrchestrator()
            
            # å£“åŠ›æ¸¬è©¦ - é€£çºŒè«‹æ±‚
            pressure_test_count = 20
            start_time = time.time()
            
            successful_responses = 0
            for i in range(pressure_test_count):
                try:
                    response = await orchestrator.process_user_input(f"Test message {i+1}")
                    if response and "response" in response:
                        successful_responses += 1
                except:
                    pass
                    
            total_time = (time.time() - start_time) * 1000
            avg_time = total_time / pressure_test_count
            
            if successful_responses >= pressure_test_count * 0.9:  # 90%æˆåŠŸç‡
                self.results["performance"]["pressure_test"] = self.log_test(
                    "Pressure Test (20 requests)", "PASS",
                    f"Success rate: {successful_responses}/{pressure_test_count}, Avg: {avg_time:.2f}ms",
                    total_time
                )
            else:
                self.results["performance"]["pressure_test"] = self.log_test(
                    "Pressure Test (20 requests)", "FAIL",
                    f"Low success rate: {successful_responses}/{pressure_test_count}",
                    total_time
                )
                
            # è¨˜æ†¶æ´©æ¼æ¸¬è©¦
            initial_history = len(orchestrator.conversation_history)
            for i in range(10):
                await orchestrator.process_user_input(f"Memory test {i+1}")
            
            final_history = len(orchestrator.conversation_history)
            expected_growth = 20  # 10 user + 10 assistant messages
            
            if final_history == initial_history + expected_growth:
                self.results["performance"]["memory_leak_test"] = self.log_test(
                    "Memory Leak Test", "PASS",
                    f"History grew as expected: {initial_history} -> {final_history}"
                )
            else:
                self.results["performance"]["memory_leak_test"] = self.log_test(
                    "Memory Leak Test", "WARNING",
                    f"Unexpected growth: {initial_history} -> {final_history}"
                )
                
        except Exception as e:
            self.results["performance"]["stability_test"] = self.log_test(
                "Performance and Stability", "FAIL", str(e)
            )
            self.results["errors"].append(f"Performance test failed: {e}")
            
    async def check_error_handling(self):
        """æª¢æŸ¥éŒ¯èª¤è™•ç†"""
        print("\nğŸ›¡ï¸ æª¢æŸ¥éŒ¯èª¤è™•ç†...")
        
        try:
            from src.core.orchestrator import CognitiveOrchestrator
            orchestrator = CognitiveOrchestrator()
            
            # æ¸¬è©¦ç©ºè¼¸å…¥
            response = await orchestrator.process_user_input("")
            if response and "response" in response:
                self.results["components"]["error_empty_input"] = self.log_test(
                    "Empty Input Handling", "PASS", "Empty input handled gracefully"
                )
            else:
                self.results["components"]["error_empty_input"] = self.log_test(
                    "Empty Input Handling", "FAIL", "Empty input not handled properly"
                )
                
            # æ¸¬è©¦è¶…é•·è¼¸å…¥
            long_input = "x" * 10000
            start_time = time.time()
            response = await orchestrator.process_user_input(long_input)
            response_time = (time.time() - start_time) * 1000
            
            if response and "response" in response:
                self.results["components"]["error_long_input"] = self.log_test(
                    "Long Input Handling", "PASS",
                    f"Long input handled in {response_time:.2f}ms",
                    response_time
                )
            else:
                self.results["components"]["error_long_input"] = self.log_test(
                    "Long Input Handling", "FAIL", "Long input not handled properly"
                )
                
            # æ¸¬è©¦ç‰¹æ®Šå­—ç¬¦
            special_input = "ğŸ¤– Test with ç‰¹æ®Š characters and Ã©mojis!"
            response = await orchestrator.process_user_input(special_input)
            if response and "response" in response:
                self.results["components"]["error_special_chars"] = self.log_test(
                    "Special Characters Handling", "PASS", "Special characters handled properly"
                )
            else:
                self.results["components"]["error_special_chars"] = self.log_test(
                    "Special Characters Handling", "FAIL", "Special characters not handled properly"
                )
                
        except Exception as e:
            self.results["components"]["error_handling"] = self.log_test(
                "Error Handling", "FAIL", str(e)
            )
            self.results["errors"].append(f"Error handling test failed: {e}")
            
    def check_file_structure(self):
        """æª¢æŸ¥æ–‡ä»¶çµæ§‹"""
        print("\nğŸ“ æª¢æŸ¥æ–‡ä»¶çµæ§‹...")
        
        required_files = [
            "apps/backend/src/core/orchestrator.py",
            "apps/backend/src/game/angela.py",
            "apps/backend/src/lu/logic_unit.py",
            "apps/backend/src/core/perception/receptor_system.py",
            "apps/backend/src/core/perception/synesthesia.py"
        ]
        
        missing_files = []
        for file_path in required_files:
            if Path(file_path).exists():
                self.results["components"][f"file_{file_path.replace('/', '_')}"] = self.log_test(
                    f"File: {file_path}", "PASS", "File exists"
                )
            else:
                missing_files.append(file_path)
                self.results["components"][f"file_{file_path.replace('/', '_')}"] = self.log_test(
                    f"File: {file_path}", "FAIL", "File missing"
                )
                
        if missing_files:
            self.results["errors"].append(f"Missing files: {missing_files}")
            
    def generate_recommendations(self):
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼éŒ¯èª¤ç”Ÿæˆå»ºè­°
        if self.results["errors"]:
            recommendations.append("ğŸ”§ Fix all critical errors before production deployment")
            
        # åŸºæ–¼æ€§èƒ½ç”Ÿæˆå»ºè­°
        if "dialogue_avg_response_time" in self.results["performance"]:
            avg_time = self.results["performance"]["dialogue_avg_response_time"]
            if avg_time > 3000:
                recommendations.append("âš¡ Optimize response generation - current average is slow")
            elif avg_time > 1000:
                recommendations.append("âš¡ Consider response time optimization")
                
        # åŸºæ–¼çµ„ä»¶ç‹€æ…‹ç”Ÿæˆå»ºè­°
        failed_components = [k for k, v in self.results["components"].items() 
                           if isinstance(v, dict) and v.get("status") == "FAIL"]
        if failed_components:
            recommendations.append(f"ğŸ”§ Address failing components: {failed_components}")
            
        # åŸºæ–¼æ¸¬è©¦è¦†è“‹ç‡ç”Ÿæˆå»ºè­°
        success_rate = (self.passed_tests / self.total_tests * 100) if self.total_tests > 0 else 0
        if success_rate < 90:
            recommendations.append(f"ğŸ“Š Improve test coverage - current success rate: {success_rate:.1f}%")
            
        self.results["recommendations"] = recommendations
        
    def calculate_overall_status(self):
        """è¨ˆç®—æ•´é«”ç‹€æ…‹"""
        success_rate = (self.passed_tests / self.total_tests * 100) if self.total_tests > 0 else 0
        
        if success_rate >= 95 and not self.results["errors"]:
            self.results["overall_status"] = "EXCELLENT - Production Ready"
        elif success_rate >= 85 and len(self.results["errors"]) <= 2:
            self.results["overall_status"] = "GOOD - Nearly Production Ready"
        elif success_rate >= 70:
            self.results["overall_status"] = "FAIR - Needs Improvement"
        else:
            self.results["overall_status"] = "POOR - Not Ready"
            
        self.results["success_rate"] = success_rate
        self.results["total_tests"] = self.total_tests
        self.results["passed_tests"] = self.passed_tests
        self.results["failed_tests"] = self.total_tests - self.passed_tests
        
    async def run_comprehensive_check(self):
        """é‹è¡Œå…¨é¢æª¢æŸ¥"""
        print("ğŸš€ Angela AI ç³»çµ±è¶…ç´šè©³ç´°æª¢æŸ¥é–‹å§‹...")
        print("=" * 60)
        
        # æ–‡ä»¶çµæ§‹æª¢æŸ¥
        self.check_file_structure()
        
        # æ ¸å¿ƒçµ„ä»¶æª¢æŸ¥
        core_ok = await self.check_core_components()
        if not core_ok:
            print("\nâŒ æ ¸å¿ƒçµ„ä»¶æª¢æŸ¥å¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒ")
            return self.results
            
        # ç³»çµ±çµ„ä»¶æª¢æŸ¥
        await self.check_dialogue_system()
        await self.check_memory_system()
        await self.check_llm_integration()
        await self.check_angela_character()
        await self.check_performance_and_stability()
        await self.check_error_handling()
        
        # ç”Ÿæˆå»ºè­°å’Œç¸½é«”ç‹€æ…‹
        self.generate_recommendations()
        self.calculate_overall_status()
        
        return self.results
        
    def print_final_report(self):
        """æ‰“å°æœ€çµ‚å ±å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š Angela AI ç³»çµ±æª¢æŸ¥å ±å‘Š")
        print("=" * 60)
        
        print(f"\nğŸ¯ æ•´é«”ç‹€æ…‹: {self.results.get('overall_status', 'UNKNOWN')}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {self.results.get('success_rate', 0):.1f}%")
        print(f"âœ… é€šéæ¸¬è©¦: {self.results.get('passed_tests', 0)}/{self.results.get('total_tests', 0)}")
        
        if self.results["errors"]:
            print(f"\nâŒ ç™¼ç¾éŒ¯èª¤ ({len(self.results['errors'])}):")
            for error in self.results["errors"]:
                print(f"   â€¢ {error}")
                
        if self.results["recommendations"]:
            print(f"\nğŸ’¡ æ”¹é€²å»ºè­° ({len(self.results['recommendations'])}):")
            for rec in self.results["recommendations"]:
                print(f"   {rec}")
                
        # æ€§èƒ½æ‘˜è¦
        if self.results["performance"]:
            print(f"\nâš¡ æ€§èƒ½æ‘˜è¦:")
            for key, value in self.results["performance"].items():
                if isinstance(value, dict) and "performance_ms" in value:
                    print(f"   â€¢ {key}: {value['performance_ms']:.2f}ms")
                    
        print(f"\nğŸ• æª¢æŸ¥å®Œæˆæ™‚é–“: {self.results['timestamp']}")
        
        # ç”Ÿç”¢å°±ç·’æ€§æª¢æŸ¥æ¸…å–®
        print(f"\nâœ… ç”Ÿç”¢å°±ç·’æ€§æª¢æŸ¥:")
        checklist = [
            (self.results['success_rate'] >= 90, "90%+ æ¸¬è©¦é€šéç‡"),
            (len(self.results['errors']) == 0, "ç„¡é—œéµéŒ¯èª¤"),
            (self.results.get('performance', {}).get('dialogue_avg_response_time', 0) < 5000, "éŸ¿æ‡‰æ™‚é–“ < 5ç§’"),
            (self.results.get('passed_tests', 0) >= 20, "è¶³å¤ çš„æ¸¬è©¦è¦†è“‹")
        ]
        
        ready_count = 0
        for condition, description in checklist:
            status = "âœ…" if condition else "âŒ"
            print(f"   {status} {description}")
            if condition:
                ready_count += 1
                
        print(f"\nğŸ¯ ç”Ÿç”¢å°±ç·’åº¦: {ready_count}/{len(checklist)} é …ç›®æ»¿è¶³")
        
        if ready_count == len(checklist):
            print("ğŸ‰ ç³»çµ±å®Œå…¨å°±ç·’ï¼Œå¯ä»¥éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒï¼")
        elif ready_count >= len(checklist) * 0.75:
            print("âš ï¸ ç³»çµ±åŸºæœ¬å°±ç·’ï¼Œå»ºè­°ä¿®å¾©å‰©é¤˜å•é¡Œå¾Œéƒ¨ç½²")
        else:
            print("ğŸ”§ ç³»çµ±éœ€è¦é‡å¤§æ”¹é€²ï¼Œä¸å»ºè­°éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ")

async def main():
    checker = ComprehensiveSystemChecker()
    results = await checker.run_comprehensive_check()
    checker.print_final_report()
    
    # ä¿å­˜è©³ç´°å ±å‘Š
    report_path = "COMPREHENSIVE_ANGELA_AI_CHECK_REPORT.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    return results

if __name__ == "__main__":
    asyncio.run(main())