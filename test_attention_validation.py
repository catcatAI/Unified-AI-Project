#!/usr/bin/env python3
"""
æ³¨æ„åŠ›æœºåˆ¶éªŒè¯å™¨
éªŒè¯æ³¨æ„åŠ›æƒé‡çš„æ­£ç¡®æ€§å’Œåˆç†æ€§
"""

import sys
sys.path.append('apps/backend/src')

import numpy as np
import asyncio
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

@dataclass
class AttentionValidationResult:
    """æ³¨æ„åŠ›éªŒè¯ç»“æœ"""
    is_valid: bool
    attention_entropy: float
    attention_variance: float
    max_attention_weight: float
    min_attention_weight: float
    attention_concentration: float
    validation_score: float
    issues: List[str]

class AttentionMechanismValidator:
    """æ³¨æ„åŠ›æœºåˆ¶éªŒè¯å™¨"""
    
    def __init__(self):
        self.validation_thresholds = {
            'max_entropy': 3.0,           # æœ€å¤§ç†µå€¼
            'min_entropy': 0.1,           # æœ€å°ç†µå€¼
            'max_variance': 0.15,         # æœ€å¤§æ–¹å·®
            'min_max_attention': 0.1,     # æœ€å¤§æ³¨æ„åŠ›æƒé‡çš„æœ€å°å€¼
            'max_concentration': 0.8,     # æœ€å¤§é›†ä¸­åº¦
            'min_validation_score': 0.6   # æœ€å°éªŒè¯åˆ†æ•°
        }
    
    def validate_attention_weights(
        self,
        attention_weights: np.ndarray,
        tokens: List[str],
        layer_info: Optional[Dict[str, Any]] = None
    ) -> AttentionValidationResult:
        """
        éªŒè¯æ³¨æ„åŠ›æƒé‡
        
        Args:
            attention_weights: æ³¨æ„åŠ›æƒé‡çŸ©é˜µ [num_heads, seq_len, seq_len]
            tokens: tokenåˆ—è¡¨
            layer_info: å±‚ä¿¡æ¯
            
        Returns:
            éªŒè¯ç»“æœ
        """
        print(f"å¼€å§‹éªŒè¯æ³¨æ„åŠ›æƒé‡ï¼Œæƒé‡å½¢çŠ¶: {attention_weights.shape}")
        
        issues = []
        
        # åŸºæœ¬æ£€æŸ¥
        if attention_weights.size == 0:
            issues.append("æ³¨æ„åŠ›æƒé‡ä¸ºç©º")
            return AttentionValidationResult(
                is_valid=False,
                attention_entropy=0.0,
                attention_variance=0.0,
                max_attention_weight=0.0,
                min_attention_weight=0.0,
                attention_concentration=0.0,
                validation_score=0.0,
                issues=issues
            )
        
        # æ£€æŸ¥æƒé‡èŒƒå›´
        if np.any(attention_weights < 0):
            issues.append("å­˜åœ¨è´Ÿçš„æ³¨æ„åŠ›æƒé‡")
        
        if np.any(attention_weights > 1):
            issues.append("æ³¨æ„åŠ›æƒé‡å¤§äº1")
        
        # æ£€æŸ¥æƒé‡å’Œæ˜¯å¦ä¸º1ï¼ˆå½’ä¸€åŒ–æ£€æŸ¥ï¼‰
        weight_sums = np.sum(attention_weights, axis=-1)
        if not np.allclose(weight_sums, 1.0, atol=1e-6):
            issues.append("æ³¨æ„åŠ›æƒé‡æœªæ­£ç¡®å½’ä¸€åŒ–")
        
        # è®¡ç®—æ³¨æ„åŠ›ç»Ÿè®¡ç‰¹å¾
        attention_entropy = self._calculate_attention_entropy(attention_weights)
        attention_variance = self._calculate_attention_variance(attention_weights)
        max_attention_weight = np.max(attention_weights)
        min_attention_weight = np.min(attention_weights)
        attention_concentration = self._calculate_attention_concentration(attention_weights)
        
        # éªŒè¯å„ä¸ªæŒ‡æ ‡
        validation_score = self._calculate_validation_score(
            attention_entropy, attention_variance, max_attention_weight, 
            min_attention_weight, attention_concentration, issues
        )
        
        # åˆ¤æ–­æ•´ä½“æœ‰æ•ˆæ€§
        is_valid = (
            validation_score >= self.validation_thresholds['min_validation_score'] and
            len(issues) == 0  # æ²¡æœ‰ä¸¥é‡é—®é¢˜
        )
        
        result = AttentionValidationResult(
            is_valid=is_valid,
            attention_entropy=attention_entropy,
            attention_variance=attention_variance,
            max_attention_weight=max_attention_weight,
            min_attention_weight=min_attention_weight,
            attention_concentration=attention_concentration,
            validation_score=validation_score,
            issues=issues
        )
        
        print(f"âœ“ æ³¨æ„åŠ›éªŒè¯å®Œæˆï¼Œæœ‰æ•ˆæ€§: {is_valid}, éªŒè¯åˆ†æ•°: {validation_score:.3f}")
        if issues:
            print(f"  å‘ç°çš„é—®é¢˜: {issues}")
        
        return result
    
    def _calculate_attention_entropy(self, attention_weights: np.ndarray) -> float:
        """è®¡ç®—æ³¨æ„åŠ›ç†µ"""
        # é¿å…log(0)
        weights_clipped = np.clip(attention_weights, 1e-10, 1.0)
        
        # è®¡ç®—æ¯ä¸ªä½ç½®çš„ç†µ
        entropy_per_position = -np.sum(weights_clipped * np.log(weights_clipped), axis=-1)
        
        # è¿”å›å¹³å‡ç†µ
        return float(np.mean(entropy_per_position))
    
    def _calculate_attention_variance(self, attention_weights: np.ndarray) -> float:
        """è®¡ç®—æ³¨æ„åŠ›æ–¹å·®"""
        return float(np.var(attention_weights))
    
    def _calculate_attention_concentration(self, attention_weights: np.ndarray) -> float:
        """è®¡ç®—æ³¨æ„åŠ›é›†ä¸­åº¦"""
        # è®¡ç®—æ¯ä¸ªä½ç½®çš„æœ€å¤§æ³¨æ„åŠ›æƒé‡
        max_weights_per_position = np.max(attention_weights, axis=-1)
        
        # è¿”å›å¹³å‡æœ€å¤§æƒé‡ä½œä¸ºé›†ä¸­åº¦æŒ‡æ ‡
        return float(np.mean(max_weights_per_position))
    
    def _calculate_validation_score(
        self,
        entropy: float,
        variance: float,
        max_weight: float,
        min_weight: float,
        concentration: float,
        issues: List[str]
    ) -> float:
        """è®¡ç®—éªŒè¯åˆ†æ•°"""
        
        # ç†µå€¼è¯„åˆ†ï¼ˆé€‚ä¸­ä¸ºå¥½ï¼‰
        if entropy < self.validation_thresholds['min_entropy']:
            entropy_score = 0.3
        elif entropy > self.validation_thresholds['max_entropy']:
            entropy_score = 0.5
        else:
            entropy_score = 1.0
        
        # æ–¹å·®è¯„åˆ†ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        if variance > self.validation_thresholds['max_variance']:
            variance_score = 0.3
        else:
            variance_score = 1.0 - (variance / self.validation_thresholds['max_variance'])
        
        # æœ€å¤§æƒé‡è¯„åˆ†ï¼ˆä¸èƒ½å¤ªå°ä¹Ÿä¸èƒ½å¤ªå¤§ï¼‰
        if max_weight < self.validation_thresholds['min_max_attention']:
            max_weight_score = 0.2
        elif max_weight > 0.9:  # è¿‡äºé›†ä¸­
            max_weight_score = 0.6
        else:
            max_weight_score = 0.8
        
        # é›†ä¸­åº¦è¯„åˆ†ï¼ˆé€‚ä¸­ä¸ºå¥½ï¼‰
        if concentration > self.validation_thresholds['max_concentration']:
            concentration_score = 0.3
        else:
            concentration_score = 1.0 - (concentration / self.validation_thresholds['max_concentration'])
        
        # ç»¼åˆåˆ†æ•°
        base_score = (
            entropy_score * 0.3 +
            variance_score * 0.25 +
            max_weight_score * 0.25 +
            concentration_score * 0.2
        )
        
        # æ ¹æ®é—®é¢˜æ•°é‡è°ƒæ•´åˆ†æ•°
        issue_penalty = len(issues) * 0.2
        final_score = max(0.0, base_score - issue_penalty)
        
        return final_score
    
    def analyze_attention_patterns(
        self,
        attention_weights: np.ndarray,
        tokens: List[str],
        layer_idx: int = 0
    ) -> Dict[str, Any]:
        """åˆ†ææ³¨æ„åŠ›æ¨¡å¼"""
        print(f"åˆ†æç¬¬{layer_idx}å±‚æ³¨æ„åŠ›æ¨¡å¼...")
        
        num_heads, seq_len, _ = attention_weights.shape
        
        # è®¡ç®—å„ç§æ³¨æ„åŠ›æ¨¡å¼æŒ‡æ ‡
        patterns = {
            "layer_index": layer_idx,
            "num_heads": num_heads,
            "sequence_length": seq_len,
            "attention_heads_analysis": []
        }
        
        for head_idx in range(num_heads):
            head_weights = attention_weights[head_idx]
            
            # è®¡ç®—è¯¥æ³¨æ„åŠ›å¤´çš„ç»Ÿè®¡ç‰¹å¾
            head_entropy = self._calculate_attention_entropy(head_weights[np.newaxis, :, :])
            head_variance = self._calculate_attention_variance(head_weights[np.newaxis, :, :])
            head_concentration = self._calculate_attention_concentration(head_weights[np.newaxis, :, :])
            
            # åˆ¤æ–­æ³¨æ„åŠ›å¤´ç±»å‹
            if head_concentration > 0.8:
                head_type = "highly_concentrated"
            elif head_entropy < 1.0:
                head_type = "focused"
            elif head_entropy > 2.5:
                head_type = "dispersed"
            else:
                head_type = "balanced"
            
            patterns["attention_heads_analysis"].append({
                "head_index": head_idx,
                "entropy": head_entropy,
                "variance": head_variance,
                "concentration": head_concentration,
                "head_type": head_type,
                "is_valid": head_entropy >= self.validation_thresholds['min_entropy'] and
                           head_variance <= self.validation_thresholds['max_variance']
            })
        
        # æ•´ä½“æ¨¡å¼åˆ†æ
        entropies = [head["entropy"] for head in patterns["attention_heads_analysis"]]
        variances = [head["variance"] for head in patterns["attention_heads_analysis"]]
        
        patterns["overall_analysis"] = {
            "average_entropy": float(np.mean(entropies)),
            "entropy_std": float(np.std(entropies)),
            "average_variance": float(np.mean(variances)),
            "variance_std": float(np.std(variances)),
            "valid_heads_ratio": sum(1 for head in patterns["attention_heads_analysis"] if head["is_valid"]) / num_heads
        }
        
        return patterns
    
    def detect_attention_anomalies(
        self,
        attention_weights: np.ndarray,
        tokens: List[str]
    ) -> List[Dict[str, Any]]:
        """æ£€æµ‹æ³¨æ„åŠ›å¼‚å¸¸"""
        anomalies = []
        
        num_heads, seq_len, _ = attention_weights.shape
        
        for head_idx in range(num_heads):
            head_weights = attention_weights[head_idx]
            
            # æ£€æŸ¥å¼‚å¸¸æ¨¡å¼
            if np.allclose(head_weights, 1.0 / seq_len):
                anomalies.append({
                    "type": "uniform_attention",
                    "head_index": head_idx,
                    "description": "æ³¨æ„åŠ›æƒé‡å®Œå…¨å‡åŒ€åˆ†å¸ƒ",
                    "severity": "medium"
                })
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¿‡é«˜çš„æ³¨æ„åŠ›æƒé‡
            max_attention = np.max(head_weights)
            if max_attention > 0.95:
                max_positions = np.where(head_weights == max_attention)
                anomalies.append({
                    "type": "excessive_attention",
                    "head_index": head_idx,
                    "description": f"æ³¨æ„åŠ›æƒé‡è¿‡é«˜ ({max_attention:.3f})",
                    "max_positions": list(zip(max_positions[0], max_positions[1])),
                    "severity": "high"
                })
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¿‡ä½çš„æ³¨æ„åŠ›æƒé‡
            min_attention = np.min(head_weights)
            if min_attention < 0.01:
                anomalies.append({
                    "type": "insufficient_attention",
                    "head_index": head_idx,
                    "description": f"æ³¨æ„åŠ›æƒé‡è¿‡ä½ ({min_attention:.3f})",
                    "severity": "medium"
                })
        
        return anomalies


async def test_attention_mechanism_validation():
    """æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶éªŒè¯"""
    print("=== å¼€å§‹æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶éªŒè¯ ===\n")
    
    validator = AttentionMechanismValidator()
    
    # æµ‹è¯•1: æ­£å¸¸çš„æ³¨æ„åŠ›æƒé‡
    print("--- æµ‹è¯•1: æ­£å¸¸æ³¨æ„åŠ›æƒé‡ ---")
    normal_weights = np.array([
        [[0.1, 0.2, 0.3, 0.4],
         [0.15, 0.25, 0.35, 0.25],
         [0.2, 0.2, 0.3, 0.3],
         [0.25, 0.25, 0.25, 0.25]]
    ])
    tokens = ["The", "cat", "sat", "mat"]
    
    result = validator.validate_attention_weights(normal_weights, tokens)
    print(f"âœ“ æ­£å¸¸æƒé‡éªŒè¯ç»“æœ: æœ‰æ•ˆæ€§={result.is_valid}, åˆ†æ•°={result.validation_score:.3f}")
    print(f"  ç†µå€¼: {result.attention_entropy:.3f}, æ–¹å·®: {result.attention_variance:.3f}")
    print(f"  æœ€å¤§æƒé‡: {result.max_attention_weight:.3f}, é›†ä¸­åº¦: {result.attention_concentration:.3f}")
    
    # æµ‹è¯•2: è¿‡åº¦é›†ä¸­çš„æ³¨æ„åŠ›
    print("\n--- æµ‹è¯•2: è¿‡åº¦é›†ä¸­çš„æ³¨æ„åŠ› ---")
    concentrated_weights = np.array([
        [[0.01, 0.01, 0.95, 0.03],
         [0.02, 0.02, 0.94, 0.02],
         [0.01, 0.01, 0.96, 0.02],
         [0.02, 0.02, 0.94, 0.02]]
    ])
    
    result = validator.validate_attention_weights(concentrated_weights, tokens)
    print(f"âœ“ é›†ä¸­æƒé‡éªŒè¯ç»“æœ: æœ‰æ•ˆæ€§={result.is_valid}, åˆ†æ•°={result.validation_score:.3f}")
    if result.issues:
        print(f"  é—®é¢˜: {result.issues}")
    
    # æµ‹è¯•3: æ³¨æ„åŠ›æ¨¡å¼åˆ†æ
    print("\n--- æµ‹è¯•3: æ³¨æ„åŠ›æ¨¡å¼åˆ†æ ---")
    multi_head_weights = np.random.dirichlet(np.ones(4), size=(4, 4))  # éšæœºä½†æœ‰æ•ˆçš„æ³¨æ„åŠ›æƒé‡
    
    patterns = validator.analyze_attention_patterns(multi_head_weights, tokens)
    print(f"âœ“ åˆ†æäº† {patterns['num_heads']} ä¸ªæ³¨æ„åŠ›å¤´")
    print(f"âœ“ å¹³å‡ç†µå€¼: {patterns['overall_analysis']['average_entropy']:.3f}")
    print(f"âœ“ æœ‰æ•ˆæ³¨æ„åŠ›å¤´æ¯”ä¾‹: {patterns['overall_analysis']['valid_heads_ratio']:.1%}")
    
    # æµ‹è¯•4: å¼‚å¸¸æ£€æµ‹
    print("\n--- æµ‹è¯•4: å¼‚å¸¸æ£€æµ‹ ---")
    # åˆ›å»ºæœ‰å¼‚å¸¸çš„æ³¨æ„åŠ›æƒé‡
    anomalous_weights = np.array([
        [[0.25, 0.25, 0.25, 0.25],  # æ­£å¸¸
         [0.99, 0.003, 0.003, 0.004],  # è¿‡åº¦é›†ä¸­
         [0.0, 0.0, 0.0, 1.0],  # å®Œå…¨é›†ä¸­
         [0.25, 0.25, 0.25, 0.25]]  # æ­£å¸¸
    ])
    
    anomalies = validator.detect_attention_anomalies(anomalous_weights, tokens)
    print(f"âœ“ æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸")
    for anomaly in anomalies:
        print(f"  å¼‚å¸¸ç±»å‹: {anomaly['type']}, ä¸¥é‡ç¨‹åº¦: {anomaly['severity']}")
    
    print("\n=== æ³¨æ„åŠ›æœºåˆ¶éªŒè¯æµ‹è¯•å®Œæˆ ===")
    return True


if __name__ == '__main__':
    success = asyncio.run(test_attention_mechanism_validation())
    if success:
        print("\nğŸ‰ æ³¨æ„åŠ›æœºåˆ¶éªŒè¯ç³»ç»Ÿå·¥ä½œæ­£å¸¸ï¼")
        sys.exit(0)
    else:
        print("\nâŒ æ³¨æ„åŠ›æœºåˆ¶éªŒè¯ç³»ç»Ÿå­˜åœ¨é—®é¢˜")
        sys.exit(1)