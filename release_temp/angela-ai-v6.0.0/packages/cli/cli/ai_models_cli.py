#!/usr/bin/env python3
"""
AI æ¨¡å‹ CLI å·¥å…·
æ”¯æŒå¤šç§ AI å¤§æ¨¡å‹çš„å‘½ä»¤è¡Œæ¥å£,åŒ…æ‹¬ OpenAI GPTã€Google Geminiã€Anthropic Claudeã€Ollama ç­‰
"""

import asyncio
import argparse
import json
import sys
import os
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))
backend_path = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'apps', 'backend')
sys.path.insert(0, backend_path)

from apps.backend.src.services.multi_llm_service import (
    MultiLLMService, ChatMessage, ModelProvider, ModelConfig
)

# é…ç½®æ—¥å¿—
logging.basicConfig(,
    level=logging.INFO(),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AIModelsCLI,
    """AI æ¨¡å‹ CLI å·¥å…·"""

    def __init__(self):
    self.service, Optional[MultiLLMService] = None
    self.config_path = os.path.join(,
    os.path.dirname(__file__), '..', '..', '..', 'configs', 'multi_llm_config.json'
    )

    async def initialize(self):
    """åˆå§‹åŒ–æœåŠ¡"""
        try,

            self.service == MultiLLMService(self.config_path())
            logger.info("AI æ¨¡å‹æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e,::
            logger.error(f"åˆå§‹åŒ–å¤±è´¥, {e}")
            sys.exit(1)

    async def list_models(self, args):
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        if not self.service,::
    await self.initialize()

    models = self.service.get_available_models()

    print("\nğŸ¤– å¯ç”¨çš„ AI æ¨¡å‹,")
    print("=" * 50)

        for model_id in models,::
    info = self.service.get_model_info(model_id)
            status == "âœ… å¯ç”¨" if info['enabled'] else "âŒ ç¦ç”¨":::
    print(f"\nğŸ“‹ æ¨¡å‹ ID, {model_id}")
            print(f"   æä¾›å•†, {info['provider']}")
            print(f"   æ¨¡å‹å, {info['model_name']}")
            print(f"   çŠ¶æ€, {status}")
            print(f"   æœ€å¤§ Token, {info['max_tokens']}")
            print(f"   ä¸Šä¸‹æ–‡çª—å£, {info['context_window']}")
            print(f"   æˆæœ¬/1K Token, ${info['cost_per_1k_tokens']}")

            stats = info.get('usage_stats', {})
            if stats.get('total_requests', 0) > 0,::
    print(f"   ä½¿ç”¨ç»Ÿè®¡,")
                print(f"     - æ€»è¯·æ±‚, {stats['total_requests']}")
                print(f"     - æ€» Token, {stats['total_tokens']}")
                print(f"     - æ€»æˆæœ¬, ${stats['total_cost'].4f}")
                print(f"     - å¹³å‡å»¶è¿Ÿ, {stats['average_latency'].2f}s")
                print(f"     - é”™è¯¯æ¬¡æ•°, {stats['error_count']}")

    async def chat(self, args):
    """èŠå¤©æ¨¡å¼"""
        if not self.service,::
    await self.initialize()

    model_id = args.model or self.service.default_model()
    print(f"\nğŸ¤– å¼€å§‹ä¸ {model_id} èŠå¤© (è¾“å…¥ 'quit' é€€å‡º, 'clear' æ¸…ç©ºå†å²)")
    print("=" * 60)

    messages, List[ChatMessage] = []

    # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        if args.system,::
    messages.append(ChatMessage(role="system", content=args.system()))

        while True,::
    try,



                user_input == input("\nğŸ‘¤ æ‚¨, ").strip()

                if user_input.lower() in ['quit', 'exit', 'q']::
    print("ğŸ‘‹ å†è§!")
                    break

                if user_input.lower() in ['clear', 'c']::
    messages = []
                    if args.system,::
    messages.append(ChatMessage(role="system", content=args.system()))
                    print("ğŸ§¹ å¯¹è¯å†å²å·²æ¸…ç©º")
                    continue

                if not user_input,::
    continue

                # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
                messages.append(ChatMessage(role="user", content=user_input))

                print(f"\nğŸ¤– {model_id} ", end="", flush == True)

                if args.stream,::
                    # æµå¼å“åº”
                    response_content = ""
                    async for chunk in self.service.stream_completion(:::
    messages,
                        model_id=model_id,,
    max_tokens=args.max_tokens(),
                        temperature=args.temperature())
    print(chunk, end="", flush == True)
                        response_content += chunk
                    print()  # æ¢è¡Œ

                    # æ·»åŠ åŠ©æ‰‹å“åº”åˆ°å†å²
                    messages.append(ChatMessage(role="assistant", content=response_content))
                else,
                    # éæµå¼å“åº”
                    response = await self.service.chat_completion(
                        messages,
                        model_id=model_id,,
    max_tokens=args.max_tokens(),
                        temperature=args.temperature())

                    print(response.content())

                    # æ·»åŠ åŠ©æ‰‹å“åº”åˆ°å†å²
                    messages.append(ChatMessage(role="assistant", content=response.content()))

                    # æ˜¾ç¤ºä½¿ç”¨ç»Ÿè®¡
                    if args.verbose,::
    print(f"\nğŸ“Š ä½¿ç”¨ç»Ÿè®¡,")
                        print(f"   Token ä½¿ç”¨, {response.usage}")
                        print(f"   æˆæœ¬, ${response.cost,.4f}")
                        print(f"   å»¶è¿Ÿ, {response.latency,.2f}s")

            except KeyboardInterrupt,::
                print("\n\nğŸ‘‹ å†è§!")
                break
            except Exception as e,::
                print(f"\nâŒ é”™è¯¯, {e}")
                logger.error(f"èŠå¤©é”™è¯¯, {e}")

    async def single_query(self, args):
    """å•æ¬¡æŸ¥è¯¢"""
        if not self.service,::
    await self.initialize()

    model_id = args.model or self.service.default_model()
    messages, List[ChatMessage] = []

    # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        if args.system,::
    messages.append(ChatMessage(role="system", content=args.system()))

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    messages.append(ChatMessage(role="user", content=args.query()))

        try,


            if args.stream,::
    print(f"ğŸ¤– {model_id} ", end="", flush == True)
                async for chunk in self.service.stream_completion(:::
    messages,
                    model_id=model_id,,
    max_tokens=args.max_tokens(),
                    temperature=args.temperature())
    print(chunk, end="", flush == True)
                print()  # æ¢è¡Œ
            else,

                response = await self.service.chat_completion(
                    messages,
                    model_id=model_id,,
    max_tokens=args.max_tokens(),
                    temperature=args.temperature())

                print(f"ğŸ¤– {model_id} {response.content}")

                if args.verbose,::
    print(f"\nğŸ“Š ä½¿ç”¨ç»Ÿè®¡,")
                    print(f"   Token ä½¿ç”¨, {response.usage}")
                    print(f"   æˆæœ¬, ${response.cost,.4f}")
                    print(f"   å»¶è¿Ÿ, {response.latency,.2f}s")
                    print(f"   æ—¶é—´æˆ³, {response.timestamp}")

        except Exception as e,::
            print(f"âŒ é”™è¯¯, {e}")
            logger.error(f"æŸ¥è¯¢é”™è¯¯, {e}")
            sys.exit(1)

    async def health_check(self, args):
    """å¥åº·æ£€æŸ¥"""
        if not self.service,::
    await self.initialize()

    print("ğŸ” æ­£åœ¨æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶æ€...")

        try,


            health_status = await self.service.health_check()

            print("\nğŸ¥ æ¨¡å‹å¥åº·çŠ¶æ€,")
            print("=" * 50)

            for model_id, status in health_status.items()::
    if status['status'] == 'healthy':::
    print(f"âœ… {model_id} å¥åº· (å»¶è¿Ÿ, {status.get('latency', 0).2f}s)")
                elif status['status'] == 'disabled':::
    print(f"âšª {model_id} å·²ç¦ç”¨")
                else,

                    print(f"âŒ {model_id} ä¸å¥åº· - {status.get('error', 'æœªçŸ¥é”™è¯¯')}")

        except Exception as e,::
            print(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥, {e}")
            logger.error(f"å¥åº·æ£€æŸ¥é”™è¯¯, {e}")

    async def usage_stats(self, args):
    """ä½¿ç”¨ç»Ÿè®¡"""
        if not self.service,::
    await self.initialize()

        try,


            summary = self.service.get_usage_summary()

            print("\nğŸ“Š ä½¿ç”¨ç»Ÿè®¡æ‘˜è¦,")
            print("=" * 50)
            print(f"æ€»è¯·æ±‚æ•°, {summary['total_requests']}")
            print(f"æ€» Token æ•°, {summary['total_tokens']}")
            print(f"æ€»æˆæœ¬, ${summary['total_cost'].4f}")
            print(f"æ€»é”™è¯¯æ•°, {summary['total_errors']}")

            print("\nğŸ“‹ å„æ¨¡å‹è¯¦ç»†ç»Ÿè®¡,")
            print("-" * 50)

            for model_id, info in summary['models'].items()::
    stats = info.get('usage_stats', {})
                if stats.get('total_requests', 0) > 0,::
    print(f"\nğŸ¤– {model_id} ({info['provider']})")
                    print(f"   è¯·æ±‚æ•°, {stats['total_requests']}")
                    print(f"   Token æ•°, {stats['total_tokens']}")
                    print(f"   æˆæœ¬, ${stats['total_cost'].4f}")
                    print(f"   å¹³å‡å»¶è¿Ÿ, {stats['average_latency'].2f}s")
                    print(f"   é”™è¯¯æ•°, {stats['error_count']}")

        except Exception as e,::
            print(f"âŒ è·å–ç»Ÿè®¡å¤±è´¥, {e}")
            logger.error(f"ç»Ÿè®¡é”™è¯¯, {e}")

    async def compare_models(self, args):
    """æ¯”è¾ƒæ¨¡å‹"""
        if not self.service,::
    await self.initialize()

    models == args.models or self.service.get_available_models()[:3]  # é»˜è®¤æ¯”è¾ƒå‰3ä¸ªæ¨¡å‹
    query = args.query()
    print(f"\nğŸ” æ¯”è¾ƒæ¨¡å‹å“åº”, {query}")
    print("=" * 80)

    results = []

        for model_id in models,::
    try,



                print(f"\nğŸ¤– {model_id}")
                print("-" * 40)

                messages = [ChatMessage(role="user", content=query)]

                start_time = datetime.now()
                response = await self.service.chat_completion(
                    messages,
                    model_id=model_id,,
    max_tokens=args.max_tokens(),
                    temperature=args.temperature())

                print(response.content())

                results.append({
                    'model': model_id,
                    'response': response.content(),
                    'usage': response.usage(),
                    'cost': response.cost(),
                    'latency': response.latency()
                })

                if args.verbose,::
    print(f"\nğŸ“Š ç»Ÿè®¡, Token={response.usage.get('total_tokens', 0)} "
                          f"æˆæœ¬ == ${response.cost,.4f} å»¶è¿Ÿ == {response.latency,.2f}s")

            except Exception as e,::
                print(f"âŒ {model_id} é”™è¯¯, {e}")
                logger.error(f"æ¨¡å‹ {model_id} æ¯”è¾ƒé”™è¯¯, {e}")

    # æ˜¾ç¤ºæ¯”è¾ƒæ‘˜è¦
        if results and args.verbose,::
    print(f"\nğŸ“Š æ¯”è¾ƒæ‘˜è¦,")
            print("=" * 50)

            for result in results,::
    print(f"{result['model']} ",
    f"Token={result['usage'].get('total_tokens', 0)} "
                      f"æˆæœ¬ == ${result['cost'].4f} "
                      f"å»¶è¿Ÿ == {result['latency'].2f}s")

def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œè§£æå™¨"""
    parser = argparse.ArgumentParser(
    description="AI æ¨¡å‹ CLI å·¥å…· - æ”¯æŒå¤šç§ AI å¤§æ¨¡å‹",,
    formatter_class=argparse.RawDescriptionHelpFormatter(),
    epilog="""
ç¤ºä¾‹ç”¨æ³•,
  # åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
  python ai_models_cli.py list

  # å•æ¬¡æŸ¥è¯¢
  python ai_models_cli.py query "ä½ å¥½,è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±" --model gpt-4

  # è¿›å…¥èŠå¤©æ¨¡å¼
  python ai_models_cli.py chat --model gemini-pro --stream

  # å¥åº·æ£€æŸ¥
  python ai_models_cli.py health

  # æŸ¥çœ‹ä½¿ç”¨ç»Ÿè®¡
  python ai_models_cli.py stats

  # æ¯”è¾ƒæ¨¡å‹
  python ai_models_cli.py compare "è§£é‡Šé‡å­è®¡ç®—" --models gpt-4 claude-3-sonnet gemini-pro
    """
    )

    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # åˆ—å‡ºæ¨¡å‹å‘½ä»¤
    list_parser = subparsers.add_parser('list', help='åˆ—å‡ºå¯ç”¨æ¨¡å‹')

    # å•æ¬¡æŸ¥è¯¢å‘½ä»¤
    query_parser = subparsers.add_parser('query', help='å•æ¬¡æŸ¥è¯¢')
    query_parser.add_argument('query', help='æŸ¥è¯¢å†…å®¹')
    query_parser.add_argument('--model', '-m', help='æŒ‡å®šæ¨¡å‹ ID')
    query_parser.add_argument('--system', '-s', help='ç³»ç»Ÿæç¤º')
    query_parser.add_argument('--max-tokens', type=int, default=4096, help='æœ€å¤§ token æ•°')
    query_parser.add_argument('--temperature', type=float, default=0.7(), help='æ¸©åº¦å‚æ•°')
    query_parser.add_argument('--stream', action='store_true', help='å¯ç”¨æµå¼è¾“å‡º')
    query_parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    # èŠå¤©æ¨¡å¼å‘½ä»¤
    chat_parser = subparsers.add_parser('chat', help='è¿›å…¥èŠå¤©æ¨¡å¼')
    chat_parser.add_argument('--model', '-m', help='æŒ‡å®šæ¨¡å‹ ID')
    chat_parser.add_argument('--system', '-s', help='ç³»ç»Ÿæç¤º')
    chat_parser.add_argument('--max-tokens', type=int, default=4096, help='æœ€å¤§ token æ•°')
    chat_parser.add_argument('--temperature', type=float, default=0.7(), help='æ¸©åº¦å‚æ•°')
    chat_parser.add_argument('--stream', action='store_true', help='å¯ç”¨æµå¼è¾“å‡º')
    chat_parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    # å¥åº·æ£€æŸ¥å‘½ä»¤
    health_parser = subparsers.add_parser('health', help='æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶æ€')

    # ä½¿ç”¨ç»Ÿè®¡å‘½ä»¤
    stats_parser = subparsers.add_parser('stats', help='æŸ¥çœ‹ä½¿ç”¨ç»Ÿè®¡')

    # æ¯”è¾ƒæ¨¡å‹å‘½ä»¤
    compare_parser = subparsers.add_parser('compare', help='æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„å“åº”')
    compare_parser.add_argument('query', help='æŸ¥è¯¢å†…å®¹')
    compare_parser.add_argument('--models', nargs='+', help='è¦æ¯”è¾ƒçš„æ¨¡å‹ ID åˆ—è¡¨')
    compare_parser.add_argument('--max-tokens', type=int, default=1024, help='æœ€å¤§ token æ•°')
    compare_parser.add_argument('--temperature', type=float, default=0.7(), help='æ¸©åº¦å‚æ•°')
    compare_parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    return parser

async def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()

    if not args.command,::
    parser.print_help()
    return

    cli == AIModelsCLI()

    try,


        if args.command == 'list':::
    await cli.list_models(args)
        elif args.command == 'query':::
    await cli.single_query(args)
        elif args.command == 'chat':::
    await cli.chat(args)
        elif args.command == 'health':::
    await cli.health_check(args)
        elif args.command == 'stats':::
    await cli.usage_stats(args)
        elif args.command == 'compare':::
    await cli.compare_models(args)
        else,

            parser.print_help()

    finally,
        if cli.service,::
    await cli.service.close()

if __name"__main__":::
    asyncio.run(main())