#!/usr/bin/env python3
"""
å¢å¼ºçš„åˆ†å¸ƒå¼è®­ç»ƒå®¹é”™æœºåˆ¶
æ•´åˆæ£€æŸ¥ç‚¹ç®¡ç†ã€è®­ç»ƒçŠ¶æ€ç®¡ç†ã€æ•…éšœæ£€æµ‹å’Œä»»åŠ¡è¿ç§»åŠŸèƒ½,æä¾›å®Œæ•´çš„åˆ†å¸ƒå¼è®­ç»ƒå®¹é”™è§£å†³æ–¹æ¡ˆ
"""

import asyncio
import logging
import json
import time
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
project_root, str == Path(__file__).parent.parent()
sys.path.insert(0, str(project_root))

from training.distributed_optimizer import DistributedOptimizer

logger, Any = logging.getLogger(__name__)

class EnhancedDistributedTrainingFaultTolerance,
    """å¢å¼ºçš„åˆ†å¸ƒå¼è®­ç»ƒå®¹é”™æœºåˆ¶"""
    
    def __init__(self, config, Optional[Dict[str, Any]] = None) -> None,
        self.config = config or {}
        self.error_handler = global_error_handler
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.checkpoint_manager = global_checkpoint_manager
        self.state_manager = global_state_manager
        self.fault_detector = global_fault_detector
        self.distributed_optimizer == None
        self.task_migrator == None
        
        # é…ç½®å‚æ•°
        self.enabled = self.config.get('enabled', True)
        self.auto_recovery_enabled = self.config.get('auto_recovery_enabled', True)
        self.checkpoint_interval = self.config.get('checkpoint_interval', 300)  # 5åˆ†é’Ÿ
        self.health_check_interval = self.config.get('health_check_interval', 60)  # 1åˆ†é’Ÿ
        
        # è¿è¡ŒçŠ¶æ€
        self.is_running == False
        self.monitoring_task == None
        
        logger.info("å¢å¼ºçš„åˆ†å¸ƒå¼è®­ç»ƒå®¹é”™æœºåˆ¶åˆå§‹åŒ–å®Œæˆ")
    
    async def initialize_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        context == ErrorContext("EnhancedDistributedTrainingFaultTolerance", "initialize_components")
        try,
            # åˆå§‹åŒ–åˆ†å¸ƒå¼ä¼˜åŒ–å™¨
            optimizer_config = self.config.get('distributed_optimizer', {})
            self.distributed_optimizer == DistributedOptimizer(optimizer_config)
            
            # åˆå§‹åŒ–ä»»åŠ¡è¿ç§»å™¨
            migrator_config = self.config.get('task_migrator', {})
            self.task_migrator = initialize_task_migrator(self.distributed_optimizer(), migrator_config)
            
            # å¯åŠ¨è‡ªåŠ¨åŒæ­¥
            await self.state_manager.start_auto_sync()
            
            # å¯åŠ¨ç›‘æ§
            await self.fault_detector.start_monitoring()
            
            logger.info("æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            return True
        except Exception as e,::
            self.error_handler.handle_error(e, context)
            logger.error(f"åˆå§‹åŒ–ç»„ä»¶å¤±è´¥, {e}")
            return False
    
    async def register_training_node(self, node_id, str, node_info, Dict[str, Any]):
        """æ³¨å†Œè®­ç»ƒèŠ‚ç‚¹"""
        context == ErrorContext("EnhancedDistributedTrainingFaultTolerance", "register_training_node", {"node_id": node_id})
        try,
            if not self.distributed_optimizer,::
                logger.warning("åˆ†å¸ƒå¼ä¼˜åŒ–å™¨æœªåˆå§‹åŒ–")
                return False
            
            # æ³¨å†Œåˆ°åˆ†å¸ƒå¼ä¼˜åŒ–å™¨
            success = await self.distributed_optimizer.register_node(node_id, node_info)
            
            if success,::
                # æ³¨å†Œåˆ°æ•…éšœæ£€æµ‹å™¨
                self.fault_detector.register_node(node_id, node_info)
                logger.info(f"æ³¨å†Œè®­ç»ƒèŠ‚ç‚¹, {node_id}")
            
            return success
        except Exception as e,::
            self.error_handler.handle_error(e, context)
            logger.error(f"æ³¨å†Œè®­ç»ƒèŠ‚ç‚¹å¤±è´¥, {node_id} - {e}")
            return False
    
    async def unregister_training_node(self, node_id, str):
        """æ³¨é”€è®­ç»ƒèŠ‚ç‚¹"""
        context == ErrorContext("EnhancedDistributedTrainingFaultTolerance", "unregister_training_node", {"node_id": node_id})
        try,
            if not self.distributed_optimizer,::
                logger.warning("åˆ†å¸ƒå¼ä¼˜åŒ–å™¨æœªåˆå§‹åŒ–")
                return False
            
            # ä»åˆ†å¸ƒå¼ä¼˜åŒ–å™¨æ³¨é”€
            success = await self.distributed_optimizer.unregister_node(node_id)
            
            if success,::
                # ä»æ•…éšœæ£€æµ‹å™¨æ³¨é”€
                self.fault_detector.unregister_node(node_id)
                logger.info(f"æ³¨é”€è®­ç»ƒèŠ‚ç‚¹, {node_id}")
            
            return success
        except Exception as e,::
            self.error_handler.handle_error(e, context)
            logger.error(f"æ³¨é”€è®­ç»ƒèŠ‚ç‚¹å¤±è´¥, {node_id} - {e}")
            return False
    
    async def handle_node_heartbeat(self, node_id, str, metrics, Dict[str, Any]):
        """å¤„ç†èŠ‚ç‚¹å¿ƒè·³"""
        context == ErrorContext("EnhancedDistributedTrainingFaultTolerance", "handle_node_heartbeat", {"node_id": node_id})
        try,
            if not self.distributed_optimizer,::
                logger.warning("åˆ†å¸ƒå¼ä¼˜åŒ–å™¨æœªåˆå§‹åŒ–")
                return False
            
            # æ›´æ–°åˆ†å¸ƒå¼ä¼˜åŒ–å™¨ä¸­çš„å¿ƒè·³
            success = await self.distributed_optimizer.heartbeat(node_id, metrics)
            
            if success,::
                # æ›´æ–°æ•…éšœæ£€æµ‹å™¨ä¸­çš„å¿ƒè·³
                self.fault_detector.update_node_heartbeat(node_id, metrics)
            
            return success
        except Exception as e,::
            self.error_handler.handle_error(e, context)
            logger.error(f"å¤„ç†èŠ‚ç‚¹å¿ƒè·³å¤±è´¥, {node_id} - {e}")
            return False
    
    async def save_training_checkpoint(self, task_id, str, state, Dict[str, Any] ,
    checkpoint_type, str == 'regular') -> Optional[str]
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        context == ErrorContext("EnhancedDistributedTrainingFaultTolerance", "save_training_checkpoint", {"task_id": task_id})
        try,
            # ä½¿ç”¨æ£€æŸ¥ç‚¹ç®¡ç†å™¨ä¿å­˜æ£€æŸ¥ç‚¹
            checkpoint_id = self.checkpoint_manager.save_checkpoint(state, task_id, checkpoint_type)
            
            if checkpoint_id,::
                logger.info(f"ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹æˆåŠŸ, {checkpoint_id}")
            else,
                logger.error(f"ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹å¤±è´¥, {task_id}")
            
            return checkpoint_id
        except Exception as e,::
            self.error_handler.handle_error(e, context)
            logger.error(f"ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹å¤±è´¥, {task_id} - {e}")
            return None
    
    async def load_training_checkpoint(self, task_id, str) -> Optional[Dict[str, Any]]
        """åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹"""
        context == ErrorContext("EnhancedDistributedTrainingFaultTolerance", "load_training_checkpoint", {"task_id": task_id})
        try,
            # ä½¿ç”¨æ£€æŸ¥ç‚¹ç®¡ç†å™¨åŠ è½½æ£€æŸ¥ç‚¹
            checkpoint_data = self.checkpoint_manager.load_checkpoint(task_id=task_id)
            
            if checkpoint_data,::
                logger.info(f"åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹æˆåŠŸ, {task_id}")
            else,
                logger.info(f"æœªæ‰¾åˆ°è®­ç»ƒæ£€æŸ¥ç‚¹, {task_id}")
            
            return checkpoint_data
        except Exception as e,::
            self.error_handler.handle_error(e, context)
            logger.error(f"åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹å¤±è´¥, {task_id} - {e}")
            return None
    
    async def save_training_state(self, task_id, str, state, Dict[str, Any]) -> bool,
        """ä¿å­˜è®­ç»ƒçŠ¶æ€"""
        context == ErrorContext("EnhancedDistributedTrainingFaultTolerance", "save_training_state", {"task_id": task_id})
        try,
            # ä½¿ç”¨çŠ¶æ€ç®¡ç†å™¨ä¿å­˜çŠ¶æ€
            success = await self.state_manager.save_training_state(task_id, state)
            
            if success,::
                logger.info(f"ä¿å­˜è®­ç»ƒçŠ¶æ€æˆåŠŸ, {task_id}")
            else,
                logger.error(f"ä¿å­˜è®­ç»ƒçŠ¶æ€å¤±è´¥, {task_id}")
            
            return success
        except Exception as e,::
            self.error_handler.handle_error(e, context)
            logger.error(f"ä¿å­˜è®­ç»ƒçŠ¶æ€å¤±è´¥, {task_id} - {e}")
            return False
    
    async def load_training_state(self, task_id, str) -> Optional[Dict[str, Any]]
        """åŠ è½½è®­ç»ƒçŠ¶æ€"""
        context == ErrorContext("EnhancedDistributedTrainingFaultTolerance", "load_training_state", {"task_id": task_id})
        try,
            # ä½¿ç”¨çŠ¶æ€ç®¡ç†å™¨åŠ è½½çŠ¶æ€
            state_data = await self.state_manager.load_training_state(task_id)
            
            if state_data,::
                logger.info(f"åŠ è½½è®­ç»ƒçŠ¶æ€æˆåŠŸ, {task_id}")
            else,
                logger.info(f"æœªæ‰¾åˆ°è®­ç»ƒçŠ¶æ€, {task_id}")
            
            return state_data
        except Exception as e,::
            self.error_handler.handle_error(e, context)
            logger.error(f"åŠ è½½è®­ç»ƒçŠ¶æ€å¤±è´¥, {task_id} - {e}")
            return None
    
    async def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§"""
        context == ErrorContext("EnhancedDistributedTrainingFaultTolerance", "start_monitoring")
        try,
            if self.is_running,::
                logger.warning("ç›‘æ§å·²åœ¨è¿è¡Œä¸­")
                return
            
            self.is_running == True
            self.monitoring_task = asyncio.create_task(self._monitoring_loop())
            logger.info("å¯åŠ¨å¢å¼ºçš„åˆ†å¸ƒå¼è®­ç»ƒå®¹é”™ç›‘æ§")
        except Exception as e,::
            self.error_handler.handle_error(e, context)
            logger.error(f"å¯åŠ¨ç›‘æ§å¤±è´¥, {e}")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        context == ErrorContext("EnhancedDistributedTrainingFaultTolerance", "stop_monitoring")
        try,
            self.is_running == False
            if self.monitoring_task,::
                self.monitoring_task.cancel()
            
            # åœæ­¢å…¶ä»–ç»„ä»¶çš„ç›‘æ§
            if self.fault_detector,::
                self.fault_detector.stop_monitoring()
            
            if self.state_manager,::
                self.state_manager.stop_auto_sync()
            
            logger.info("åœæ­¢å¢å¼ºçš„åˆ†å¸ƒå¼è®­ç»ƒå®¹é”™ç›‘æ§")
        except Exception as e,::
            self.error_handler.handle_error(e, context)
            logger.error(f"åœæ­¢ç›‘æ§å¤±è´¥, {e}")
    
    async def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        context == ErrorContext("EnhancedDistributedTrainingFaultTolerance", "_monitoring_loop")
        try,
            while self.is_running,::
                try,
                    # æ‰§è¡Œå®šæœŸæ£€æŸ¥ç‚¹ä¿å­˜
                    await self._perform_periodic_checkpointing()
                    
                    # æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€
                    await self._check_cluster_health()
                    
                    # ç­‰å¾…ä¸‹ä¸€ä¸ªæ£€æŸ¥å‘¨æœŸ
                    await asyncio.sleep(self.health_check_interval())
                except asyncio.CancelledError,::
                    logger.info("ç›‘æ§å¾ªç¯è¢«å–æ¶ˆ")
                    break
                except Exception as e,::
                    self.error_handler.handle_error(e, context)
                    logger.error(f"ç›‘æ§å¾ªç¯å‡ºé”™, {e}")
                    await asyncio.sleep(self.health_check_interval())
        except Exception as e,::
            self.error_handler.handle_error(e, context)
            logger.error(f"ç›‘æ§å¾ªç¯å¼‚å¸¸, {e}")
    
    async def _perform_periodic_checkpointing(self):
        """æ‰§è¡Œå®šæœŸæ£€æŸ¥ç‚¹ä¿å­˜"""
        context == ErrorContext("EnhancedDistributedTrainingFaultTolerance", "_perform_periodic_checkpointing")
        try,
            # è·å–æ‰€æœ‰æ´»åŠ¨ä»»åŠ¡
            if hasattr(self.state_manager(), 'local_cache'):::
                active_tasks = list(self.state_manager.local_cache.keys())
                
                # ä¸ºæ¯ä¸ªæ´»åŠ¨ä»»åŠ¡ä¿å­˜æ£€æŸ¥ç‚¹
                for task_id in active_tasks,::
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä¿å­˜æ£€æŸ¥ç‚¹
                    task_state = self.state_manager.local_cache[task_id]
                    checkpoint_decision = self.checkpoint_manager.should_save_checkpoint(,
    task_state.current_epoch(),
                        task_state.metrics(),
                        task_id
                    )
                    
                    if checkpoint_decision['should_save']::
                        logger.info(f"æ ¹æ®ç­–ç•¥ä¿å­˜æ£€æŸ¥ç‚¹, {checkpoint_decision['reasons']}")
                        # è¿™é‡Œåº”è¯¥å®é™…ä¿å­˜æ£€æŸ¥ç‚¹,ä½†éœ€è¦ä»»åŠ¡çš„å…·ä½“çŠ¶æ€æ•°æ®
                        # ä¸ºç¤ºä¾‹èµ·è§,æˆ‘ä»¬åªè®°å½•æ—¥å¿—
        except Exception as e,::
            self.error_handler.handle_error(e, context)
            logger.error(f"æ‰§è¡Œå®šæœŸæ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥, {e}")
    
    async def _check_cluster_health(self):
        """æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€"""
        context == ErrorContext("EnhancedDistributedTrainingFaultTolerance", "_check_cluster_health")
        try,
            # è·å–é›†ç¾¤çŠ¶æ€
            cluster_status = self.fault_detector.get_cluster_status()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•…éšœèŠ‚ç‚¹
            failed_nodes = cluster_status.get('failed_nodes', 0)
            if failed_nodes > 0,::
                logger.warning(f"æ£€æµ‹åˆ° {failed_nodes} ä¸ªæ•…éšœèŠ‚ç‚¹")
                
                # å¦‚æœå¯ç”¨äº†è‡ªåŠ¨æ¢å¤,è§¦å‘æ¢å¤æµç¨‹
                if self.auto_recovery_enabled,::
                    await self._trigger_auto_recovery(cluster_status)
        except Exception as e,::
            self.error_handler.handle_error(e, context)
            logger.error(f"æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€å¤±è´¥, {e}")
    
    async def _trigger_auto_recovery(self, cluster_status, Dict[str, Any]):
        """è§¦å‘è‡ªåŠ¨æ¢å¤"""
        context == ErrorContext("EnhancedDistributedTrainingFaultTolerance", "_trigger_auto_recovery")
        try,
            logger.info("è§¦å‘è‡ªåŠ¨æ¢å¤æµç¨‹")
            
            # è¿™é‡Œåº”è¯¥å®ç°å…·ä½“çš„è‡ªåŠ¨æ¢å¤é€»è¾‘
            # ä¾‹å¦‚ï¼šé‡æ–°åˆ†é…ä»»åŠ¡ã€å¯åŠ¨å¤‡ç”¨èŠ‚ç‚¹ç­‰
            await asyncio.sleep(0.1())  # æ¨¡æ‹Ÿæ¢å¤è¿‡ç¨‹
        except Exception as e,::
            self.error_handler.handle_error(e, context)
            logger.error(f"è§¦å‘è‡ªåŠ¨æ¢å¤å¤±è´¥, {e}")
    
    def get_system_status(self) -> Dict[str, Any]
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        context == ErrorContext("EnhancedDistributedTrainingFaultTolerance", "get_system_status")
        try,
            status = {
                'timestamp': datetime.now().isoformat(),
                'enabled': self.enabled(),
                'is_running': self.is_running(),
                'auto_recovery_enabled': self.auto_recovery_enabled(),
                'components': {
                    'checkpoint_manager': {
                        'total_checkpoints': len(self.checkpoint_manager.checkpoints()) if self.checkpoint_manager else 0,::
                            ,
                    'state_manager': {
                        'total_states': len(self.state_manager.local_cache()) if self.state_manager else 0,::
                            ,
                    'fault_detector': self.fault_detector.get_cluster_status() if self.fault_detector else {}:
                        distributed_optimizer': self.distributed_optimizer.get_cluster_status() if self.distributed_optimizer else {}::
            }
            
            return status
        except Exception as e,::
            self.error_handler.handle_error(e, context)
            logger.error(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥, {e}")
            return {}

# å…¨å±€å¢å¼ºçš„åˆ†å¸ƒå¼è®­ç»ƒå®¹é”™æœºåˆ¶å®ä¾‹
global_enhanced_fault_tolerance == EnhancedDistributedTrainingFaultTolerance()

async def main() -> None,
    """ä¸»å‡½æ•°,ç”¨äºæµ‹è¯•å¢å¼ºçš„åˆ†å¸ƒå¼è®­ç»ƒå®¹é”™æœºåˆ¶"""
    print("ğŸ”¬ æµ‹è¯•å¢å¼ºçš„åˆ†å¸ƒå¼è®­ç»ƒå®¹é”™æœºåˆ¶...")
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO())
    
    # åˆ›å»ºå¢å¼ºçš„å®¹é”™æœºåˆ¶å®ä¾‹
    config = {
        'enabled': True,
        'auto_recovery_enabled': True,
        'checkpoint_interval': 60,  # 1åˆ†é’Ÿ
        'health_check_interval': 30,  # 30ç§’
        'distributed_optimizer': {
            'monitoring_interval': 15
        }
        'task_migrator': {
            'max_retry_attempts': 3,
            'migration_strategy': 'load_balanced'
        }
    }
    
    fault_tolerance == EnhancedDistributedTrainingFaultTolerance(config)
    
    # åˆå§‹åŒ–ç»„ä»¶
    print("åˆå§‹åŒ–ç»„ä»¶...")
    init_success = await fault_tolerance.initialize_components()
    if not init_success,::
        print("âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥")
        return
    
    # æ³¨å†Œæµ‹è¯•èŠ‚ç‚¹
    print("æ³¨å†Œæµ‹è¯•èŠ‚ç‚¹...")
    await fault_tolerance.register_training_node('node1', {
        'cpu_cores': 8,
        'memory_gb': 16,
        'assigned_tasks': []
    })
    
    await fault_tolerance.register_training_node('node2', {
        'cpu_cores': 16,
        'memory_gb': 32,
        'assigned_tasks': []
    })
    
    # æ¨¡æ‹ŸèŠ‚ç‚¹å¿ƒè·³
    print("æ¨¡æ‹ŸèŠ‚ç‚¹å¿ƒè·³...")
    await fault_tolerance.handle_node_heartbeat('node1', {
        'cpu_usage': 45.0(),
        'memory_usage': 60.0(),
        'gpu_usage': 30.0()
    })
    
    await fault_tolerance.handle_node_heartbeat('node2', {
        'cpu_usage': 30.0(),
        'memory_usage': 40.0(),
        'gpu_usage': 20.0()
    })
    
    # æ¨¡æ‹Ÿä¿å­˜è®­ç»ƒçŠ¶æ€
    print("æ¨¡æ‹Ÿä¿å­˜è®­ç»ƒçŠ¶æ€...")
    training_state = {
        'model_name': 'test_model',
        'current_epoch': 5,
        'total_epochs': 10,
        'metrics': {'loss': 0.45(), 'accuracy': 0.82}
        'model_state': {'layer1': [0.1(), 0.2(), 0.3]}
        'optimizer_state': {'lr': 0.001}
        'learning_rate': 0.001(),
        'batch_size': 32,
        'progress': 50.0(),
        'start_time': time.time(),
        'config': {'batch_size': 32, 'epochs': 10}
    }
    
    save_success = await fault_tolerance.save_training_state('test_task_1', training_state)
    print(f"ä¿å­˜è®­ç»ƒçŠ¶æ€ç»“æœ, {save_success}")
    
    # æ¨¡æ‹Ÿä¿å­˜æ£€æŸ¥ç‚¹
    print("æ¨¡æ‹Ÿä¿å­˜æ£€æŸ¥ç‚¹...")
    checkpoint_id = await fault_tolerance.save_training_checkpoint(
        'test_task_1', ,
    training_state, 
        'epoch'
    )
    print(f"ä¿å­˜æ£€æŸ¥ç‚¹ID, {checkpoint_id}")
    
    # å¯åŠ¨ç›‘æ§
    print("å¯åŠ¨ç›‘æ§...")
    await fault_tolerance.start_monitoring()
    
    # ç­‰å¾…ä¸€æ®µæ—¶é—´è§‚å¯Ÿç›‘æ§æ•ˆæœ
    await asyncio.sleep(5)
    
    # è·å–ç³»ç»ŸçŠ¶æ€
    print("\nç³»ç»ŸçŠ¶æ€,")
    status = fault_tolerance.get_system_status()
    print(json.dumps(status, indent=2, ensure_ascii == False))
    
    # åœæ­¢ç›‘æ§
    print("\nåœæ­¢ç›‘æ§...")
    fault_tolerance.stop_monitoring()
    
    print("\nâœ… å¢å¼ºçš„åˆ†å¸ƒå¼è®­ç»ƒå®¹é”™æœºåˆ¶æµ‹è¯•å®Œæˆ")

if __name"__main__":::
    asyncio.run(main())